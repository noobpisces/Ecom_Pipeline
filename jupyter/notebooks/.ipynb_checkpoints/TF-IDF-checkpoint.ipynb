{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638f47a1-cae7-428d-a3e0-4b88217860cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  regexp_replace,udf,explode,col, expr,when,to_date, sum, from_json,size,length, collect_set, broadcast,concat_ws\n",
    "from pyspark.sql.types import  ArrayType,StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099d0057-3dbd-4c6f-af7d-287a2cd5af66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lake\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"9g\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.2.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04bb2311-54a4-47d7-9f3d-2adf26c27598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "silver_merge_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc0fd99-ca15-4e34-9c68-30f59150e0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bf452-9bf7-4eec-8413-4f201f4bd6f7",
   "metadata": {},
   "source": [
    "# HAsing TF- IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7aae55e-6091-405b-9e10-a5a7aa0985ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Giả sử spark_df đã được định nghĩa từ silver_merge_df\n",
    "spark_df = silver_merge_df.select('id', 'comb')\n",
    "\n",
    "# Định nghĩa các stage trong pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"comb\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Tạo pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "\n",
    "# Fit pipeline vào dữ liệu\n",
    "pipeline_mdl = pipeline.fit(spark_df)\n",
    "\n",
    "# Transform dữ liệu để tạo vector TF-IDF\n",
    "new_df = pipeline_mdl.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f439a8b-af8d-4c61-8295-33da6b0f8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa UDF để chuyển sparse vector thành dense vector\n",
    "def to_dense(sparse_vector):\n",
    "    return Vectors.dense(sparse_vector).tolist()\n",
    "\n",
    "to_dense_udf = udf(to_dense, ArrayType(DoubleType()))\n",
    "\n",
    "# Áp dụng UDF để tạo cột 'vecs'\n",
    "new_df_with_dense = new_df.withColumn(\"vecs\", to_dense_udf(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5beac9-6e72-44da-b9f1-8bd4de499e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df_with_dense.select('id', 'vecs').write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/all_movies_delta_IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae97cc88-cde9-499c-a2fb-0d4f8f3bee09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta_IDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ef914d8-dad3-483a-b0aa-77007d30305a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.4626, 0.0, 0.0, 2.4443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.2816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.62, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.9317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.2258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.8324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.6615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0751, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.9843, 0.0, 0.0, 0.0, 0.0, 0.0, 4.7918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.8604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.9589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.3259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = get_vecs()\n",
    "input_vec = [r[1] for r in f if r[0] == 2698][0]\n",
    "input_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e398e749-c595-4f71-9c6d-1b182e51f218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a movie name:  The Box\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số phần tử trong f: 45435\n",
      "Movie ID cho 'The Box': 22825\n",
      "Kiểu dữ liệu của m_id: <class 'str'>\n",
      "Kiểu dữ liệu của r[0] trong all_movies_vecs: <class 'int'>\n",
      "Recommended movies:\n",
      "   movies_id     score  input_movies_id\n",
      "0     288130  0.376962            22825\n",
      "1     126560  0.333881            22825\n",
      "2      49950  0.318400            22825\n",
      "3      86004  0.311725            22825\n",
      "4     247451  0.311446            22825\n",
      "Detailed movie recommendations:\n",
      "   movies_id  input_movies_id         title         genres_convert   \n",
      "0      49950            22825  The Roommate  Thriller Drama Horror  \\\n",
      "1      86004            22825       Talaash   Crime Drama Thriller   \n",
      "\n",
      "                  director                              cast_names     score   \n",
      "0  ChristianE.Christiansen  MinkaKelly LeightonMeester CamGigandet  0.318400  \\\n",
      "1               ReemaKagti     AamirKhan KareenaKapoor RaniMukerji  0.311725   \n",
      "\n",
      "   weighted_rating  \n",
      "0         5.060611  \n",
      "1         5.885010  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, mean, lit, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df_merge = None\n",
    "df_list = None\n",
    "flag1 = False\n",
    "flag2 = False\n",
    "\n",
    "\n",
    "def CosineSim(vec1, vec2):\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2))\n",
    "    return float(numerator / denominator) if denominator != 0 else 0\n",
    "\n",
    "def get_titles():\n",
    "    global df_merge, flag1\n",
    "    if not flag1:\n",
    "        df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "        flag1 = True\n",
    "    return list(df_merge.select(\"title\").toPandas()[\"title\"])\n",
    "\n",
    "def get_vecs():\n",
    "    global df_list, all_vecs, flag2\n",
    "    if not flag2:\n",
    "        df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta_IDF\")\n",
    "        data_original = df_list.collect()\n",
    "        all_vecs = [(row.id, Vectors.dense(row.vecs)) for row in data_original]\n",
    "        flag2 = True\n",
    "    return all_vecs\n",
    "\n",
    "def recommendation(m_title, sim_mov_limit=5):\n",
    "    global df_merge\n",
    "    df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "    if df_merge.filter(col(\"title\") == m_title).count() == 0:\n",
    "        return \"Sorry! The movie you searched is not in our database. Please check the spelling or try another movie.\"\n",
    "    \n",
    "    all_movies_vecs = get_vecs()\n",
    "    print(\"Số phần tử trong f:\", len(all_movies_vecs))\n",
    "\n",
    "    m_id_row = df_merge.filter(col(\"title\") == m_title).select(col('id')).collect()\n",
    "    if not m_id_row:\n",
    "        return \"Movie not found.\"\n",
    "    m_id = m_id_row[0][0]\n",
    "    print(f\"Movie ID cho '{m_title}': {m_id}\")\n",
    "\n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    print(f\"Kiểu dữ liệu của m_id: {type(m_id)}\")\n",
    "    print(f\"Kiểu dữ liệu của r[0] trong all_movies_vecs: {type(all_movies_vecs[0][0])}\")\n",
    "\n",
    "    # Đồng bộ kiểu dữ liệu nếu cần\n",
    "    if isinstance(m_id, str) and isinstance(all_movies_vecs[0][0], int):\n",
    "        m_id = int(m_id)\n",
    "    elif isinstance(m_id, int) and isinstance(all_movies_vecs[0][0], str):\n",
    "        m_id = str(m_id)\n",
    "\n",
    "    # Lấy input_vec với kiểm tra\n",
    "    vec_list = [r[1] for r in all_movies_vecs if r[0] == m_id]\n",
    "    if not vec_list:\n",
    "        raise ValueError(f\"Không tìm thấy vector cho movie_id {m_id}\")\n",
    "    input_vec = vec_list[0]\n",
    "    \n",
    "    similar_movies_rdd = spark.sparkContext.parallelize(\n",
    "        [(i[0], CosineSim(input_vec, i[1])) for i in all_movies_vecs]\n",
    "    )\n",
    "    \n",
    "    similar_movies_df = spark.createDataFrame(similar_movies_rdd, [\"movies_id\", \"score\"]) \\\n",
    "        .orderBy(col(\"score\").desc()) \\\n",
    "        .filter(col(\"movies_id\") != m_id) \\\n",
    "        .limit(sim_mov_limit)\n",
    "    \n",
    "    similar_movies_df = similar_movies_df.withColumn(\"input_movies_id\", lit(m_id))\n",
    "    return similar_movies_df.toPandas()\n",
    "\n",
    "def getMovieDetails(in_mov):\n",
    "    global df_merge\n",
    "    vote_counts = df_merge.filter(col(\"vote_count\").isNotNull()).select(col(\"vote_count\"))\n",
    "    vote_averages = df_merge.filter(col(\"vote_average\").isNotNull()).select(col(\"vote_average\"))\n",
    "    C = vote_averages.select(mean(\"vote_average\")).collect()[0][0]\n",
    "    quantiles = vote_counts.approxQuantile(\"vote_count\", [0.7], 0.001)\n",
    "    m = quantiles[0]\n",
    "    qualified = df_merge.filter((col(\"vote_count\") >= m) & col(\"vote_count\").isNotNull() & col(\"vote_average\").isNotNull())\n",
    "    qualified = qualified.withColumn(\"vote_count\", col(\"vote_count\").cast(\"int\")) \\\n",
    "        .withColumn(\"vote_average\", col(\"vote_average\").cast(\"int\"))\n",
    "    weighted_rating_udf = udf(lambda v, R: (\n",
    "        v / (v + m) * R) + (m / (m + v) * C), FloatType())\n",
    "    qualified = qualified.withColumn(\"weighted_rating\", weighted_rating_udf(\n",
    "        col(\"vote_count\"), col(\"vote_average\")))\n",
    "    qualified = qualified.orderBy(col(\"weighted_rating\").desc())\n",
    "\n",
    "    if isinstance(in_mov, str):\n",
    "        return \"Invalid input\"\n",
    "    a = in_mov.alias(\"a\")\n",
    "    b = qualified.alias(\"b\")\n",
    "\n",
    "    raw = a.join(b, col(\"a.movies_id\") == col(\"b.id\"), 'inner') \\\n",
    "        .orderBy(\"score\", ascending=False) \\\n",
    "        .select([col('a.' + c) for c in a.columns] + [col('b.title'), col('b.genres_convert'), col('b.keyword_convert'), col(\"b.director\"), col(\"b.cast_names\"), col(\"b.weighted_rating\")])\n",
    "    \n",
    "    return raw.select(\"movies_id\", \"input_movies_id\", \"title\", \"genres_convert\", \"director\", \"cast_names\", \"score\", \"weighted_rating\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     movie_name = input(\"Enter a movie name: \")\n",
    "#     recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "#     print(\"Recommended movies:\")\n",
    "#     print(recommendations)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    movie_name = input(\"Enter a movie name: \")\n",
    "\n",
    "    recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "\n",
    "    if isinstance(recommendations, str):  # Nếu không tìm thấy phim\n",
    "        print(recommendations)\n",
    "    else:\n",
    "        print(\"Recommended movies:\")\n",
    "        print(recommendations)\n",
    "\n",
    "        # Chuyển đổi kiểu dữ liệu của Pandas DataFrame về đúng dạng trước khi đưa vào Spark\n",
    "        recommendations = recommendations.astype({\n",
    "            \"movies_id\": int,\n",
    "            \"input_movies_id\": int,\n",
    "            \"score\": float\n",
    "        })\n",
    "\n",
    "        # Xác định schema cho Spark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"movies_id\", IntegerType(), True),\n",
    "            StructField(\"score\", FloatType(), True),\n",
    "            StructField(\"input_movies_id\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        # Chuyển đổi Pandas DataFrame thành danh sách từ điển\n",
    "        recommendations_list = recommendations.to_dict(orient=\"records\")\n",
    "\n",
    "        # Chuyển đổi danh sách từ điển thành Spark DataFrame\n",
    "        recommendations_spark_df = spark.createDataFrame(recommendations_list, schema=schema)\n",
    "        # recommendations_spark_df.show()\n",
    "        \n",
    "        details = getMovieDetails(recommendations_spark_df)\n",
    "\n",
    "\n",
    "        # Hiển thị thông tin chi tiết\n",
    "        print(\"Detailed movie recommendations:\")\n",
    "        print(details.toPandas())  # Chuyển về Pandas DataFrame để hiển thị\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513f1eb-a765-45ce-8238-917ce847b866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = silver_merge_df.filter(col('id') == 948)\n",
    "d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87cb7a-7826-4036-b746-c4576cb2fd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092a53f-905f-4bd3-84c6-643e2e1c8dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f33e9d-cd87-4315-8b59-6ef425ab4911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26536f-e06c-4ca5-9643-98ff353ef898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32566af-b44f-49d3-932a-2ac6ee7fee75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
