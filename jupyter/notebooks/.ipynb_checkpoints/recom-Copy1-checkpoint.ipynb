{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2057043-2a4f-4878-96cb-3a18fb3c4def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, DateType\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, explode, to_date, date_format,\n",
    "    dayofweek, dayofmonth, dayofyear, weekofyear,\n",
    "    month, quarter, year, when, unix_timestamp\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101c085b-c35c-4c16-b1d6-da51c9679607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lake\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "276b478a-0cb6-474b-a691-cc0a02039439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13e60333-5973-4bdf-8146-252515a37e58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7368"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d86a88d-1933-4ca7-a687-0350bfc66b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- budget: integer (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      " |-- date_id: long (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- cast_names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- director_names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447ca6d-33bb-4b32-9296-d43338420545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4ef8c5-fb41-48e8-a9d9-1884a6e3e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39223b1d-e476-4778-932c-35344781fa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e69f7-fdcf-4bbc-9716-47a5bdacbec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f92591-13bf-4eaf-9fd6-f0c3ee9cd5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- budget: integer (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      " |-- date_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_movies = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/fact_movies\")\n",
    "fact_movies.createOrReplaceTempView(\"fact_movies\")\n",
    "fact_movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9220a90-a8a1-474a-9b12-9b0080bd3d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_movie = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/dim_movie\")\n",
    "dim_movie.createOrReplaceTempView(\"dim_movie\")\n",
    "dim_movie.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e7d9ed-181b-4a84-bc60-c216e2f684ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- profile_path: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_cast = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/dim_cast\")\n",
    "dim_cast.createOrReplaceTempView(\"dim_cast\")\n",
    "dim_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cd866df-29b6-40a0-ba10-e152507c7271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast_id: integer (nullable = true)\n",
      " |-- character: string (nullable = true)\n",
      " |-- movie_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_cast = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/movie_cast\")\n",
    "movie_cast.createOrReplaceTempView(\"movie_cast\")\n",
    "movie_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5dde35b-7a40-44e1-9d6c-602c9ac48518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- profile_path: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_crew = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/dim_crew\")\n",
    "dim_crew.createOrReplaceTempView(\"dim_crew\")\n",
    "dim_crew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54506591-2817-44f4-92a0-d9b89a17c383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- crew_id: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- movie_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_crew = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/movie_crew\")\n",
    "movie_crew.createOrReplaceTempView(\"movie_crew\")\n",
    "movie_crew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa8f42a-2594-4046-95a4-d5d78e69aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_genre = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/dim_genre\")\n",
    "dim_genre.createOrReplaceTempView(\"dim_genre\")\n",
    "dim_genre.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc648520-ef9d-4065-93f2-1b77c29ed6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genres_id: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_genre = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/movie_genre\")\n",
    "movie_genre.createOrReplaceTempView(\"movie_genre\")\n",
    "movie_genre.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16a83e05-40fa-4570-8be7-3a9df87b3cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "               SELECT distinct fm.id\n",
    "               FROM fact_movies as fm\n",
    "               JOIN movie_crew as mcr on fm.id = mcr.movie_id and mcr.job = 'Director'\n",
    "               \n",
    "              \"\"\")\n",
    "\n",
    "# df = spark.sql(\"\"\"\n",
    "#                SELECT distinct movie_id\n",
    "#                FROM movie_crew\n",
    "               \n",
    "#               \"\"\")\n",
    "\n",
    "# df = spark.sql(\"\"\"\n",
    "#                SELECT distinct id\n",
    "#                FROM dim_movie\n",
    "               \n",
    "#               \"\"\")\n",
    "\n",
    "# df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")\n",
    "\n",
    "# df = df.filter(\n",
    "#                                 (col(\"budget\") != \"0\") &            # loại bỏ dòng có budget là \"0\" (kiểu string)\n",
    "#                                 (col(\"id\").isNotNull()) &           # loại bỏ dòng có id là null\n",
    "#                                 (col(\"revenue\") != 0) &             # loại bỏ dòng có revenue bằng 0\n",
    "#                                 (col(\"vote_average\") != 0) &        # loại bỏ dòng có vote_average bằng 0\n",
    "#                                 (col(\"vote_count\") != 0) &          # loại bỏ dòng có vote_count bằng 0\n",
    "#                                 (col(\"popularity\") != \"0\") &        # loại bỏ dòng có popularity là \"0\" (kiểu string)\n",
    "#                                 (col(\"release_date\").isNotNull()) & # loại bỏ dòng có release_date là null\n",
    "#                                 (col(\"runtime\") != 0)               # loại bỏ dòng có runtime bằng 0\n",
    "#                             )\n",
    "#df = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/movies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38549e3b-6a87-40f0-b213-1af2c2762e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0da35d-85e9-4f66-9537-9cf073a5e0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = spark.sql(\"\"\"\n",
    "#                SELECT distinct fm.*, dm.language, dm.overview, dm.tagline, dm.runtime, dca.name as cast_name, dcr.name as director_name, dg.name as genre\n",
    "#                FROM fact_movies as fm\n",
    "#                JOIN dim_movie as dm on fm.id = dm.id\n",
    "#                JOIN movie_cast as mca on fm.id = mca.movie_id\n",
    "#                JOIN dim_cast as dca on dca.id = mca.cast_id\n",
    "#                JOIN movie_crew as mcr on fm.id = mcr.movie_id and mcr.job = 'Director'\n",
    "#                JOIN dim_crew as dcr on dcr.id = mcr.crew_id\n",
    "#                JOIN movie_genre as mg on fm.id = mg.id\n",
    "#                JOIN dim_genre as dg on dg.id = mg.genres_id\n",
    "#               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d222321-dbdf-4a23-8ec0-621091118962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = spark.sql(\"\"\"\n",
    "#     SELECT fm.id, fm.budget, fm.popularity, fm.revenue, \n",
    "#            fm.vote_average, fm.vote_count, fm.date_id, \n",
    "#            dm.language, dm.overview, dm.tagline, dm.runtime,\n",
    "#            COLLECT_SET(dca.name) AS cast_names,  -- Gộp diễn viên thành danh sách\n",
    "#            COLLECT_SET(dcr.name) AS director_names,  -- Gộp đạo diễn thành danh sách\n",
    "#            COLLECT_SET(dg.name) AS genres  -- Gộp thể loại thành danh sách\n",
    "#     FROM fact_movies AS fm\n",
    "#     JOIN dim_movie AS dm ON fm.id = dm.id\n",
    "#     JOIN movie_cast AS mca ON fm.id = mca.movie_id\n",
    "#     JOIN dim_cast AS dca ON dca.id = mca.cast_id\n",
    "#     JOIN movie_crew AS mcr ON fm.id = mcr.movie_id AND mcr.job = 'Director'\n",
    "#     JOIN dim_crew AS dcr ON dcr.id = mcr.crew_id\n",
    "#     JOIN movie_genre AS mg ON fm.id = mg.id\n",
    "#     JOIN dim_genre AS dg ON dg.id = mg.genres_id\n",
    "#     GROUP BY fm.id, fm.budget, fm.popularity, fm.revenue, \n",
    "#              fm.vote_average, fm.vote_count, fm.date_id, \n",
    "#              dm.language, dm.overview, dm.tagline, dm.runtime\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3944ff-4108-49a6-b34e-28a3f74fccb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c06fc6cc-70fa-4fac-a400-60e0c2df2f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- budget: integer (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- cast_names: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- director_names: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- genres: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "028a48fd-887a-4e37-a9b1-9a3da69c9962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = df.fillna({\"overview\": \"\"})\n",
    "df_cleaned = df.filter(\n",
    "    (df.budget > 0) & \n",
    "    (df.revenue > 0) & \n",
    "    (df.vote_count > 0) & \n",
    "    (df.runtime > 0) & \n",
    "    (df.language.isNotNull()) & \n",
    "    (df.overview.isNotNull()) & \n",
    "    (df.tagline.isNotNull()) & \n",
    "    (df.cast_names.isNotNull()) & \n",
    "    (df.director_names.isNotNull()) & \n",
    "    (df.genres.isNotNull())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d9c92eeb-f6f3-4182-ad4b-356f03b14158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"overview\", outputCol=\"overview_tokens\")\n",
    "df = tokenizer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "116f98e5-250b-4521-8b80-3413c8aaf68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopword_remover = StopWordsRemover(inputCol=\"overview_tokens\", outputCol=\"filtered_overview\")\n",
    "df = stopword_remover.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fde8b948-008c-4ac1-a1c7-46b93a765b37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o874.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 725.0 failed 1 times, most recent failure: Lost task 0.0 in stage 725.0 (TID 11188) (07cc574957ad executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$5789/0x0000000802172bd8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_ScalaUDF_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.NullPointerException: Cannot invoke \"String.toLowerCase()\" because \"x$1\" is null\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:191)\n\tat org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:312)\n\tat org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:182)\n\tat org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:121)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$5789/0x0000000802172bd8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_ScalaUDF_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m Word2Vec(vectorSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, minCount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_overview\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverview_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o874.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 725.0 failed 1 times, most recent failure: Lost task 0.0 in stage 725.0 (TID 11188) (07cc574957ad executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$5789/0x0000000802172bd8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_ScalaUDF_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.NullPointerException: Cannot invoke \"String.toLowerCase()\" because \"x$1\" is null\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:191)\n\tat org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:312)\n\tat org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:182)\n\tat org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:121)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$5789/0x0000000802172bd8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_ScalaUDF_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"filtered_overview\", outputCol=\"overview_vector\")\n",
    "model = word2vec.fit(df)\n",
    "df = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc023212-5b01-4c34-9c91-91cf0341195d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=161495, budget=22000000, popularity=3.395867, revenue=12400000.0, vote_average=6.4, vote_count=7.0, date_id=19950301, language=\"[{'iso_639_1': 'en', 'name': 'English'}]\", overview='', tagline=None, runtime=108.0, cast_names=['D. B. Sweeney', 'Peter Falk', 'Jan Rubes', 'Ellen Burstyn', 'Ernie Sabella', 'Julianne Moore'], director_names=['Peter Yates'], genres=['Comedy', 'Drama'], overview_tokens=[''], filtered_overview=[''], overview_vector=DenseVector([0.0028, 0.0023, 0.0007, 0.004, -0.0008, -0.0, -0.003, -0.0012, -0.0001, -0.0042, -0.0042, 0.0012, 0.0004, 0.0047, -0.0027, -0.0017, 0.0019, 0.0008, -0.0035, 0.002, 0.0017, 0.0021, -0.0017, -0.0001, 0.0, -0.0052, -0.0046, -0.0012, -0.0006, 0.002, 0.0018, -0.0049, 0.0008, -0.0021, -0.004, -0.004, -0.0057, 0.0029, 0.0004, -0.0024, 0.0007, 0.0058, -0.0041, 0.004, -0.0019, 0.0039, -0.0012, 0.0039, -0.0003, -0.0024, -0.0008, 0.0039, -0.0013, 0.0045, 0.0008, -0.003, 0.0046, -0.0029, -0.0042, -0.0015, -0.006, 0.0011, -0.0039, -0.0046, -0.001, 0.0001, 0.0048, 0.0022, 0.003, 0.0027, 0.0033, -0.0051, -0.0045, -0.0002, 0.0051, 0.0004, 0.0029, 0.0013, -0.0026, -0.0048, 0.0043, -0.001, -0.004, 0.0047, -0.0032, -0.0022, 0.0042, 0.0028, 0.005, 0.0036, -0.0001, 0.0017, 0.0008, -0.0013, -0.0028, -0.0015, 0.0, 0.0054, 0.0026, -0.0004]))]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b3c06-a8ab-4192-b905-e4f309482af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8b6fb-377a-4d29-a816-375757fbb602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a2d3a-4324-48e5-ae4d-e5e6c65b1786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc00c4-f454-4e34-bfbc-7b7185061135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e60a10ff-128f-4fb1-a3e4-e5eecfa57ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4023"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = spark.sql(\"\"\"\n",
    "        select distinct movie_id from movie_crew\n",
    "\"\"\")\n",
    "dfs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "12a1c00b-f71f-412a-b514-d38ece7f80c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5404"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = spark.sql(\"\"\"\n",
    "        select distinct id from fact_movies\n",
    "\"\"\")\n",
    "dfs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1084f0a4-bb51-416b-a02e-9fc4b52b699b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = spark.sql(\"\"\"\n",
    "    select distinct fact_movies.id from fact_movies join movie_cast on id = movie_id\n",
    "\"\"\")\n",
    "\n",
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bb67c2f-5c31-4110-b2ac-496c4ef8f288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cast_id=238486, character='Alexey Grekov, nicknamed \"Greek\"', movie_id=223176),\n",
       " Row(cast_id=262929, character='Anna', movie_id=223176),\n",
       " Row(cast_id=99308, character='Director', movie_id=223176),\n",
       " Row(cast_id=1191678, character='Crucian', movie_id=223176),\n",
       " Row(cast_id=1208670, character='Lola', movie_id=223176),\n",
       " Row(cast_id=1208674, character='', movie_id=223176),\n",
       " Row(cast_id=30405, character='Schota', movie_id=223176),\n",
       " Row(cast_id=2669793, character='', movie_id=223176),\n",
       " Row(cast_id=2669794, character='', movie_id=223176),\n",
       " Row(cast_id=663595, character='', movie_id=223176)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = spark.sql(\"\"\"\n",
    "        select * from movie_cast where movie_id = 223176\n",
    "\"\"\")\n",
    "dfs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3a317-783e-4497-9bbb-5b430d25d004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
