{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6e664c-041a-4c82-bd7f-036ee6bdcef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp==5.5.3\n",
      "  Downloading spark_nlp-5.5.3-py2.py3-none-any.whl (635 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install spark-nlp==5.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f536e544-ba9e-40b3-a6a5-e2d786bd2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  regexp_replace,udf,explode,col, expr,when,to_date, sum, from_json,size,length, collect_set, broadcast,concat_ws\n",
    "from pyspark.sql.types import  ArrayType,StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603a3de8-e7df-4406-894a-279eac6478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# nếu bạn đã khởi spark trước, dừng lại:\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO + Delta + Spark NLP\") \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"9g\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "        \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.3,\"\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,\"\n",
    "        \"io.delta:delta-core_2.12:2.2.0\"\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8a300b-925e-4c22-b4e1-dce1acf28a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738d8fe1-0e6a-47fc-a540-2b45b05c738a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\n",
      "Approximate size to download 16.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler, EmbeddingsFinisher\n",
    "from sparknlp.annotator import Tokenizer, BertEmbeddings, SentenceEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comb\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings.pretrained(\"small_bert_L2_128\", \"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"token_embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"token_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "embeddings_finisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCols([\"finished_embeddings\"]) \\\n",
    "    .setOutputAsVector(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    embeddings_finisher\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(spark_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69304c6a-417e-4cdd-9db2-ffd05fe50996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform và lấy cột finished_embeddings\n",
    "result_df = pipeline_model.transform(spark_df)\n",
    "\n",
    "# Lưu delta gồm id + finished_embeddings\n",
    "result_df.select(\"id\", \"finished_embeddings\") \\\n",
    "    .write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"s3a://lakehouse/data/all_movies_BERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526a7ed9-7769-4bd2-8ada-c687e16aac09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số phần tử trong f: 45435\n",
      "Movie ID cho 'The Box': 22825\n",
      "Kiểu dữ liệu của m_id: <class 'str'>\n",
      "Kiểu dữ liệu của r[0] trong all_movies_vecs: <class 'int'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,128) and (1,128) not aligned: 128 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 113\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m     \n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m#movie_name = input(\"Enter a movie name: \")\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Box\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 113\u001b[0m     recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommendation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_mov_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(recommendations, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# Nếu không tìm thấy phim\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mprint\u001b[39m(recommendations)\n",
      "Cell \u001b[0;32mIn[15], line 65\u001b[0m, in \u001b[0;36mrecommendation\u001b[0;34m(m_title, sim_mov_limit)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKhông tìm thấy vector cho movie_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m input_vec \u001b[38;5;241m=\u001b[39m vec_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m similar_movies_rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(\n\u001b[0;32m---> 65\u001b[0m     [(i[\u001b[38;5;241m0\u001b[39m], CosineSim(input_vec, i[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m all_movies_vecs]\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m similar_movies_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(similar_movies_rdd, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc()) \\\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m m_id) \\\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m.\u001b[39mlimit(sim_mov_limit)\n\u001b[1;32m     73\u001b[0m similar_movies_df \u001b[38;5;241m=\u001b[39m similar_movies_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_movies_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(m_id))\n",
      "Cell \u001b[0;32mIn[15], line 65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKhông tìm thấy vector cho movie_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m input_vec \u001b[38;5;241m=\u001b[39m vec_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m similar_movies_rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(\n\u001b[0;32m---> 65\u001b[0m     [(i[\u001b[38;5;241m0\u001b[39m], \u001b[43mCosineSim\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m all_movies_vecs]\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m similar_movies_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(similar_movies_rdd, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc()) \\\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m m_id) \\\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m.\u001b[39mlimit(sim_mov_limit)\n\u001b[1;32m     73\u001b[0m similar_movies_df \u001b[38;5;241m=\u001b[39m similar_movies_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_movies_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(m_id))\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mCosineSim\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCosineSim\u001b[39m(vec1, vec2):\n\u001b[0;32m---> 13\u001b[0m     numerator \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     denominator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(vec1, vec1)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(vec2, vec2))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(numerator \u001b[38;5;241m/\u001b[39m denominator) \u001b[38;5;28;01mif\u001b[39;00m denominator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,128) and (1,128) not aligned: 128 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, mean, lit, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df_merge = None\n",
    "df_list = None\n",
    "flag1 = False\n",
    "flag2 = False\n",
    "\n",
    "\n",
    "def CosineSim(vec1, vec2):\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2))\n",
    "    return float(numerator / denominator) if denominator != 0 else 0\n",
    "\n",
    "def get_titles():\n",
    "    global df_merge, flag1\n",
    "    if not flag1:\n",
    "        df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "        flag1 = True\n",
    "    return list(df_merge.select(\"title\").toPandas()[\"title\"])\n",
    "\n",
    "def get_vecs():\n",
    "    global df_list, all_vecs, flag2\n",
    "    if not flag2:\n",
    "        df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_BERT\")\n",
    "        data_original = df_list.collect()\n",
    "        all_vecs = [(row.id, Vectors.dense(row.finished_embeddings)) for row in data_original]\n",
    "        flag2 = True\n",
    "    return all_vecs\n",
    "\n",
    "def recommendation(m_title, sim_mov_limit=5):\n",
    "    global df_merge\n",
    "    df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "    if df_merge.filter(col(\"title\") == m_title).count() == 0:\n",
    "        return \"Sorry! The movie you searched is not in our database. Please check the spelling or try another movie.\"\n",
    "    \n",
    "    all_movies_vecs = get_vecs()\n",
    "    print(\"Số phần tử trong f:\", len(all_movies_vecs))\n",
    "\n",
    "    m_id_row = df_merge.filter(col(\"title\") == m_title).select(col('id')).collect()\n",
    "    if not m_id_row:\n",
    "        return \"Movie not found.\"\n",
    "    m_id = m_id_row[0][0]\n",
    "    print(f\"Movie ID cho '{m_title}': {m_id}\")\n",
    "\n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    print(f\"Kiểu dữ liệu của m_id: {type(m_id)}\")\n",
    "    print(f\"Kiểu dữ liệu của r[0] trong all_movies_vecs: {type(all_movies_vecs[0][0])}\")\n",
    "\n",
    "    # Đồng bộ kiểu dữ liệu nếu cần\n",
    "    if isinstance(m_id, str) and isinstance(all_movies_vecs[0][0], int):\n",
    "        m_id = int(m_id)\n",
    "    elif isinstance(m_id, int) and isinstance(all_movies_vecs[0][0], str):\n",
    "        m_id = str(m_id)\n",
    "\n",
    "    # Lấy input_vec với kiểm tra\n",
    "    vec_list = [r[1] for r in all_movies_vecs if r[0] == m_id]\n",
    "    if not vec_list:\n",
    "        raise ValueError(f\"Không tìm thấy vector cho movie_id {m_id}\")\n",
    "    input_vec = vec_list[0]\n",
    "    \n",
    "    similar_movies_rdd = spark.sparkContext.parallelize(\n",
    "        [(i[0], CosineSim(input_vec, i[1])) for i in all_movies_vecs]\n",
    "    )\n",
    "    \n",
    "    similar_movies_df = spark.createDataFrame(similar_movies_rdd, [\"movies_id\", \"score\"]) \\\n",
    "        .orderBy(col(\"score\").desc()) \\\n",
    "        .filter(col(\"movies_id\") != m_id) \\\n",
    "        .limit(sim_mov_limit)\n",
    "    \n",
    "    similar_movies_df = similar_movies_df.withColumn(\"input_movies_id\", lit(m_id))\n",
    "    return similar_movies_df.toPandas()\n",
    "\n",
    "def getMovieDetails(in_mov):\n",
    "    global df_merge\n",
    "    vote_counts = df_merge.filter(col(\"vote_count\").isNotNull()).select(col(\"vote_count\"))\n",
    "    vote_averages = df_merge.filter(col(\"vote_average\").isNotNull()).select(col(\"vote_average\"))\n",
    "    C = vote_averages.select(mean(\"vote_average\")).collect()[0][0]\n",
    "    quantiles = vote_counts.approxQuantile(\"vote_count\", [0.7], 0.001)\n",
    "    m = quantiles[0]\n",
    "    qualified = df_merge.filter((col(\"vote_count\") >= m) & col(\"vote_count\").isNotNull() & col(\"vote_average\").isNotNull())\n",
    "    qualified = qualified.withColumn(\"vote_count\", col(\"vote_count\").cast(\"int\")) \\\n",
    "        .withColumn(\"vote_average\", col(\"vote_average\").cast(\"int\"))\n",
    "    weighted_rating_udf = udf(lambda v, R: (\n",
    "        v / (v + m) * R) + (m / (m + v) * C), FloatType())\n",
    "    qualified = qualified.withColumn(\"weighted_rating\", weighted_rating_udf(\n",
    "        col(\"vote_count\"), col(\"vote_average\")))\n",
    "    qualified = qualified.orderBy(col(\"weighted_rating\").desc())\n",
    "\n",
    "    if isinstance(in_mov, str):\n",
    "        return \"Invalid input\"\n",
    "    a = in_mov.alias(\"a\")\n",
    "    b = qualified.alias(\"b\")\n",
    "\n",
    "    raw = a.join(b, col(\"a.movies_id\") == col(\"b.id\"), 'inner') \\\n",
    "        .orderBy(\"score\", ascending=False) \\\n",
    "        .select([col('a.' + c) for c in a.columns] + [col('b.title'), col('b.genres_convert'), col('b.keyword_convert'), col(\"b.director\"), col(\"b.cast_names\"), col(\"b.weighted_rating\")])\n",
    "    \n",
    "    return raw.select(\"movies_id\", \"input_movies_id\", \"title\", \"genres_convert\", \"director\", \"cast_names\", \"score\", \"weighted_rating\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     movie_name = input(\"Enter a movie name: \")\n",
    "#     recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "#     print(\"Recommended movies:\")\n",
    "#     print(recommendations)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    #movie_name = input(\"Enter a movie name: \")\n",
    "    movie_name = \"The Box\"\n",
    "    recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "\n",
    "    if isinstance(recommendations, str):  # Nếu không tìm thấy phim\n",
    "        print(recommendations)\n",
    "    else:\n",
    "        print(\"Recommended movies:\")\n",
    "        print(recommendations)\n",
    "\n",
    "        # Chuyển đổi kiểu dữ liệu của Pandas DataFrame về đúng dạng trước khi đưa vào Spark\n",
    "        recommendations = recommendations.astype({\n",
    "            \"movies_id\": int,\n",
    "            \"input_movies_id\": int,\n",
    "            \"score\": float\n",
    "        })\n",
    "\n",
    "        # Xác định schema cho Spark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"movies_id\", IntegerType(), True),\n",
    "            StructField(\"score\", FloatType(), True),\n",
    "            StructField(\"input_movies_id\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        # Chuyển đổi Pandas DataFrame thành danh sách từ điển\n",
    "        recommendations_list = recommendations.to_dict(orient=\"records\")\n",
    "\n",
    "        # Chuyển đổi danh sách từ điển thành Spark DataFrame\n",
    "        recommendations_spark_df = spark.createDataFrame(recommendations_list, schema=schema)\n",
    "        # recommendations_spark_df.show()\n",
    "        \n",
    "        details = getMovieDetails(recommendations_spark_df)\n",
    "\n",
    "\n",
    "        # Hiển thị thông tin chi tiết\n",
    "        print(\"Detailed movie recommendations:\")\n",
    "        print(details.toPandas())  # Chuyển về Pandas DataFrame để hiển thị\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293b78a-10e8-41df-8c87-7fe0c2ae2e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6397eb1-5996-4c01-b9ff-59d9cd517c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda66271-c831-4a83-b944-493f7330580a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
