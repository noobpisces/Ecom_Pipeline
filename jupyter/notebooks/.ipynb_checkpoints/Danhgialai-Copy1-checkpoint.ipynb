{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500a07cd-354f-494a-8602-114cc07107cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8735"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, IntegerType\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit, collect_list, struct\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lakeeeeeeeeeeeee\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 1. Chuẩn bị dữ liệu và broadcast movie vectors\n",
    "CB = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta\")\n",
    "filtered_CB = CB.filter(CB[\"id\"].isNotNull())\n",
    "movie_vec_dict = {int(row.id): row.vecs for row in filtered_CB.select(\"id\", \"vecs\").collect()}\n",
    "b_movie_vec = sc.broadcast(movie_vec_dict)\n",
    "\n",
    "ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "ratings = ratings.withColumn('userId', ratings['userId'].cast('int'))\\\n",
    "                 .withColumn('movieId', ratings['movieId'].cast('int'))\\\n",
    "                 .withColumn('rating', ratings['rating'].cast('float'))\n",
    "train, validation, test = ratings.randomSplit([0.8, 0.1, 0.1], seed=0)\n",
    "# 2. Tải mô hình ALS và dự đoán CF\n",
    "CF_model = ALSModel.load(\"s3a://lakehouse/CF/als_best_model\")\n",
    "PredcitCF = CF_model.transform(ratings)\n",
    "test = test.limit(20000)\n",
    "# 3. Lọc ratings chỉ giữ phim có trong CB\n",
    "ratings_in_cb = test.join(CB.select(F.col(\"id\").alias(\"cb_id\")), \n",
    "                              test.movieId == F.col(\"cb_id\"), \"inner\")\\\n",
    "                       .select(\"userId\", \"movieId\", \"rating\")\n",
    "# 4. Gom lịch sử người dùng thành array\n",
    "\n",
    "ratings_in_cb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06adaf1-6864-47ad-874f-ad5def0b0280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, IntegerType\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit, collect_list, struct,sqrt,avg\n",
    "from pyspark.sql import types as T\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad38475e-ec97-4a6c-8efd-36a92eed72a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lakeeeeeeeeeeeee\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d485ae2-778e-4502-8977-616f430f0413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CB = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/haha\")\n",
    "filtered_CB = CB.filter(CB[\"id\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe08f8aa-c694-45e0-ab6e-836dda2a5473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_vec_dict = {int(row.id): row.vecs for row in filtered_CB.select(\"id\", \"vecs\").collect()}\n",
    "b_movie_vec = sc.broadcast(movie_vec_dict)\n",
    "ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "ratings = ratings.withColumn('userId', ratings['userId'].cast('int')).\\\n",
    "withColumn('movieId', ratings['movieId'].cast('int')).withColumn('rating', ratings['rating'].cast('float'))\n",
    "train, validation, test = ratings.randomSplit([0.8, 0.1, 0.1], seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45199044-4f91-44b6-9620-346a6100f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "CF_model = ALSModel.load(\"s3a://lakehouse/CF/als_best_model\")\n",
    "PredcitCF = CF_model.transform(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4d76f7c-ab65-47b5-9508-79fa2da579af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rating(user_history, target_id):\n",
    "    vec_dict = b_movie_vec.value\n",
    "    target = vec_dict.get(target_id)\n",
    "    if target is None: return 0.0\n",
    "    num, den = 0.0, 0.0\n",
    "    for mid, r in user_history:\n",
    "        v = vec_dict.get(mid)\n",
    "        if v is None: continue\n",
    "        # cosine similarity\n",
    "        sim = float(np.dot(target, v) / (np.linalg.norm(target)*np.linalg.norm(v))) if np.linalg.norm(target)*np.linalg.norm(v)>0 else 0.0\n",
    "        num += sim * r\n",
    "        den += builtins.abs(sim)\n",
    "    return float(num/den) if den>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141628ec-a188-4bd5-8d82-2ba568d1863e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_udf = F.udf(predict_rating, FloatType())\n",
    "ratings_in_cb = (\n",
    "    test\n",
    "    .join(CB.select(F.col(\"id\").alias(\"cb_id\")), test.movieId == F.col(\"cb_id\"), \"inner\")\n",
    "    .select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    ")\n",
    "# 4. Gom lịch sử train thành array per user\n",
    "train_hist = (\n",
    "    ratings_in_cb\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(\n",
    "        collect_list(\n",
    "            struct(\"movieId\",\"rating\")\n",
    "        ).alias(\"history\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Kết hợp sample_ratings (test) với history train\n",
    "eval_df = (\n",
    "    ratings_in_cb\n",
    "    .join(train_hist, on=\"userId\", how=\"left\")\n",
    ")\n",
    "\n",
    "# 6. Dự đoán và tính bình phương sai số\n",
    "eval_df = (\n",
    "    eval_df\n",
    "    .withColumn(\"predicted_rating_CT\",\n",
    "                predict_udf(\"history\", \"movieId\"))\n",
    "    .join(\n",
    "            PredcitCF.select(\"userId\", \"movieId\", \"prediction\").withColumnRenamed(\"prediction\", \"predicted_rating_CF\"),\n",
    "            on=[\"userId\", \"movieId\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee706481-6839-4c6b-aa3a-f041179eb242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha - RMSE:\n",
      "0.00 -> 0.7403\n",
      "0.05 -> 0.7365\n",
      "0.10 -> 0.7338\n",
      "0.15 -> 0.7322\n",
      "0.20 -> 0.7317\n",
      "0.25 -> 0.7324\n",
      "0.30 -> 0.7342\n",
      "0.35 -> 0.7371\n",
      "0.40 -> 0.7411\n",
      "0.45 -> 0.7462\n",
      "0.50 -> 0.7524\n",
      "0.55 -> 0.7596\n",
      "0.60 -> 0.7678\n",
      "0.65 -> 0.7770\n",
      "0.70 -> 0.7871\n",
      "0.75 -> 0.7981\n",
      "0.80 -> 0.8100\n",
      "0.85 -> 0.8228\n",
      "0.90 -> 0.8363\n",
      "0.95 -> 0.8506\n",
      "1.00 -> 0.8656\n",
      "\n",
      "✅ Best alpha = 0.20 with RMSE = 0.7317\n"
     ]
    }
   ],
   "source": [
    "# Chỉ giữ lại các cột cần thiết\n",
    "eval_df = eval_df.select(\"predicted_rating_CT\", \"predicted_rating_CF\", \"rating\")\n",
    "\n",
    "# Cache để tránh tính lại nhiều lần\n",
    "eval_df.cache()\n",
    "eval_df.count()  # Action để trigger cache\n",
    "\n",
    "# Hàm tính RMSE cho mỗi alpha\n",
    "def compute_rmse(alpha):\n",
    "    hybrid_df = eval_df.withColumn(\n",
    "        \"predicted_hybrid\",\n",
    "        lit(alpha) * col(\"predicted_rating_CT\") + (1 - alpha) * col(\"predicted_rating_CF\")\n",
    "    )\n",
    "    rmse = hybrid_df.withColumn(\n",
    "        \"squared_error\", (col(\"predicted_hybrid\") - col(\"rating\")) ** 2\n",
    "    ).agg(sqrt(avg(\"squared_error\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n",
    "    return rmse\n",
    "\n",
    "# Tìm best alpha\n",
    "best_alpha = None\n",
    "best_rmse = float(\"inf\")\n",
    "alpha_rmse_list = []\n",
    "\n",
    "for alpha in np.arange(0.0, 1.05, 0.05):\n",
    "    rmse = compute_rmse(alpha)\n",
    "    alpha_rmse_list.append((alpha, rmse))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_alpha = alpha\n",
    "\n",
    "# In kết quả\n",
    "print(\"Alpha - RMSE:\")\n",
    "for alpha, rmse in alpha_rmse_list:\n",
    "    print(f\"{alpha:.2f} -> {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Best alpha = {best_alpha:.2f} with RMSE = {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269829da-4180-44d3-b323-339d7d06babc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c253f-6fd4-46bd-9d85-b5a3879f01aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d32f6-f951-4e64-a5ce-c43157cf7e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326733f5-30bf-4fa9-99e0-3d847e3d41a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921cc1b-df8e-46da-b0fd-e65f979d6348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24950da2-b62c-42d8-9ac1-059f695e6896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
