{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638f47a1-cae7-428d-a3e0-4b88217860cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099d0057-3dbd-4c6f-af7d-287a2cd5af66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lake\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bb2311-54a4-47d7-9f3d-2adf26c27598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    id|                comb|\n",
      "+------+--------------------+\n",
      "|124057| JohnnyMorina Mal...|\n",
      "|  1572|bomb taxi riddle ...|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_merge_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "silver_merge_df.select(\"id\",\"comb\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fd9ac6-9845-4311-8500-21d120f7c148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45538"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_merge_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc0fd99-ca15-4e34-9c68-30f59150e0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a978c40c-2613-4337-893c-71a677ee11f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df = silver_merge_df.select('id', 'comb')\n",
    "regexTokenizer = RegexTokenizer(\n",
    "    gaps=False, pattern='\\w+', inputCol='comb', outputCol='token')\n",
    "stopWordsRemover = StopWordsRemover(\n",
    "    inputCol='token', outputCol='nostopwrd')\n",
    "# countVectorizer = CountVectorizer(inputCol=\"nostopwrd\", outputCol=\"rawFeature\")\n",
    "# iDF = IDF(inputCol=\"rawFeature\", outputCol=\"idf_vec\")\n",
    "word2Vec = Word2Vec(vectorSize=150, minCount=3,windowSize=10, \n",
    "                    inputCol='nostopwrd', outputCol='word_vec', seed=123)\n",
    "# vectorAssembler = VectorAssembler(inputCols=['idf_vec', 'word_vec'], outputCol='comb_vec')\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, word2Vec])\n",
    "pipeline_model2 = pipeline.fit(spark_df)\n",
    "pipeline_model2.write().overwrite().save(\n",
    "    \"s3a://lakehouse/model/\" + 'pipeline_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7aae55e-6091-405b-9e10-a5a7aa0985ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9defaa8-8b91-4658-990d-f5a6eb798356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"vecs\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "spark_df = silver_merge_df.select('id', 'comb')\n",
    "pipeline_mdl = PipelineModel.load(\"s3a://lakehouse/model/\" + 'pipeline_model2')\n",
    "new_df = pipeline_mdl.transform(spark_df)\n",
    "all_movies_vecs = new_df.select('id', 'word_vec').rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "data = [(id, [float(x) for x in vec]) for id, vec in all_movies_vecs]\n",
    "\n",
    "all_movies_df = spark.createDataFrame(data, schema)\n",
    "all_movies_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/all_movies_delta2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5cdbff-b705-41c3-95c1-0a23e864ba7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merge = None\n",
    "df_list = None\n",
    "flag1 = False\n",
    "flag2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f3c550-f7d3-4f8a-ad27-f3386beec5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def CosineSim(vec1, vec2):\n",
    "#     numerator = np.dot(vec1, vec2)\n",
    "#     denominator = np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2))\n",
    "#     return float(numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af1694e-1f94-4652-9683-4974027ddf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        \n",
    "# def get_titles():\n",
    "#     try:\n",
    "#         global df_merge\n",
    "#         global n_df\n",
    "#         global flag1\n",
    "#         if not flag1:\n",
    "#             spark = init_spark_session()\n",
    "#             df_merge =  spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "#             n_df = df_merge.toPandas()\n",
    "#             flag1 = True\n",
    "#         return list(n_df['title'])\n",
    "#     except Exception as e:\n",
    "#         raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73a48a9-fe36-42ec-9c95-a0dbb9c281ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_vecs():\n",
    "#     try:\n",
    "#         global df_list, all_vecs, flag2\n",
    "#         if not flag2:\n",
    "#             spark = init_spark_session()\n",
    "#             df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta2\")\n",
    "#             # rows_rdd = spark.read.format(\"iceberg\").load(\"hive_prod.gold.transform_model\").rdd.map(lambda row: Row(id=row.id, vecs=Vectors.dense([float(x) for x in row.vecs.strip('[]').split(',')])))\n",
    "#             # new_df = spark.createDataFrame(rows_rdd)\n",
    "#             # df_list = spark.createDataFrame(spark.read.format(\"iceberg\").load(\"hive_prod.gold.transform_model\").rdd.map(\n",
    "#             #     lambda row: Row(id=row.id, vecs=Vectors.dense([float(x) for x in row.vecs.strip('[]').split(',')])))).collect()\n",
    "#             # all_vecs = [(row['id'], row['vecs']) for row in df_list]\n",
    "#             data_original = df_list.collect()\n",
    "#             all_vecs = [(row.id, Vectors.dense(row.vecs))\n",
    "#                         for row in data_original]\n",
    "#             flag2 = True\n",
    "#         return all_vecs\n",
    "#     except Exception as e:\n",
    "#         raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ba679a-d685-4941-8361-232731e5fc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def recommendation(m_title, sim_mov_limit=10):\n",
    "#     global df_merge\n",
    "#     schema = StructType([\n",
    "#         StructField(\"movies_id\", StringType(), True),\n",
    "#         StructField(\"score\", IntegerType(), True),\n",
    "#         StructField(\"input_movies_id\", StringType(), True)\n",
    "#     ])\n",
    "\n",
    "#     if not df_merge.filter(col(\"title\") == m_title).count() > 0:\n",
    "#         return ('Sorry! The movie you searched is not in our database. Please check the spelling or try with some other movies')\n",
    "#     else:\n",
    "#         spark = init_spark_session()\n",
    "#         sc = spark.sparkContext\n",
    "#         similar_movies_df = spark.createDataFrame([], schema)\n",
    "#         all_movies_vecs = get_vecs()\n",
    "#         m_id = df_merge.filter(col(\"title\") == m_title).select(\n",
    "#             col('id')).collect()[0][0]\n",
    "#         input_vec = [(r[1]) for r in all_movies_vecs if r[0] == m_id][0]\n",
    "#         similar_movies_rdd = sc.parallelize(\n",
    "#             [(i[0], float(CosineSim(input_vec, i[1]))) for i in all_movies_vecs], numSlices=5)\n",
    "#         similar_movies_df = spark.createDataFrame(similar_movies_rdd) \\\n",
    "#             .withColumnRenamed('_1', 'movies_id') \\\n",
    "#             .withColumnRenamed('_2', 'score') \\\n",
    "#             .orderBy(\"score\", ascending=False) \\\n",
    "#             .na.drop()\n",
    "#         similar_movies_df = similar_movies_df.filter(\n",
    "#             col(\"movies_id\") != m_id).limit(sim_mov_limit)\n",
    "#         similar_movies_df = similar_movies_df.withColumn(\n",
    "#             'input_movies_id', lit(m_id))\n",
    "#         # spark.stop()\n",
    "#         return similar_movies_df\n",
    "#     # return 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f63a795f-0059-4895-bf0b-d801571c678f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def getMovieDetails(in_mov):\n",
    "#     global df_merge\n",
    "#     vote_counts = df_merge.filter(\n",
    "#         col(\"vote_count\").isNotNull()).select(col(\"vote_count\"))\n",
    "#     vote_averages = df_merge.filter(\n",
    "#         col(\"vote_average\").isNotNull()).select(col(\"vote_average\"))\n",
    "#     C = vote_averages.select(mean(\"vote_average\")).collect()[0][0]\n",
    "#     quantiles = vote_counts.approxQuantile(\"vote_count\", [0.7], 0.001)\n",
    "#     m = quantiles[0]\n",
    "#     qualified = df_merge.filter((col(\"vote_count\") >= m) & col(\n",
    "#         \"vote_count\").isNotNull() & col(\"vote_average\").isNotNull())\n",
    "#     qualified = qualified.withColumn(\"vote_count\", col(\"vote_count\").cast(\"int\")) \\\n",
    "#         .withColumn(\"vote_average\", col(\"vote_average\").cast(\"int\"))\n",
    "#     weighted_rating_udf = udf(lambda v, R: (\n",
    "#         v / (v + m) * R) + (m / (m + v) * C), FloatType())\n",
    "#     qualified = qualified.withColumn(\"weighted_rating\", weighted_rating_udf(\n",
    "#         col(\"vote_count\"), col(\"vote_average\")))\n",
    "#     qualified = qualified.orderBy(col(\"weighted_rating\").desc())\n",
    "\n",
    "#     if type(in_mov) == type('string'):\n",
    "#         return \"f\"\n",
    "#     a = in_mov.alias(\"a\")\n",
    "#     b = qualified.alias(\"b\")\n",
    "\n",
    "#     raw = a.join(b, col(\"a.movies_id\") == col(\"b.id\"), 'inner') \\\n",
    "#         .orderBy(\"score\", ascending=False) \\\n",
    "#         .select([col('a.' + c) for c in a.columns] + [col('b.title'), col('b.genres_convert'), col('b.keyword_convert'), col(\"b.director\"), col(\"b.cast_names\"), col(\"b.weighted_rating\")])\n",
    "#     # return raw.select(\"title\").rdd.flatMap(lambda x: x).collect()\n",
    "#     return raw.select(\"movies_id\", \"input_movies_id\", \"title\", \"genres_convert\", \"director\", \"cast_names\", \"score\", \"weighted_rating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e398e749-c595-4f71-9c6d-1b182e51f218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a movie name:  Toy Story 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies:\n",
      "  movies_id     score input_movies_id\n",
      "0      7518  0.986922             863\n",
      "1      9836  0.985934             863\n",
      "2     14444  0.983440             863\n",
      "3       585  0.982839             863\n",
      "4      9982  0.980047             863\n",
      "Detailed movie recommendations:\n",
      "   movies_id  input_movies_id              title           genres_convert   \n",
      "0       7518              863     Over the Hedge  Comedy Animation Family  \\\n",
      "1       9836              863         Happy Feet         Animation Comedy   \n",
      "2      14444              863  The Rugrats Movie         Animation Family   \n",
      "3        585              863     Monsters, Inc.  Animation Comedy Family   \n",
      "4       9982              863     Chicken Little  Animation Family Comedy   \n",
      "\n",
      "                     director                               cast_names   \n",
      "0  KareyKirkpatrickTimJohnson   BruceWillis GarryShandling SteveCarell  \\\n",
      "1                GeorgeMiller  ElijahWood RobinWilliams BrittanyMurphy   \n",
      "2   NortonVirgienIgorKovalyov      WhoopiGoldberg DavidSpade E.G.Daily   \n",
      "3                  PeteDocter       JohnGoodman BillyCrystal MaryGibbs   \n",
      "4                  MarkDindal           ZachBraff JoanCusack DanMolina   \n",
      "\n",
      "      score  weighted_rating  \n",
      "0  0.986922         5.991635  \n",
      "1  0.985934         5.010429  \n",
      "2  0.983440         5.088827  \n",
      "3  0.982839         6.994406  \n",
      "4  0.980047         5.015471  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, mean, lit, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df_merge = None\n",
    "df_list = None\n",
    "flag1 = False\n",
    "flag2 = False\n",
    "\n",
    "\n",
    "def CosineSim(vec1, vec2):\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2))\n",
    "    return float(numerator / denominator) if denominator != 0 else 0\n",
    "\n",
    "def get_titles():\n",
    "    global df_merge, flag1\n",
    "    if not flag1:\n",
    "        df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "        flag1 = True\n",
    "    return list(df_merge.select(\"title\").toPandas()[\"title\"])\n",
    "\n",
    "def get_vecs():\n",
    "    global df_list, all_vecs, flag2\n",
    "    if not flag2:\n",
    "        df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta2\")\n",
    "        data_original = df_list.collect()\n",
    "        all_vecs = [(row.id, Vectors.dense(row.vecs)) for row in data_original]\n",
    "        flag2 = True\n",
    "    return all_vecs\n",
    "\n",
    "def recommendation(m_title, sim_mov_limit=5):\n",
    "    global df_merge\n",
    "    df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/merge_data-movies/merged_data\")\n",
    "    if df_merge.filter(col(\"title\") == m_title).count() == 0:\n",
    "        return \"Sorry! The movie you searched is not in our database. Please check the spelling or try another movie.\"\n",
    "    \n",
    "    all_movies_vecs = get_vecs()\n",
    "    m_id = df_merge.filter(col(\"title\") == m_title).select(col('id')).collect()[0][0]\n",
    "    input_vec = [r[1] for r in all_movies_vecs if r[0] == m_id][0]\n",
    "    \n",
    "    similar_movies_rdd = spark.sparkContext.parallelize(\n",
    "        [(i[0], CosineSim(input_vec, i[1])) for i in all_movies_vecs]\n",
    "    )\n",
    "    \n",
    "    similar_movies_df = spark.createDataFrame(similar_movies_rdd, [\"movies_id\", \"score\"]) \\\n",
    "        .orderBy(col(\"score\").desc()) \\\n",
    "        .filter(col(\"movies_id\") != m_id) \\\n",
    "        .limit(sim_mov_limit)\n",
    "    \n",
    "    similar_movies_df = similar_movies_df.withColumn(\"input_movies_id\", lit(m_id))\n",
    "    return similar_movies_df.toPandas()\n",
    "\n",
    "def getMovieDetails(in_mov):\n",
    "    global df_merge\n",
    "    vote_counts = df_merge.filter(col(\"vote_count\").isNotNull()).select(col(\"vote_count\"))\n",
    "    vote_averages = df_merge.filter(col(\"vote_average\").isNotNull()).select(col(\"vote_average\"))\n",
    "    C = vote_averages.select(mean(\"vote_average\")).collect()[0][0]\n",
    "    quantiles = vote_counts.approxQuantile(\"vote_count\", [0.7], 0.001)\n",
    "    m = quantiles[0]\n",
    "    qualified = df_merge.filter((col(\"vote_count\") >= m) & col(\"vote_count\").isNotNull() & col(\"vote_average\").isNotNull())\n",
    "    qualified = qualified.withColumn(\"vote_count\", col(\"vote_count\").cast(\"int\")) \\\n",
    "        .withColumn(\"vote_average\", col(\"vote_average\").cast(\"int\"))\n",
    "    weighted_rating_udf = udf(lambda v, R: (\n",
    "        v / (v + m) * R) + (m / (m + v) * C), FloatType())\n",
    "    qualified = qualified.withColumn(\"weighted_rating\", weighted_rating_udf(\n",
    "        col(\"vote_count\"), col(\"vote_average\")))\n",
    "    qualified = qualified.orderBy(col(\"weighted_rating\").desc())\n",
    "\n",
    "    if isinstance(in_mov, str):\n",
    "        return \"Invalid input\"\n",
    "    a = in_mov.alias(\"a\")\n",
    "    b = qualified.alias(\"b\")\n",
    "\n",
    "    raw = a.join(b, col(\"a.movies_id\") == col(\"b.id\"), 'inner') \\\n",
    "        .orderBy(\"score\", ascending=False) \\\n",
    "        .select([col('a.' + c) for c in a.columns] + [col('b.title'), col('b.genres_convert'), col('b.keyword_convert'), col(\"b.director\"), col(\"b.cast_names\"), col(\"b.weighted_rating\")])\n",
    "    \n",
    "    return raw.select(\"movies_id\", \"input_movies_id\", \"title\", \"genres_convert\", \"director\", \"cast_names\", \"score\", \"weighted_rating\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     movie_name = input(\"Enter a movie name: \")\n",
    "#     recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "#     print(\"Recommended movies:\")\n",
    "#     print(recommendations)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    movie_name = input(\"Enter a movie name: \")\n",
    "    recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "\n",
    "    if isinstance(recommendations, str):  # Nếu không tìm thấy phim\n",
    "        print(recommendations)\n",
    "    else:\n",
    "        print(\"Recommended movies:\")\n",
    "        print(recommendations)\n",
    "\n",
    "        # Chuyển đổi kiểu dữ liệu của Pandas DataFrame về đúng dạng trước khi đưa vào Spark\n",
    "        recommendations = recommendations.astype({\n",
    "            \"movies_id\": int,\n",
    "            \"input_movies_id\": int,\n",
    "            \"score\": float\n",
    "        })\n",
    "\n",
    "        # Xác định schema cho Spark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"movies_id\", IntegerType(), True),\n",
    "            StructField(\"score\", FloatType(), True),\n",
    "            StructField(\"input_movies_id\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        # Chuyển đổi Pandas DataFrame thành danh sách từ điển\n",
    "        recommendations_list = recommendations.to_dict(orient=\"records\")\n",
    "\n",
    "        # Chuyển đổi danh sách từ điển thành Spark DataFrame\n",
    "        recommendations_spark_df = spark.createDataFrame(recommendations_list, schema=schema)\n",
    "        # recommendations_spark_df.show()\n",
    "        details = getMovieDetails(recommendations_spark_df)\n",
    "\n",
    "\n",
    "        # Hiển thị thông tin chi tiết\n",
    "        print(\"Detailed movie recommendations:\")\n",
    "        print(details.toPandas())  # Chuyển về Pandas DataFrame để hiển thị\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6513f1eb-a765-45ce-8238-917ce847b866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='948', comb='femalenudity nudity mask babysitter halloween police psychopathickiller independentfilm stalking serialkiller marijuana maskedkiller blood potsmoking slasher murderer throatslitting maniac violence killingspree family cultfilm evil psychotic smokingpot institution escapedkiller DonaldPleasence JamieLeeCurtis P.J.Soles JohnCarpenter Horror Thriller')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = spark_df.filter(col('id') == 948)\n",
    "d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87cb7a-7826-4036-b746-c4576cb2fd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092a53f-905f-4bd3-84c6-643e2e1c8dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
