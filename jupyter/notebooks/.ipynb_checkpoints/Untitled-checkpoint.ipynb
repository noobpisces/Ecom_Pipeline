{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e161494-f6b9-43e8-877e-b6ce383b4c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd2bb9e-78b1-4df0-8bfe-829cc68700e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  explode,col, expr,when,to_date, sum, from_json,size,length, lit, to_timestamp,current_timestamp, max\n",
    "from pyspark.sql.types import  ArrayType,StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType, LongType\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, explode, to_date, date_format,\n",
    "    dayofweek, dayofmonth, dayofyear, weekofyear,\n",
    "    month, quarter, year, when, unix_timestamp\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccc049e-6b25-40a7-a9f9-ef572f840c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lake\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f68928-ca66-4eb9-ac81-c383ad2cd103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/Bronze_Crews_API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001311c5-0e08-48c7-ad3d-254d99ff1d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc76322-be1a-4641-a096-7a831013681b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e91d7-487d-4355-915e-58ded27a5973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bf7d1-5d00-410e-b6e4-af7c77b241eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc84fe-1036-4f5f-9b81-dd0ca520c75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3972da96-f8cb-4957-9758-06155286b8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_processed = df.filter(df.processed == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "166b042c-7def-4521-9d67-10cb77053d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea4c4ff2-8ed1-46fe-b3a2-89d9d6cb244f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042f953-c886-45ba-a575-c3801a264805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c01903-fd5d-4490-a347-91cbdc8e8d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009a9f1-269c-43f5-89a8-214de537efb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b144c661-9dab-4959-9641-386df59cad68",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1963bde7-ec66-4bc5-be36-31fa53553087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enable-non-concurrent-writes\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/24 20:12:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/24 20:12:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/24 20:12:10 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:467)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:218)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1(SparkSession.scala:1221)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1$adapted(SparkSession.scala:1219)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1219)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:106)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/24 20:12:11 WARN SharedState: Cannot qualify the warehouse path, leaving it unqualified.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:282)\n",
      "\tat org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:212)\n",
      "\tat org.apache.spark.sql.SparkSession.range(SparkSession.scala:554)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 32 more\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'date_add(to_date('1800-01-01'), id)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'id' is of bigint type.; line 1 pos 0;\n'Project [date_add(to_date(1800-01-01, None, Some(Etc/UTC)), id#0L) AS date#2]\n+- Range (0, 146098, step=1, splits=Some(8))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m days_diff \u001b[38;5;241m=\u001b[39m (end_date \u001b[38;5;241m-\u001b[39m start_date)\u001b[38;5;241m.\u001b[39mdays\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 3. Tạo DataFrame gồm các ngày liên tiếp, dùng sequence số nguyên đại diện cho ngày kể từ start_date\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m df_dates \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays_diff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_add(to_date(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstart_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m), id)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 4. Trích xuất các trường tương tự như trước\u001b[39;00m\n\u001b[1;32m     67\u001b[0m df_dim_date \u001b[38;5;241m=\u001b[39m df_dates\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     68\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_date\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     69\u001b[0m     F\u001b[38;5;241m.\u001b[39mdayofweek(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDayOfWeek\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonthOfYear\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDayOfMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'date_add(to_date('1800-01-01'), id)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'id' is of bigint type.; line 1 pos 0;\n'Project [date_add(to_date(1800-01-01, None, Some(Etc/UTC)), id#0L) AS date#2]\n+- Range (0, 146098, step=1, splits=Some(8))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  explode,col, expr,when,to_date, sum, from_json,size,length\n",
    "from pyspark.sql.types import  ArrayType,StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lake\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# df = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/movies\")\n",
    "# df = df.withColumn(\"parsed_date\", F.to_date(F.col(\"release_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# result = df.select(\n",
    "#     F.col(\"release_date\"),\n",
    "#     F.dayofweek(\"parsed_date\").alias(\"DayOfWeek\"),\n",
    "#     F.date_format(\"parsed_date\", \"EEEE\").alias(\"DayName\"),\n",
    "#     F.dayofmonth(\"parsed_date\").alias(\"DayOfMonth\"),\n",
    "#     F.dayofyear(\"parsed_date\").alias(\"DayOfYear\"),\n",
    "#     F.weekofyear(\"parsed_date\").alias(\"WeekOfYear\"),\n",
    "#     F.date_format(\"parsed_date\", \"MMMM\").alias(\"MonthName\"),\n",
    "#     F.month(\"parsed_date\").alias(\"MonthOfYear\"),\n",
    "#     F.quarter(\"parsed_date\").alias(\"Quarter\"),\n",
    "#     F.year(\"parsed_date\").alias(\"Year\"),\n",
    "#     F.when((F.dayofweek(\"parsed_date\") >= 2) & (F.dayofweek(\"parsed_date\") <= 6), True).otherwise(False).alias(\"IsWeekDay\")\n",
    "# )\n",
    "# result = result.withColumn(\n",
    "#     \"date_id\",\n",
    "#     (F.col(\"Year\") * 10000 + F.col(\"MonthOfYear\") * 100 + F.col(\"DayOfMonth\"))\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#     dimdate = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/dim_date\")\n",
    "#     dimdate.alias(\"target\").merge(\n",
    "#         result.alias(\"source\"),\n",
    "#         \"target.date_id = source.date_id\"\n",
    "#     ).whenNotMatchedInsertAll().execute()\n",
    "# except:\n",
    "#     result.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/gold/dim_date\")\n",
    "\n",
    "start_date = datetime.date(1800, 1, 1)\n",
    "end_date = datetime.date(2200, 1, 1)\n",
    "\n",
    "# 2. Tính số ngày giữa 2 ngày\n",
    "days_diff = (end_date - start_date).days\n",
    "\n",
    "# 3. Tạo DataFrame gồm các ngày liên tiếp, dùng sequence số nguyên đại diện cho ngày kể từ start_date\n",
    "df_dates = spark.range(0, days_diff + 1).select(\n",
    "    (F.expr(f\"date_add(to_date('{start_date}'), id)\")).alias(\"date\")\n",
    ")\n",
    "\n",
    "# 4. Trích xuất các trường tương tự như trước\n",
    "df_dim_date = df_dates.select(\n",
    "    F.col(\"date\").alias(\"full_date\"),\n",
    "    F.dayofweek(\"date\").alias(\"DayOfWeek\"),\n",
    "    F.date_format(\"date\", \"EEEE\").alias(\"DayName\"),\n",
    "    F.dayofmonth(\"date\").alias(\"DayOfMonth\"),\n",
    "    F.dayofyear(\"date\").alias(\"DayOfYear\"),\n",
    "    F.weekofyear(\"date\").alias(\"WeekOfYear\"),\n",
    "    F.date_format(\"date\", \"MMMM\").alias(\"MonthName\"),\n",
    "    F.month(\"date\").alias(\"MonthOfYear\"),\n",
    "    F.quarter(\"date\").alias(\"Quarter\"),\n",
    "    F.year(\"date\").alias(\"Year\"),\n",
    "    F.when((F.dayofweek(\"date\") >= 2) & (F.dayofweek(\"date\") <= 6), True).otherwise(False).alias(\"IsWeekDay\")\n",
    ").withColumn(\n",
    "    \"date_id\",\n",
    "    F.col(\"Year\") * 10000 + F.col(\"MonthOfYear\") * 100 + F.col(\"DayOfMonth\")\n",
    ")\n",
    "df_dim_date = df_dim_date.dropDuplicates([\"date_id\"])\n",
    "# 5. Ghi dữ liệu vào Delta Lake bảng dim_date\n",
    "try:\n",
    "    dimdate = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/dim_date\")\n",
    "    dimdate.alias(\"target\").merge(\n",
    "        df_dim_date.alias(\"source\"),\n",
    "        \"target.date_id = source.date_id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except:\n",
    "    df_dim_date.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/gold/dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82231349-d851-4d6e-8c62-f9b15aaa99d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438ee2f-b726-49aa-96c2-ac8948abe459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b2c24-e202-4b8b-8ae7-b05c6f88aac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c33633-981e-46f8-9a19-9daff6ab542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5089e-c945-4c1c-a7f5-fa88ebecc7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c1629-fc58-472f-a95f-c782c2d87760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7d4ef48-f45f-4a22-a8a2-71209639d3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/bronze/Bronze_API_1\")\n",
    "df = df.select(\n",
    "    col(\"id\").cast(\"integer\"),\n",
    "    col(\"budget\").cast(\"integer\"),\n",
    "    col(\"popularity\").cast(\"double\").alias(\"popularity\"),\n",
    "    col(\"revenue\").cast(\"double\").alias(\"revenue\"),\n",
    "    col(\"vote_average\").cast(\"double\").alias(\"vote_average\"),\n",
    "    col(\"vote_count\").cast(\"double\").alias(\"vote_count\"),\n",
    "    date_format(col(\"release_date\"), \"yyyyMMdd\").cast(\"integer\").alias(\"date_id\"),\n",
    "    col(\"title\"),                                                # Giữ nguyên kiểu string\n",
    "    col(\"original_title\"),                                       # Giữ nguyên kiểu string\n",
    "    col(\"original_language\").alias(\"language\"),                  # Đổi tên trường: original_language -> language\n",
    "    col(\"overview\"),                                             # Giữ nguyên kiểu string\n",
    "    col(\"runtime\").cast(\"double\").alias(\"runtime\"),              # Ép về double\n",
    "    col(\"tagline\"),                                              # Giữ nguyên kiểu string\n",
    "    col(\"status\"),                                               # Giữ nguyên kiểu string\n",
    "    col(\"homepage\"),\n",
    "    col(\"genres\"),\n",
    "    col(\"release_date\"),\n",
    "    col(\"read_time\")\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    readTime = spark.read.format(\"delta\").load(\"s3a://lakehouse/ReadTime\")\n",
    "except:\n",
    "    spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta.`s3a://your-bucket/processing_state` (\n",
    "    task_id STRING,\n",
    "    last_read_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "    spark.sql(\"\"\"\n",
    "    INSERT INTO delta.`s3a://lakehouse/ReadTime`\n",
    "    VALUES ('BatchApi_Process', '1970-01-01 00:00:00')\n",
    "    \"\"\")\n",
    "    readTime = spark.read.format(\"delta\").load(\"s3a://lakehouse/ReadTime\")\n",
    "\n",
    "result = readTime.filter(f\"task_id = 'BatchApi_Process'\").select(\"last_read_time\").collect()\n",
    "last_read_time = result[0][0]\n",
    "\n",
    "# last_read_time = Variable.get(\"last_read_time\", default_var=\"1970-01-01T00:00:00\")\n",
    "# last_read_time_ts = to_timestamp(lit(last_read_time), \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "# df = df.filter(\n",
    "#                                 (col(\"budget\") != 0) &            # loại bỏ dòng có budget là \"0\" (kiểu string)\n",
    "#                                 (col(\"id\") != 0) &           # loại bỏ dòng có id là null\n",
    "#                                 (col(\"revenue\") != 0) &             # loại bỏ dòng có revenue bằng 0\n",
    "#                                 (col(\"vote_average\") != 0) &        # loại bỏ dòng có vote_average bằng 0\n",
    "#                                 (col(\"vote_count\") != 0) &          # loại bỏ dòng có vote_count bằng 0\n",
    "#                                 (col(\"popularity\") != 0) &        # loại bỏ dòng có popularity là \"0\" (kiểu string)\n",
    "#                                 (col(\"date_id\") != 0) & # loại bỏ dòng có release_date là null\n",
    "#                                 (col(\"runtime\") != 0) &               # loại bỏ dòng có runtime bằng 0\n",
    "#                                 (col(\"read_time\") > last_read_time_ts)\n",
    "#                             )\n",
    "# df = df.filter(\n",
    "#                                 (col(\"read_time\") >= last_read_time_ts)\n",
    "#                             )\n",
    "df = df.filter(f\"read_time > '{last_read_time}'\")\n",
    "# last_read_time = Variable.get(\"last_read_time\", default_var=\"1970-01-01T00:00:00\")\n",
    "\n",
    "# # Chuyển đổi thành kiểu timestamp nếu cần\n",
    "# filtered_df = df.filter(col(\"read_time\") > lit(last_read_time).cast(\"timestamp\"))\n",
    "\n",
    "try:\n",
    "    tb = DeltaTable.forPath(spark, \"s3a://lakehouse/silver/Silver_API_1\")\n",
    "    tb.alias(\"target\").merge(\n",
    "        df.alias(\"source\"),\n",
    "        \"target.id = source.id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except:\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver/Silver_API_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf950bba-4e80-4f3e-b511-8329999e05d2",
   "metadata": {},
   "source": [
    "# SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e1a6ba-a5a9-475a-abd7-4a9d80528964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/Silver_API_1\")\n",
    "# last_read_time = Variable.get(\"last_read_time_SG\", default_var=\"1970-01-01T00:00:00\")\n",
    "# last_read_time_ts = to_timestamp(lit(last_read_time), \"yyyy-MM-dd HH:mm:ss\")\n",
    "try:\n",
    "    readTime = spark.read.format(\"delta\").load(\"s3a://lakehouse/ReadTime\")\n",
    "except:\n",
    "    spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta.`s3a://your-bucket/processing_state` (\n",
    "    task_id STRING,\n",
    "    last_read_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "    spark.sql(\"\"\"\n",
    "    INSERT INTO delta.`s3a://lakehouse/ReadTime`\n",
    "    VALUES ('BatchApi_Process', '1970-01-01 00:00:00')\n",
    "    \"\"\")\n",
    "    readTime = spark.read.format(\"delta\").load(\"s3a://lakehouse/ReadTime\")\n",
    "\n",
    "result = readTime.filter(f\"task_id = 'BatchApi_Process'\").select(\"last_read_time\").collect()\n",
    "last_read_time = result[0][0]\n",
    "\n",
    "df = df.filter(f\"read_time > '{last_read_time}'\")\n",
    "# --------------------------------------------------\n",
    "# FACT TABLE: fact_movie\n",
    "# --------------------------------------------------\n",
    "fact_movie_df = df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"budget\"),\n",
    "    col(\"popularity\"),\n",
    "    col(\"revenue\"),\n",
    "    col(\"vote_average\"),\n",
    "    col(\"vote_count\"),\n",
    "    col(\"date_id\")\n",
    ").dropDuplicates([\"id\"])\n",
    "\n",
    "try:\n",
    "\n",
    "    fact_movie = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/fact_movies\")\n",
    "\n",
    "    fact_movie.alias(\"target\").merge(\n",
    "        fact_movie_df.alias(\"source\"),\n",
    "        \"target.id = source.id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except :\n",
    "    fact_movie_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/fact_movies\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DIMENSION TABLE: dim_movie\n",
    "# Lưu ý: Đổi tên các cột để khớp với schema của Delta table hiện có:\n",
    "# Schema mong đợi: id, title, original_title, language, overview, runtime, tagline, status, homepage\n",
    "# --------------------------------------------------\n",
    "dimmovie_df = df.select(\n",
    "    col(\"id\"),                        # Ép sang long\n",
    "    col(\"title\"),                                                # Giữ nguyên kiểu string\n",
    "    col(\"original_title\"),                                       # Giữ nguyên kiểu string\n",
    "    col(\"language\"),                  # Đổi tên trường: original_language -> language\n",
    "    col(\"overview\"),                                             # Giữ nguyên kiểu string\n",
    "    col(\"runtime\"),              # Ép về double\n",
    "    col(\"tagline\"),                                              # Giữ nguyên kiểu string\n",
    "    col(\"status\"),                                               # Giữ nguyên kiểu string\n",
    "    col(\"homepage\")                                              # Giữ nguyên kiểu string\n",
    ").dropDuplicates([\"id\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    dimmovie = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/dim_movie\")\n",
    "    dimmovie.alias(\"target\").merge(\n",
    "        dimmovie_df.alias(\"source\"),\n",
    "        \"target.id = source.id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except:\n",
    "    dimmovie_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/dim_movie\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DIMENSION TABLE: dim_date\n",
    "# --------------------------------------------------\n",
    "dimdate_df = df.withColumn(\"release_date\", to_date(col(\"release_date\"), \"yyyy-MM-dd\")) \\\n",
    "    .select(\n",
    "        date_format(col(\"release_date\"), \"yyyy-MM-dd\").alias(\"release_date\"),\n",
    "        dayofweek(col(\"release_date\")).alias(\"DayOfWeek\"),\n",
    "        date_format(col(\"release_date\"), \"EEEE\").alias(\"DayName\"),\n",
    "        dayofmonth(col(\"release_date\")).alias(\"DayOfMonth\"),\n",
    "        dayofyear(col(\"release_date\")).alias(\"DayOfYear\"),\n",
    "        weekofyear(col(\"release_date\")).alias(\"WeekOfYear\"),\n",
    "        date_format(col(\"release_date\"), \"MMMM\").alias(\"MonthName\"),\n",
    "        month(col(\"release_date\")).alias(\"MonthOfYear\"),\n",
    "        quarter(col(\"release_date\")).alias(\"Quarter\"),\n",
    "        year(col(\"release_date\")).alias(\"Year\"),\n",
    "        when(dayofweek(col(\"release_date\")).between(2, 6), True).otherwise(False).alias(\"IsWeekDay\"),\n",
    "        col(\"date_id\")\n",
    "    ).dropDuplicates([\"date_id\"])\n",
    "\n",
    "try:\n",
    "    dimdate = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/dim_date\")\n",
    "    dimdate.alias(\"target\").merge(\n",
    "        dimdate_df.alias(\"source\"),\n",
    "        \"target.date_id = source.date_id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except:\n",
    "    dimdate_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/dim_date\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DIMENSION TABLE: dim_genre\n",
    "# --------------------------------------------------\n",
    "dim_genre_df = df.select(\n",
    "    explode(col(\"genres\")).alias(\"genre\")\n",
    ").select(\n",
    "    col(\"genre.id\").cast(\"integer\").alias(\"id\"),\n",
    "    col(\"genre.name\").alias(\"name\")\n",
    ").dropDuplicates([\"id\"])\n",
    "\n",
    "try:\n",
    "    dim_genre = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/dim_genre\")\n",
    "    dim_genre.alias(\"target\").merge(\n",
    "        dim_genre_df.alias(\"source\"),\n",
    "        \"target.id = source.id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except:\n",
    "    dim_genre_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/dim_genre\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# BRIDGE TABLE: movie_genres\n",
    "# --------------------------------------------------\n",
    "movie_genre_df = df.select(\n",
    "    col(\"id\").alias(\"movie_id\"),\n",
    "    explode(col(\"genres\")).alias(\"genre\")\n",
    ").select(\n",
    "    col(\"genre.id\").cast(\"integer\").alias(\"genres_id\"),\n",
    "    col(\"movie_id\").alias(\"id\")\n",
    ")\n",
    "\n",
    "try:\n",
    "    movie_genre = DeltaTable.forPath(spark, \"s3a://lakehouse/gold/movie_genre\")\n",
    "    movie_genre.alias(\"target\").merge(\n",
    "        movie_genre_df.alias(\"source\"),\n",
    "        \"target.id = source.id AND target.genres_id = source.genres_id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except:\n",
    "    movie_genre_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/movie_genres\")\n",
    "    \n",
    "max_read_time_row = df.agg(max(\"read_time\")).collect()\n",
    "max_read_time = max_read_time_row[0][\"max(read_time)\"] if max_read_time_row else None\n",
    "\n",
    "\n",
    "readTime = spark.read.format(\"delta\").load(\"s3a://lakehouse/ReadTime\")\n",
    "    \n",
    "    # Cập nhật hoặc chèn bản ghi mới\n",
    "updated_df = readTime.filter(f\"task_id != 'BatchApi_Process'\").union(\n",
    "    spark.createDataFrame([(\"BatchApi_Process\", max_read_time)], [\"task_id\", \"last_read_time\"])\n",
    ")\n",
    "\n",
    "# Ghi đè Delta Table\n",
    "updated_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/ReadTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3129995-4d5b-4721-8889-1269741bd361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd17fa4a-988f-43f3-9d34-479dd279f223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/bronze/Bronze_Movies_API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b26c5-978e-468c-95dc-a39749ddfc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e1bffd-3768-4b80-b27a-30e3cb9aa700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- budget: integer (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: integer (nullable = true)\n",
      " |-- release_date: date (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- read_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386173a4-dfb8-467e-882c-f57189b34fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- budget: integer (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: integer (nullable = true)\n",
      " |-- release_date: date (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- read_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/bronze/Bronze_API_1\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbf88e9-d704-48af-af36-dde74d0db718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-23 14:01:39.153000\n"
     ]
    }
   ],
   "source": [
    "readTime = spark.read.format(\"delta\").load(\"s3a://lakehouse/ReadTime\")\n",
    "result = readTime.filter(f\"task_id = 'BatchApi_Process'\").select(\"last_read_time\").collect()\n",
    "last_read_time = result[0][0]\n",
    "print(last_read_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b54530e-b9cc-4ad6-9efb-8572769f7271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 14, 44000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 45, 127000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 2, 63000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 29, 120000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 46, 264000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 32, 966000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 16, 369000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 17, 641000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 55, 317000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 0, 555000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 36, 249000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 27, 376000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 25, 633000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 12, 228000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 12, 228000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 12, 228000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 12, 228000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 51, 440000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 15, 281000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 15, 281000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 15, 281000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 13, 303000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 13, 303000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 13, 303000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 35, 523000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 35, 523000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 35, 523000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 35, 523000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 37, 793000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 37, 793000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 37, 793000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 38, 230000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 56, 537000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 56, 537000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 49, 593000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 3, 343000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 29, 41, 926000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 24, 417000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 55, 371000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 55, 371000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 55, 371000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 55, 371000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 3, 526000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 3, 526000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 3, 526000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 3, 526000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 23, 619000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 23, 619000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 23, 619000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 23, 619000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 36, 155000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 13, 665000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 13, 665000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 13, 665000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 13, 665000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 38, 348000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 38, 348000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 38, 348000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 38, 348000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 0, 266000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 41, 436000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 4, 178000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 4, 178000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 4, 178000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 4, 178000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 44, 742000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 44, 742000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 44, 742000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 8, 70000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 8, 70000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 8, 70000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 8, 70000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 26, 344000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 15, 385000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 40, 981000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 51, 708000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 51, 708000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 32, 495000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 32, 495000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 32, 495000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 15, 994000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 36, 282000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 36, 282000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 39, 605000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 10, 250000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 10, 250000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 26, 527000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 26, 527000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 26, 527000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 29, 0, 721000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 58, 609000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 58, 609000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 58, 609000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 23, 982000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 23, 982000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 23, 982000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 25, 590000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 25, 590000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 25, 590000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 25, 590000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 54, 805000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 54, 859000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 54, 859000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 20, 498000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 20, 498000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 20, 498000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 20, 498000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 44, 536000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 44, 536000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 44, 536000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 10, 537000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 46, 248000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 46, 248000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 46, 248000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 46, 248000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 7, 354000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 55, 427000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 55, 427000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 55, 427000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 55, 427000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 14, 144000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 22, 147000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 22, 147000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 22, 147000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 22, 147000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 39, 202000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 27, 782000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 27, 782000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 27, 782000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 27, 782000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 13, 705000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 13, 705000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 13, 705000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 17, 82000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 16, 675000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 16, 675000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 16, 675000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 16, 675000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 17, 700000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 17, 700000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 17, 700000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 17, 700000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 29, 31, 631000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 15, 134000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 15, 134000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 15, 134000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 40, 975000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 40, 975000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 40, 975000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 40, 975000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 24, 560000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 3, 861000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 1, 617000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 1, 617000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 47, 519000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 47, 519000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 5, 881000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 5, 881000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 12, 661000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 12, 661000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 12, 661000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 5, 194000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 5, 194000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 15, 659000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 15, 659000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 15, 659000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 15, 659000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 47, 19000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 47, 19000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 47, 19000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 53, 191000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 53, 191000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 53, 191000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 53, 191000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 56, 0, 921000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 56, 0, 921000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 56, 0, 921000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 30, 715000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 59, 678000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 59, 678000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 14, 59, 678000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 8, 711000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 8, 711000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 5, 405000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 5, 405000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 23, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 23, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 23, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 48, 154000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 48, 154000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 48, 154000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 48, 154000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 55, 657000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 55, 657000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 55, 657000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 53, 157000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 53, 157000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 47, 929000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 49, 882000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 58, 299000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 26, 204000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 49, 309000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 49, 309000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 49, 309000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 30, 774000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 30, 774000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 20, 886000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 58, 927000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 19, 64000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 22, 687000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 22, 687000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 5, 639000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 57, 78000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 57, 78000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 13, 670000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 34, 171000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 6, 643000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 6, 643000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 6, 643000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 56, 806000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 56, 806000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 56, 806000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 41, 394000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 41, 394000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 46, 483000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 46, 483000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 46, 483000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 20, 57000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 20, 57000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 20, 57000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 20, 57000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 37, 970000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 37, 970000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 28, 992000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 26, 162000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 26, 162000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 26, 162000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 41, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 41, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 41, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 41, 506000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 29, 418000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 29, 418000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 29, 418000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 4, 479000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 4, 479000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 4, 479000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 53, 61000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 12, 417000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 12, 417000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 18, 437000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 18, 437000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 55, 404000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 34, 814000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 34, 814000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 43, 250000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 24, 150000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 24, 150000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 24, 150000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 32, 478000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 17, 588000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 17, 588000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 46, 756000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 57, 821000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 57, 821000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 57, 821000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 42, 873000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 42, 873000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 42, 873000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 34, 231000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 34, 231000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 34, 231000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 34, 231000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 2, 132000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 1, 632000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 29, 136000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 29, 136000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 29, 136000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 20, 859000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 20, 859000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 20, 486000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 27, 11000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 27, 11000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 8, 46000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 8, 46000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 4, 58000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 56, 355000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 56, 355000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 56, 355000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 56, 355000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 36, 476000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 42, 687000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 42, 687000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 42, 687000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 37, 664000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 32, 627000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 32, 627000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 6, 818000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 57, 40000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 6, 995000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 6, 995000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 6, 995000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 53, 91000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 17, 948000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 17, 948000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 54, 17, 948000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 28, 34, 42000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 28, 34, 42000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 28, 34, 42000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 28, 34, 42000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 28, 50, 279000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 14, 290000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 22, 213000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 22, 213000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 22, 213000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 51, 843000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 34, 856000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 15, 911000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 15, 911000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 15, 911000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 29, 21, 341000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 52, 533000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 35, 93000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 35, 93000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 6, 31, 169000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 3, 810000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 3, 810000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 3, 810000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 59, 992000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 59, 992000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 42, 739000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 49, 960000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 51, 213000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 51, 213000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 15, 5, 120000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 5, 770000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 34, 769000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 53, 565000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 51, 43, 107000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 22, 197000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 21, 212000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 1, 11, 923000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 1, 871000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 1, 871000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 23, 891000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 7, 43, 271000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 22, 745000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 59, 818000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 24, 948000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 24, 948000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 5, 72000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 7, 122000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 9, 7, 122000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 31, 323000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 59, 31, 323000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 32, 964000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 4, 38, 955000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 22, 10, 857000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 27, 943000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 29, 11, 36000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 8, 57, 108000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 29, 330000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 0, 10, 89000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 55, 48, 494000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 12, 406000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 5, 8, 772000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 15, 357000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 45, 99000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 29, 52, 232000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 2, 44, 462000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 3, 46, 183000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 53, 47, 70000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 15, 52, 53, 535000)),\n",
       " Row(read_time=datetime.datetime(2025, 2, 22, 16, 21, 50, 282000))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"read_time\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9608812b-9ec2-48c2-aa88-22805245ab63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genres_id: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/movie_genre\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fd2a9c5-7443-4760-ba75-9c4d89df39db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "def connect_minio():\n",
    "    try:\n",
    "        s3_client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=\"conbo123\",\n",
    "            aws_secret_access_key=\"123conbo\",\n",
    "            endpoint_url='http://minio:9000' \n",
    "        )\n",
    "        return s3_client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error connecting to MinIO: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def get_data_from_raw(name):\n",
    "    try:\n",
    "        client = connect_minio()\n",
    "        response = client.get_object(Bucket=\"lakehouse\", Key=f'raw/{name}.csv')\n",
    "        df = pd.read_csv(response.get(\"Body\"), low_memory=False)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting data from MinIO: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Hàm ghi dữ liệu vào MinIO\n",
    "def save_data_to_bronze(df, name):\n",
    "    try:\n",
    "        client = connect_minio()\n",
    "        # Sử dụng BytesIO để lưu trữ dữ liệu dưới dạng Parquet\n",
    "        parquet_buffer = BytesIO()\n",
    "        df.to_parquet(parquet_buffer, index=False)\n",
    "        parquet_buffer.seek(0)  # Reset buffer position\n",
    "        client.put_object(Bucket=\"lakehouse\", Key=f'bronze/{name}.parquet', Body=parquet_buffer.getvalue())\n",
    "        logging.info(f\"Data saved to bronze/{name}.parquet successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to MinIO: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035a62d-c961-4814-bd08-d79b894ca427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = get_data_from_raw('ratings')\n",
    "save_data_to_bronze(df, 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd27905-cdbe-45ac-b805-734ca2045a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"s3a://lakehouse/raw/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047c82af-46a6-4885-b85c-5b20a2a8674f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"s3a://lakehouse/bronze/ratings.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15fb28-b141-4225-80e1-d71cabcd256d",
   "metadata": {},
   "source": [
    "# DIM MOVIE\n",
    "### [\"movie_id\", \"title\", \"original_title\", \"language\", \"overview\",\"runtime\", \"tagline\", \"status\", \"homepage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30f67fbb-35b7-4e17-b6d9-df48393c1c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_silver_movies = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92d7d7ad-6442-49c6-b732-bc7e375d88b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adult: string (nullable = true)\n",
      " |-- belongs_to_collection: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver_movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5a303e-774c-44ba-90cc-179ad9e7c04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_silver_movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_silver_movies \u001b[38;5;241m=\u001b[39m \u001b[43mdf_silver_movies\u001b[49m\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelease_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbudget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m df_silver_movies \u001b[38;5;241m=\u001b[39m df_silver_movies\u001b[38;5;241m.\u001b[39mdropDuplicates()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_silver_movies' is not defined"
     ]
    }
   ],
   "source": [
    "df_silver_movies = df_silver_movies.dropna(subset=[\"title\", \"release_date\", \"budget\"])\n",
    "df_silver_movies = df_silver_movies.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a59d7ae-4cf5-44c4-a52e-9b4c2dd51461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_silver_movies = df_silver_movies.withColumn(\"id\", col(\"id\").cast(LongType()))\n",
    "\n",
    "df_dim_movie = df_silver_movies.select(\n",
    "    \"id\",  \n",
    "    \"title\", \"original_title\", col(\"spoken_languages\").alias(\"language\"), \"overview\", \n",
    "    \"runtime\", \"tagline\", \"status\", \"homepage\"\n",
    ")\n",
    "df_dim_movie.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/dim_movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3809e-1a6f-4181-beef-b461f2c9a1cf",
   "metadata": {},
   "source": [
    "# DIM KEYWORD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6f8249-bea8-4844-9849-7c5df53c722c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id   |keywords                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|13685|[{'id': 380, 'name': 'brother brother relationship'}, {'id': 642, 'name': 'robbery'}, {'id': 1437, 'name': 'burglar'}, {'id': 2568, 'name': 'language barrier'}, {'id': 2669, 'name': 'motel'}, {'id': 10183, 'name': 'independent film'}, {'id': 10323, 'name': 'psychiatric hospital'}, {'id': 13097, 'name': 'maid'}, {'id': 14604, 'name': 'theft'}, {'id': 15274, 'name': 'nervous breakdown'}, {'id': 187710, 'name': 'escapade'}, {'id': 187717, 'name': 'laundry room'}]|\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/keywords.parquet\")\n",
    "df = df.filter((col(\"keywords\").isNotNull()) & (col(\"keywords\") != \"[]\"))\n",
    "df.filter(col(\"id\") == 13685).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b51d3a-e117-4d63-a842-98f9ddeeb20a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|name                    |id    |\n",
      "+------------------------+------+\n",
      "|jealousy                |931   |\n",
      "|toy                     |4290  |\n",
      "|boy                     |5202  |\n",
      "|friendship              |6054  |\n",
      "|friends                 |9713  |\n",
      "|rivalry                 |9823  |\n",
      "|boy next door           |165503|\n",
      "|new toy                 |170722|\n",
      "|toy comes to life       |187065|\n",
      "|board game              |10090 |\n",
      "|disappearance           |10941 |\n",
      "|based on children's book|15101 |\n",
      "|new home                |33467 |\n",
      "|recluse                 |158086|\n",
      "|giant insect            |158091|\n",
      "|fishing                 |1495  |\n",
      "|best friend             |12392 |\n",
      "|duringcreditsstinger    |179431|\n",
      "|old men                 |208510|\n",
      "|based on novel          |818   |\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/keywords.parquet\")\n",
    "keyword_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df.withColumn(\"keywords\", from_json(col(\"keywords\"), keyword_schema))\n",
    "\n",
    "# Explode cột cast để có nhiều dòng\n",
    "df_exploded = df_parsed.withColumn(\"keywords\", explode(col(\"keywords\")))\n",
    "\n",
    "# Chọn các trường cần thiết\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"keywords.name\"),\n",
    "    col(\"keywords.id\")\n",
    ")\n",
    "df_selected.dropDuplicates([\"id\"])\n",
    "# Hiển thị kết quả\n",
    "df_selected.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddd3c1e-3e28-4153-9c46-4a5c776e61e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/dim_keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f51cf-fb15-47b7-aba7-f57ce345ad55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48c4f8d4-f7bc-479b-bf4c-3600d3258547",
   "metadata": {},
   "source": [
    "# DIM CAST\n",
    "### cast_id, name, gender, profile_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4aa05ef-b68c-46f0-ac3c-5f410d4f8a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast: string (nullable = true)\n",
      " |-- crew: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_cast = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/credits.parquet\")\n",
    "df_dim_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297bc84f-edd4-4aa2-b86e-b2f72d56d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------------------+------------------------+-------+\n",
      "|name              |gender|profile_path                    |credit_id               |id     |\n",
      "+------------------+------+--------------------------------+------------------------+-------+\n",
      "|Walter Matthau    |2     |/xJVkvprOnzP5Zdh5y63y8HHniDZ.jpg|52fe466a9251416c75077a8d|6837   |\n",
      "|Jack Lemmon       |2     |/chZmNRYMtqkiDlatprGDH4BzGqG.jpg|52fe466a9251416c75077a91|3151   |\n",
      "|Ann-Margret       |1     |/jx5lTaJ5VXZHYB52gaOTAZ9STZk.jpg|52fe466a9251416c75077a95|13567  |\n",
      "|Sophia Loren      |1     |/emKLhbji1c7BjcA2DdbWf0EP9zH.jpg|52fe466a9251416c75077a99|16757  |\n",
      "|Daryl Hannah      |1     |/4LLmp6AQdlj6ueGCRbVRSGvvFSt.jpg|52fe466a9251416c75077a9d|589    |\n",
      "|Burgess Meredith  |2     |/lm98oKloU33Q7QDIIMSyc4Pr2jA.jpg|53e5fcc2c3a3684430000d65|16523  |\n",
      "|Kevin Pollak      |2     |/kwu2T8CDnThZTzE88uiSgJ5eHXf.jpg|53e5fcd4c3a3684433000e1a|7166   |\n",
      "|Whitney Houston   |1     |/69ouDnXnmklYPr4sMJXWKYz81AL.jpg|52fe44779251416c91011aad|8851   |\n",
      "|Angela Bassett    |1     |/tHkgSzhEuJKp5hqp0DZLad8HNZ9.jpg|52fe44779251416c91011ab1|9780   |\n",
      "|Loretta Devine    |1     |/zLQFwQTFtHkb8sbFdkPNamFI7jv.jpg|52fe44779251416c91011ab5|18284  |\n",
      "|Lela Rochon       |1     |/9DBu3r5O4fBosSS4FnSzFCVpm0O.jpg|52fe44779251416c91011ab9|51359  |\n",
      "|Gregory Hines     |2     |/rvvQWFQGeGR14WFVNe0Qg1J7uVY.jpg|52fe44779251416c91011abd|66804  |\n",
      "|Dennis Haysbert   |2     |/mn5Nc5Q31GslpVVWs8p41W4TBma.jpg|52fe44779251416c91011ac1|352    |\n",
      "|Michael Beach     |2     |/lOlWhURNKyZHn71dvq7qC7fiwN6.jpg|52fe44779251416c91011ac5|87118  |\n",
      "|Mykelti Williamson|2     |/8TTxzpuvvpw2tB8xZBCDslYupNU.jpg|52fe44779251416c91011acf|34     |\n",
      "|Lamont Johnson    |2     |/wfZzRPJBdsx62GbkUimc1PShxyC.jpg|56d1b15fc3a3681e4a008b6b|1276777|\n",
      "|Wesley Snipes     |2     |/hQ6EBa6vgu7HoZpzms8Y10VL5Iw.jpg|56f8a929c3a36816e80084f3|10814  |\n",
      "|Steve Martin      |2     |/rI2EMvkfKKPKa5z0nM2pFVBtUyO.jpg|52fe44959251416c75039eb9|67773  |\n",
      "|Diane Keaton      |1     |/fzgUMnbOkxC6E3EFcYHWHFaiKyp.jpg|52fe44959251416c75039ebd|3092   |\n",
      "|Martin Short      |2     |/oZQorXBjTxrdkTJFpoDwOcQ91ji.jpg|52fe44959251416c75039ec1|519    |\n",
      "+------------------+------+--------------------------------+------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cast_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"cast_id\", IntegerType(), True),\n",
    "        StructField(\"character\", StringType(), True),\n",
    "        StructField(\"credit_id\", StringType(), True),\n",
    "        StructField(\"gender\", IntegerType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"order\", IntegerType(), True),\n",
    "        StructField(\"profile_path\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df_dim_cast.withColumn(\"cast\", from_json(col(\"cast\"), cast_schema))\n",
    "\n",
    "# Explode cột cast để có nhiều dòng\n",
    "df_exploded = df_parsed.withColumn(\"cast\", explode(col(\"cast\")))\n",
    "\n",
    "# Chọn các trường cần thiết\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"cast.name\"),\n",
    "    col(\"cast.gender\"),\n",
    "    col(\"cast.profile_path\"),\n",
    "    col(\"cast.credit_id\"),\n",
    "    col(\"cast.id\")\n",
    ")\n",
    "\n",
    "# Hiển thị kết quả\n",
    "df_selected.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e194bc-32a5-4a05-ab77-0b728bb115ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dim_cast  = df_selected.dropDuplicates([\"id\"]).filter(col(\"id\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3ac56f-9a5b-4c1d-99c3-6ee77d87a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_cast.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/dim_cast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e848f-ce60-4008-9943-294aacc377d2",
   "metadata": {},
   "source": [
    "# DIM CREW\n",
    "### id, name,  department, job, gender, credit_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376955da-f48d-4eba-b618-3890aca0447c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                cast|                crew|   id|\n",
      "+--------------------+--------------------+-----+\n",
      "|[{'cast_id': 14, ...|[{'credit_id': '5...|  862|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...| 8844|\n",
      "|[{'cast_id': 2, '...|[{'credit_id': '5...|15602|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|31357|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|11862|\n",
      "|[{'cast_id': 25, ...|[{'credit_id': '5...|  949|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|11860|\n",
      "|[{'cast_id': 2, '...|[{'credit_id': '5...|45325|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...| 9091|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|  710|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...| 9087|\n",
      "|[{'cast_id': 9, '...|[{'credit_id': '5...|12110|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|21032|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|10858|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...| 1408|\n",
      "|[{'cast_id': 4, '...|[{'credit_id': '5...|  524|\n",
      "|[{'cast_id': 6, '...|[{'credit_id': '5...| 4584|\n",
      "|[{'cast_id': 42, ...|[{'credit_id': '5...|    5|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...| 9273|\n",
      "|[{'cast_id': 1, '...|[{'credit_id': '5...|11517|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/credits.parquet\") \n",
    "df_filtered = df.filter((col(\"cast\") != \"[]\") & (col(\"crew\") != \"[]\"))\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f5a440e-0f05-457f-b17e-f64f4d15f286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|cast|crew                                                                                                                                                                 |id   |\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[]  |[{'credit_id': '52fe4624c3a36847f80ef0a5', 'department': 'Directing', 'gender': 2, 'id': 129216, 'job': 'Director', 'name': 'Theodore Thomas', 'profile_path': None}]|42981|\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"id\") == 42981).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2bf1964-ff99-4d54-bc8f-c8d5f50748a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dim_crew = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/credits.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "445c6e28-8f77-47f3-bd32-79c3cdd38715",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------------------+------------------------+---+-----------------------+----------+\n",
      "|name                |gender|profile_path                    |credit_id               |id |job                    |department|\n",
      "+--------------------+------+--------------------------------+------------------------+---+-----------------------+----------+\n",
      "|Mark Hamill         |2     |/ws544EgE5POxGJqq9LUfhnDrHtV.jpg|52fe44dcc3a368484e03b025|2  |Director               |Directing |\n",
      "|Carrie Fisher       |1     |/pbleNurCYdrLFQMEnlQB2nkOR1O.jpg|52fe4440c3a368484e01852d|4  |Novel                  |Writing   |\n",
      "|Albert Brooks       |2     |/kahlMTdygrPJ28VYRhKPavYD9hs.jpg|52fe44e1c3a368484e03c4dd|13 |Director               |Directing |\n",
      "|Ellen DeGeneres     |1     |/4LG2bFkqOzxzR1kpnoDcwIVuQTG.jpg|52fe44fdc3a368484e0426ed|14 |Writer                 |Writing   |\n",
      "|Barry Humphries     |2     |/ccJHmzU8wzOe4sAmeVeScu5mygl.jpg|52fe4480c3a368484e026db9|22 |Writer                 |Writing   |\n",
      "|Robert Zemeckis     |2     |/isCuZ9PWIOyXzdf3ihodXzjIumL.jpg|52fe48cdc3a36847f817a4b5|24 |Director               |Directing |\n",
      "|Eric Roth           |2     |/e73ADDpR3vT1Bwa0Q0oRp5RQWTb.jpg|52fe4a82c3a36847f81d2107|27 |Writer                 |Writing   |\n",
      "|Tom Hanks           |2     |/pQFoyx7rp09CJTAb932F2g8Nlho.jpg|55e4f0abc3a368140800047e|31 |Producer               |Production|\n",
      "|Orson Welles        |2     |/2DF3e98c7GGa1uJJvpgIiMPg0h2.jpg|52fe4672c3a36847f80ff931|40 |Director               |Directing |\n",
      "|Lars von Trier      |2     |/AlfUoJTJlns0XeJROH7Zqt9SvYr.jpg|52fe45839251416c91034ed7|42 |Director               |Directing |\n",
      "|John Fawcett        |2     |/ArR5IXmo5yqjH4I6kcO9W5u9rDC.jpg|54177f100e0a2637ef0002e0|43 |Director               |Directing |\n",
      "|Luc Besson          |2     |/dXJdczT1NcvcZtnoWVGT0NmG11v.jpg|52fe47c49251416c750a4b0d|59 |Director               |Directing |\n",
      "|Chris Tucker        |2     |/9RQvgEi1GnPK4QzqOGHL3jP3VZr.jpg|559e873f9251412dbe001721|66 |Writer                 |Writing   |\n",
      "|Erich Pommer        |2     |/oClAs4W1TRc2jtYCU4CdtXlEhPZ.jpg|52fe48e99251416c750b73d1|67 |Producer               |Production|\n",
      "|Fritz Lang          |2     |/fQAlhdBDnUYXzkezvyzjLB72pvw.jpg|52fe48e99251416c750b73c5|68 |Director               |Directing |\n",
      "|Karl Freund         |2     |/rp4KICz1DHmkYRlzkrqKWwWZm9U.jpg|56d683f6c3a3682cef007d34|70 |Director of Photography|Camera    |\n",
      "|Bruce Brown         |2     |/t2yDx3OLqFNzCjIOWIjEHbzrRcb.jpg|52fe448dc3a368484e029383|86 |Director               |Directing |\n",
      "|Roger Deakins       |2     |/osGe7eLKNIErFLn1RHJDeYTYOmb.jpg|52fe44a49251416c7503be3f|151|Director of Photography|Camera    |\n",
      "|Sally Menke         |1     |/8cUdiGU7KhBgd8UNnoUYNCMZYgQ.jpg|584ef939c3a3682a8a00139e|156|Editor                 |Editing   |\n",
      "|Michael Winterbottom|2     |/hnzUhUXiWy7ThflTzPTRb869mnB.jpg|52fe4796c3a36847f813dc97|172|Director               |Directing |\n",
      "+--------------------+------+--------------------------------+------------------------+---+-----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crew_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"credit_id\", StringType(), True),\n",
    "        StructField(\"department\", StringType(), True),\n",
    "        StructField(\"gender\", IntegerType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"job\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"profile_path\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df_dim_crew.withColumn(\"crew\", from_json(col(\"crew\"), crew_schema))\n",
    "# Explode cột cast để có nhiều dòng\n",
    "df_exploded = df_parsed.withColumn(\"crew\", explode(col(\"crew\")))\n",
    "\n",
    "# Chọn các trường cần thiết\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"crew.name\"),\n",
    "    col(\"crew.gender\"),\n",
    "    col(\"crew.profile_path\"),\n",
    "    col(\"crew.credit_id\"),\n",
    "    col(\"crew.id\"),\n",
    "    col(\"crew.job\"),\n",
    "    col(\"crew.department\")\n",
    ")\n",
    "df_selected = df_selected.dropDuplicates([\"id\"]).filter(col(\"id\").isNotNull())\n",
    "# Hiển thị kết quả\n",
    "df_selected.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0913cb0f-321c-4004-83a1-698458725517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/dim_crew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020e627-ecfe-43a6-bc5e-160daa990cd1",
   "metadata": {},
   "source": [
    "# DIM DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "673fda4d-69a9-4e7d-bc71-54f0ccf6a068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+---------+----------+---------+----------+---------+-----------+-------+----+---------+--------+\n",
      "|release_date|DayOfWeek|  DayName|DayOfMonth|DayOfYear|WeekOfYear|MonthName|MonthOfYear|Quarter|Year|IsWeekDay| DATE_ID|\n",
      "+------------+---------+---------+----------+---------+----------+---------+-----------+-------+----+---------+--------+\n",
      "|  1996-10-25|        6|   Friday|        25|      299|        43|  October|         10|      4|1996|     true|19961025|\n",
      "|  1988-08-28|        1|   Sunday|        28|      241|        34|   August|          8|      3|1988|    false|19880828|\n",
      "|  1988-05-11|        4|Wednesday|        11|      132|        19|      May|          5|      2|1988|     true|19880511|\n",
      "|  1984-02-16|        5| Thursday|        16|       47|         7| February|          2|      1|1984|     true|19840216|\n",
      "|  1986-05-01|        5| Thursday|         1|      121|        18|      May|          5|      2|1986|     true|19860501|\n",
      "|  1995-01-01|        1|   Sunday|         1|        1|        52|  January|          1|      1|1995|    false|19950101|\n",
      "|  1975-09-02|        3|  Tuesday|         2|      245|        36|September|          9|      3|1975|     true|19750902|\n",
      "|  1985-09-25|        4|Wednesday|        25|      268|        39|September|          9|      3|1985|     true|19850925|\n",
      "|  1989-11-17|        6|   Friday|        17|      321|        46| November|         11|      4|1989|     true|19891117|\n",
      "|  1992-08-28|        6|   Friday|        28|      241|        35|   August|          8|      3|1992|     true|19920828|\n",
      "|  1959-09-29|        3|  Tuesday|        29|      272|        40|September|          9|      3|1959|     true|19590929|\n",
      "|  1999-10-15|        6|   Friday|        15|      288|        41|  October|         10|      4|1999|     true|19991015|\n",
      "|  1985-01-25|        6|   Friday|        25|       25|         4|  January|          1|      1|1985|     true|19850125|\n",
      "|  1992-06-04|        5| Thursday|         4|      156|        23|     June|          6|      2|1992|     true|19920604|\n",
      "|  1947-09-25|        5| Thursday|        25|      268|        39|September|          9|      3|1947|     true|19470925|\n",
      "|  1991-06-20|        5| Thursday|        20|      171|        25|     June|          6|      2|1991|     true|19910620|\n",
      "|  1979-08-03|        6|   Friday|         3|      215|        31|   August|          8|      3|1979|     true|19790803|\n",
      "|  1958-12-25|        5| Thursday|        25|      359|        52| December|         12|      4|1958|     true|19581225|\n",
      "|  2000-06-15|        5| Thursday|        15|      167|        24|     June|          6|      2|2000|     true|20000615|\n",
      "|  1986-04-18|        6|   Friday|        18|      108|        16|    April|          4|      2|1986|     true|19860418|\n",
      "+------------+---------+---------+----------+---------+----------+---------+-----------+-------+----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")\n",
    "df = df.dropna(subset=[\"title\", \"release_date\", \"budget\"])\n",
    "df = df.dropDuplicates()\n",
    "df = df.withColumn(\"parsed_date\", F.to_date(F.col(\"release_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "result = df.select(\n",
    "    F.col(\"release_date\"),\n",
    "    F.dayofweek(\"parsed_date\").alias(\"DayOfWeek\"),\n",
    "    F.date_format(\"parsed_date\", \"EEEE\").alias(\"DayName\"),\n",
    "    F.dayofmonth(\"parsed_date\").alias(\"DayOfMonth\"),\n",
    "    F.dayofyear(\"parsed_date\").alias(\"DayOfYear\"),\n",
    "    F.weekofyear(\"parsed_date\").alias(\"WeekOfYear\"),\n",
    "    F.date_format(\"parsed_date\", \"MMMM\").alias(\"MonthName\"),\n",
    "    F.month(\"parsed_date\").alias(\"MonthOfYear\"),\n",
    "    F.quarter(\"parsed_date\").alias(\"Quarter\"),\n",
    "    F.year(\"parsed_date\").alias(\"Year\"),\n",
    "    F.when((F.dayofweek(\"parsed_date\") >= 2) & (F.dayofweek(\"parsed_date\") <= 6), True).otherwise(False).alias(\"IsWeekDay\")\n",
    ")\n",
    "result = result.withColumn(\n",
    "    \"DATE_ID\",\n",
    "    (F.col(\"Year\") * 10000 + F.col(\"MonthOfYear\") * 100 + F.col(\"DayOfMonth\")).cast(\"long\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee43fd49-6585-4631-93b5-4fb5ae7cc6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/dim_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2e2d1-3bfd-468e-96c3-05eb46ad088a",
   "metadata": {},
   "source": [
    "# DIM GENRES\n",
    "### genre_id, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce681301-4e1a-4b5f-b623-681978a03c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f5934f-8e49-4dd4-8ded-58e3f21ae936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"id\" among (CAST(genres.id AS INT), name)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m df_exploded \u001b[38;5;241m=\u001b[39m df_parsed\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m, explode(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     10\u001b[0m df_selected \u001b[38;5;241m=\u001b[39m df_exploded\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     11\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres.id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInteger\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres.name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m df_selected \u001b[38;5;241m=\u001b[39m \u001b[43mdf_selected\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropDuplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2403\u001b[0m, in \u001b[0;36mDataFrame.dropDuplicates\u001b[0;34m(self, subset)\u001b[0m\n\u001b[1;32m   2401\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mdropDuplicates()\n\u001b[1;32m   2402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2403\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropDuplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"id\" among (CAST(genres.id AS INT), name)"
     ]
    }
   ],
   "source": [
    "genres_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df.withColumn(\"genres\", from_json(col(\"genres\"), genres_schema))\n",
    "df_exploded = df_parsed.withColumn(\"genres\", explode(col(\"genres\")))\n",
    "\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"genres.id\"),\n",
    "    col(\"genres.name\")\n",
    ")\n",
    "df_selected = df_selected.dropDuplicates(subset=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dff8db6-f1bb-4d8f-b05d-aba40f48d3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|                name|\n",
      "+-----+--------------------+\n",
      "|   12|           Adventure|\n",
      "|   14|             Fantasy|\n",
      "|   16|           Animation|\n",
      "|   18|               Drama|\n",
      "|   27|              Horror|\n",
      "|   28|              Action|\n",
      "|   35|              Comedy|\n",
      "|   36|             History|\n",
      "|   37|             Western|\n",
      "|   53|            Thriller|\n",
      "|   80|               Crime|\n",
      "|   99|         Documentary|\n",
      "|  878|     Science Fiction|\n",
      "| 2883|             Aniplex|\n",
      "| 7759|             GoHands|\n",
      "| 7760|           BROSTA TV|\n",
      "| 7761|Mardock Scramble ...|\n",
      "| 9648|             Mystery|\n",
      "|10402|               Music|\n",
      "|10749|             Romance|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2fa32db-2eba-4b51-8541-80b2e2e3b561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/dim_genres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd7cd3-1357-48da-8faa-6910dd40c6ac",
   "metadata": {},
   "source": [
    "# complete DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a18e3b80-eb16-4cb9-8667-326f145430c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dimmovie = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver_T/dim_movie\")\n",
    "df_dimmovie.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79a69c6-a6aa-4565-a25a-d6b99ac2b205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- profile_path: string (nullable = true)\n",
      " |-- credit_id: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver_T/dim_cast\")\n",
    "df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad22acc0-2345-43a5-be96-cbd9e556da7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- profile_path: string (nullable = true)\n",
      " |-- credit_id: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crew = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver_T/dim_crew\")\n",
    "df_crew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24cfdca5-0ba0-4f57-89a7-d67ceddba669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_genres = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver_T/dim_genres\")\n",
    "df_genres.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c392d11f-bade-48a7-ab4e-f3eb4f3f8dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DayName: string (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DayOfYear: integer (nullable = true)\n",
      " |-- WeekOfYear: integer (nullable = true)\n",
      " |-- MonthName: string (nullable = true)\n",
      " |-- MonthOfYear: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- IsWeekDay: boolean (nullable = true)\n",
      " |-- DATE_ID: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver_T/dim_date\")\n",
    "df_date.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e081a0-4d03-4ada-9071-08668b4bd8ed",
   "metadata": {},
   "source": [
    "# Cast_Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee656d3a-20d0-4947-85d4-ab0a2dae4335",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------+--------+\n",
      "|cast_id|character                   |movie_id|\n",
      "+-------+----------------------------+--------+\n",
      "|6837   |Max Goldman                 |15602   |\n",
      "|3151   |John Gustafson              |15602   |\n",
      "|13567  |Ariel Gustafson             |15602   |\n",
      "|16757  |Maria Sophia Coletta Ragetti|15602   |\n",
      "|589    |Melanie Gustafson           |15602   |\n",
      "|16523  |Grandpa Gustafson           |15602   |\n",
      "|7166   |Jacob Goldman               |15602   |\n",
      "|8851   |Savannah 'Vannah' Jackson   |31357   |\n",
      "|9780   |Bernadine 'Bernie' Harris   |31357   |\n",
      "|18284  |Gloria 'Glo' Matthews       |31357   |\n",
      "|51359  |Robin Stokes                |31357   |\n",
      "|66804  |Marvin King                 |31357   |\n",
      "|352    |Kenneth Dawkins             |31357   |\n",
      "|87118  |John Harris, Sr.            |31357   |\n",
      "|34     |Troy                        |31357   |\n",
      "|1276777|Joseph                      |31357   |\n",
      "|10814  |James Wheeler               |31357   |\n",
      "|67773  |George Banks                |11862   |\n",
      "|3092   |Nina Banks                  |11862   |\n",
      "|519    |Franck Eggelhoffer          |11862   |\n",
      "+-------+----------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/credits.parquet\")\n",
    "cast_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"cast_id\", IntegerType(), True),\n",
    "        StructField(\"character\", StringType(), True),\n",
    "        StructField(\"credit_id\", StringType(), True),\n",
    "        StructField(\"gender\", IntegerType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"order\", IntegerType(), True),\n",
    "        StructField(\"profile_path\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df.withColumn(\"cast\", from_json(col(\"cast\"), cast_schema))\n",
    "\n",
    "# Explode cột cast để có nhiều dòng\n",
    "df_exploded = df_parsed.withColumn(\"cast\", explode(col(\"cast\")))\n",
    "\n",
    "# Chọn các trường cần thiết\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"cast.id\").alias(\"cast_id\"),\n",
    "    col(\"cast.character\"),\n",
    "    col(\"id\").alias(\"movie_id\")\n",
    ")\n",
    "\n",
    "# Hiển thị kết quả\n",
    "df_selected.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11350882-2879-4bed-8f87-ca2892281b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/movie_cast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d71b2-d2df-4fcb-8e7b-1f38cfd0269e",
   "metadata": {},
   "source": [
    "# movie_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1789ed4-7bdc-4594-8069-5838667de380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast: string (nullable = true)\n",
      " |-- crew: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+------------+----------+--------+\n",
      "|crew_id|job         |department|movie_id|\n",
      "+-------+------------+----------+--------+\n",
      "|6210   |Writer      |Writing   |16420   |\n",
      "|56710  |Director    |Directing |16420   |\n",
      "|56710  |Adaptation  |Writing   |16420   |\n",
      "|33315  |Director    |Directing |31174   |\n",
      "|6210   |Theatre Play|Writing   |31174   |\n",
      "|1327   |Writer      |Writing   |31174   |\n",
      "|33315  |Writer      |Writing   |31174   |\n",
      "|16862  |Director    |Directing |48750   |\n",
      "|37127  |Writer      |Writing   |48750   |\n",
      "|16862  |Writer      |Writing   |48750   |\n",
      "|119294 |Writer      |Writing   |46785   |\n",
      "|120229 |Director    |Directing |46785   |\n",
      "|117075 |Director    |Directing |188588  |\n",
      "|14692  |Director    |Directing |47475   |\n",
      "|114997 |Director    |Directing |55475   |\n",
      "|114997 |Writer      |Writing   |55475   |\n",
      "|5281   |Director    |Directing |20649   |\n",
      "|5281   |Screenplay  |Writing   |20649   |\n",
      "|2989   |Screenplay  |Writing   |20649   |\n",
      "|2163   |Screenplay  |Writing   |52856   |\n",
      "+-------+------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/credits.parquet\")\n",
    "df.printSchema()\n",
    "crew_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"department\", StringType(), True),\n",
    "        StructField(\"gender\", IntegerType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"job\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"profile_path\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df.withColumn(\"crew\", from_json(col(\"crew\"), crew_schema))\n",
    "# Explode cột cast để có nhiều dòng\n",
    "df_exploded = df_parsed.withColumn(\"crew\", explode(col(\"crew\")))\n",
    "\n",
    "# Chọn các trường cần thiết\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"crew.id\").alias(\"crew_id\"),\n",
    "    col(\"crew.job\"),\n",
    "    col(\"crew.department\"),\n",
    "    col(\"id\").alias(\"movie_id\")\n",
    ")\n",
    "\n",
    "df_selected.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12da9567-ae16-4fa9-a7c6-02edd15f1909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/movie_crew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e87d1-9263-4615-92ce-5389852c27e4",
   "metadata": {},
   "source": [
    "# movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2608d67-31c5-41b2-adc6-b5fbc690f3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")\n",
    "genres_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df.withColumn(\"genres\", from_json(col(\"genres\"), genres_schema))\n",
    "df_exploded = df_parsed.withColumn(\"genres\", explode(col(\"genres\")))\n",
    "\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"genres.id\").alias(\"genres_id\"),\n",
    "    col(\"id\").cast(\"Integer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2a64bf-12d4-47f9-b863-96bc27f47af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/movie_genres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff88e7-46c9-45cd-99f9-3cd6ab698413",
   "metadata": {},
   "source": [
    "# Movie Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045d2a91-85f5-452b-8cf2-a3adfe6a427d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+-----+\n",
      "|name                    |keyword_id|id   |\n",
      "+------------------------+----------+-----+\n",
      "|jealousy                |931       |862  |\n",
      "|toy                     |4290      |862  |\n",
      "|boy                     |5202      |862  |\n",
      "|friendship              |6054      |862  |\n",
      "|friends                 |9713      |862  |\n",
      "|rivalry                 |9823      |862  |\n",
      "|boy next door           |165503    |862  |\n",
      "|new toy                 |170722    |862  |\n",
      "|toy comes to life       |187065    |862  |\n",
      "|board game              |10090     |8844 |\n",
      "|disappearance           |10941     |8844 |\n",
      "|based on children's book|15101     |8844 |\n",
      "|new home                |33467     |8844 |\n",
      "|recluse                 |158086    |8844 |\n",
      "|giant insect            |158091    |8844 |\n",
      "|fishing                 |1495      |15602|\n",
      "|best friend             |12392     |15602|\n",
      "|duringcreditsstinger    |179431    |15602|\n",
      "|old men                 |208510    |15602|\n",
      "|based on novel          |818       |31357|\n",
      "+------------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/keywords.parquet\")\n",
    "keyword_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "df_parsed = df.withColumn(\"keywords\", from_json(col(\"keywords\"), keyword_schema))\n",
    "\n",
    "# Explode cột cast để có nhiều dòng\n",
    "df_exploded = df_parsed.withColumn(\"keywords\", explode(col(\"keywords\")))\n",
    "\n",
    "# Chọn các trường cần thiết\n",
    "df_selected = df_exploded.select(\n",
    "    col(\"keywords.name\"),\n",
    "    col(\"keywords.id\").alias(\"keyword_id\"),\n",
    "    col(\"id\")\n",
    "                             \n",
    ")\n",
    "\n",
    "# Hiển thị kết quả\n",
    "df_selected.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61202b63-2bf6-4d55-8a0c-2704c1924575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_selected.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/movie_keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76637455-3eb4-486a-b6c4-c462711eba73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3cc4206-fb6f-491b-b847-1a8f5007a045",
   "metadata": {},
   "source": [
    "# Fact Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5b8e29c-da6b-44b0-a9f7-651774471409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adult: string (nullable = true)\n",
      " |-- belongs_to_collection: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")\n",
    "df = df.withColumn(\"popularity\", col(\"popularity\").cast(\"double\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e139577-43b2-4ceb-8d15-cf0b98323402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/bronze/movies.parquet\")\n",
    "df_fact = df.withColumn(\n",
    "    \"date_id\",\n",
    "    (F.year(\"release_date\") * 10000 + F.month(\"release_date\") * 100 + F.dayofmonth(\"release_date\")).cast(\"long\")\n",
    ")\n",
    "\n",
    "df_fact = df_fact.select(\n",
    "    col(\"id\").cast(\"Integer\"),\n",
    "    col(\"budget\"),\n",
    "    col(\"popularity\"),\n",
    "    col(\"revenue\"),\n",
    "    col(\"vote_average\"),\n",
    "    col(\"vote_count\"),\n",
    "    col(\"date_id\")\n",
    ")\n",
    "df_fact.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"s3a://lakehouse/silver_T/fact_movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "807271d2-7382-400e-8bf7-142f05eecb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------------+------------+----------+--------+\n",
      "|   id|  budget|popularity|     revenue|vote_average|vote_count| date_id|\n",
      "+-----+--------+----------+------------+------------+----------+--------+\n",
      "|  862|30000000| 21.946943|3.73554033E8|         7.7|    5415.0|19951030|\n",
      "| 8844|65000000| 17.015539|2.62797249E8|         6.9|    2413.0|19951215|\n",
      "|15602|       0|   11.7129|         0.0|         6.5|      92.0|19951222|\n",
      "|31357|16000000|  3.859495| 8.1452156E7|         6.1|      34.0|19951222|\n",
      "|11862|       0|  8.387519| 7.6578911E7|         5.7|     173.0|19950210|\n",
      "|  949|60000000| 17.924927|1.87436818E8|         7.7|    1886.0|19951215|\n",
      "|11860|58000000|  6.677277|         0.0|         6.2|     141.0|19951215|\n",
      "|45325|       0|  2.561161|         0.0|         5.4|      45.0|19951222|\n",
      "| 9091|35000000|   5.23158| 6.4350171E7|         5.5|     174.0|19951222|\n",
      "|  710|58000000| 14.686036|3.52194034E8|         6.6|    1194.0|19951116|\n",
      "| 9087|62000000|  6.318445|1.07879496E8|         6.5|     199.0|19951117|\n",
      "|12110|       0|  5.430331|         0.0|         5.7|     210.0|19951222|\n",
      "|21032|       0| 12.140733| 1.1348324E7|         7.1|     423.0|19951222|\n",
      "|10858|44000000|     5.092| 1.3681765E7|         7.1|      72.0|19951222|\n",
      "| 1408|98000000|  7.284477| 1.0017322E7|         5.7|     137.0|19951222|\n",
      "|  524|52000000| 10.137389|1.16112375E8|         7.8|    1343.0|19951122|\n",
      "| 4584|16500000| 10.673167|      1.35E8|         7.2|     364.0|19951213|\n",
      "|    5| 4000000|  9.026586|   4300000.0|         6.5|     539.0|19951209|\n",
      "| 9273|30000000|  8.205448|2.12385533E8|         6.1|    1128.0|19951110|\n",
      "|11517|60000000|  7.337906| 3.5431113E7|         5.4|     224.0|19951121|\n",
      "+-----+--------+----------+------------+------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver_T/fact_movies\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddb8f74f-a49e-492d-82d0-1cd44ac17078",
   "metadata": {},
   "source": [
    "                    dim_genres\n",
    "                          |\n",
    "             movie_genre_bridge\n",
    "                          |\n",
    "dim_movie -------------- fact_movie_revenue -------------- dim_date\n",
    "                         /        \\\n",
    "        movie_cast_bridge      movie_crew_bridge\n",
    "                |                     |\n",
    "          dim_cast                   dim_crew\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429f6a7-0b7a-41bb-9fa8-2085c5c67269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
