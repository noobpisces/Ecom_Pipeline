{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06adaf1-6864-47ad-874f-ad5def0b0280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, IntegerType\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit, collect_list, struct,sqrt,avg\n",
    "from pyspark.sql import types as T\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad38475e-ec97-4a6c-8efd-36a92eed72a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HybridRecommenderMaxPerformance\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"13g\") \\         # Dành phần lớn RAM cho executor\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\            # Dành một phần cho driver\n",
    "    .config(\"spark.executor.cores\", \"4\") \\            # Dùng toàn bộ 4 cores\n",
    "    .config(\"spark.executor.instances\", \"1\") \\        # Dùng 1 executor (vì có 1 worker)\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\       # Gợi ý phân chia task (>= cores * 2)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\ \n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d485ae2-778e-4502-8977-616f430f0413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CB = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/hehe123\")\n",
    "filtered_CB = CB.filter(CB[\"id\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe08f8aa-c694-45e0-ab6e-836dda2a5473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# movie_vec_dict = {int(row.id): row.vecs for row in filtered_CB.select(\"id\", \"vecs\").collect()}\n",
    "# b_movie_vec = sc.broadcast(movie_vec_dict)\n",
    "# ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "# ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "# ratings = ratings.withColumn('userId', ratings['userId'].cast('int')).\\\n",
    "# withColumn('movieId', ratings['movieId'].cast('int')).withColumn('rating', ratings['rating'].cast('float'))\n",
    "# train, validation, test = ratings.randomSplit([0.8, 0.1, 0.1], seed=0)\n",
    "movie_vec_dict = {int(row.id): row.vecs for row in filtered_CB.select(\"id\", \"vecs\").collect()}\n",
    "b_movie_vec = sc.broadcast(movie_vec_dict)\n",
    "ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "ratings = ratings.dropna(subset=['userId', 'movieId'])\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "ratings = ratings.withColumn('userId', ratings['userId'].cast('int')).\\\n",
    "withColumn('movieId', ratings['movieId'].cast('int')).withColumn('rating', ratings['rating'].cast('float'))\n",
    "train, validation, test = ratings.randomSplit([0.75, 0.15, 0.15], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b2f749-c344-437b-9245-3d38e891bffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test.limit(50000)\n",
    "train = train.limit(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45199044-4f91-44b6-9620-346a6100f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "CF_model = ALSModel.load(\"s3a://lakehouse/CF/als_best_model\")\n",
    "PredcitCF = CF_model.transform(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d76f7c-ab65-47b5-9508-79fa2da579af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rating(user_history, target_id):\n",
    "    vec_dict = b_movie_vec.value\n",
    "    target = vec_dict.get(target_id)\n",
    "    if target is None: return 0.0\n",
    "    num, den = 0.0, 0.0\n",
    "    for mid, r in user_history:\n",
    "        v = vec_dict.get(mid)\n",
    "        if v is None: continue\n",
    "        # cosine similarity\n",
    "        sim = float(np.dot(target, v) / (np.linalg.norm(target)*np.linalg.norm(v))) if np.linalg.norm(target)*np.linalg.norm(v)>0 else 0.0\n",
    "        num += sim * r\n",
    "        den += builtins.abs(sim)\n",
    "    return float(num/den) if den>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141628ec-a188-4bd5-8d82-2ba568d1863e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_udf = F.udf(predict_rating, FloatType())\n",
    "ratings_in_cb = (\n",
    "    train\n",
    "    .join(CB.select(F.col(\"id\").alias(\"cb_id\")), train.movieId == F.col(\"cb_id\"), \"inner\")\n",
    "    .select(\"userId\", \"movieId\", \"rating\")\n",
    ")\n",
    "# 4. Gom lịch sử train thành array per user\n",
    "train_hist = (\n",
    "    ratings_in_cb\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(\n",
    "        collect_list(\n",
    "            struct(\"movieId\",\"rating\")\n",
    "        ).alias(\"history\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Kết hợp sample_ratings (test) với history train\n",
    "eval_df = (\n",
    "    ratings_in_cb\n",
    "    .join(train_hist, on=\"userId\", how=\"left\")\n",
    ")\n",
    "\n",
    "# 6. Dự đoán và tính bình phương sai số\n",
    "eval_df = (\n",
    "    eval_df\n",
    "    .withColumn(\"predicted_rating_CT\",\n",
    "                predict_udf(\"history\", \"movieId\"))\n",
    "    .join(\n",
    "            PredcitCF.select(\"userId\", \"movieId\", \"prediction\").withColumnRenamed(\"prediction\", \"predicted_rating_CF\"),\n",
    "            on=[\"userId\", \"movieId\"],\n",
    "            how=\"inner\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d037b2e-ee38-40a5-8efd-41df8b32cc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48230"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee706481-6839-4c6b-aa3a-f041179eb242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60638)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o191.count",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Cache để tránh tính lại nhiều lần\u001b[39;00m\n\u001b[1;32m      5\u001b[0m eval_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m----> 6\u001b[0m \u001b[43meval_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Action để trigger cache\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Hàm tính RMSE cho mỗi alpha\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_rmse\u001b[39m(alpha):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o191.count"
     ]
    }
   ],
   "source": [
    "# Chỉ giữ lại các cột cần thiết\n",
    "eval_df = eval_df.select(\"predicted_rating_CT\", \"predicted_rating_CF\", \"rating\")\n",
    "\n",
    "# Cache để tránh tính lại nhiều lần\n",
    "eval_df.cache()\n",
    "eval_df.count()  # Action để trigger cache\n",
    "\n",
    "# Hàm tính RMSE cho mỗi alpha\n",
    "def compute_rmse(alpha):\n",
    "    hybrid_df = eval_df.withColumn(\n",
    "        \"predicted_hybrid\",\n",
    "        lit(alpha) * col(\"predicted_rating_CT\") + (1 - alpha) * col(\"predicted_rating_CF\")\n",
    "    )\n",
    "    rmse = hybrid_df.withColumn(\n",
    "        \"squared_error\", (col(\"predicted_hybrid\") - col(\"rating\")) ** 2\n",
    "    ).agg(sqrt(avg(\"squared_error\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n",
    "    return rmse\n",
    "\n",
    "# Tìm best alpha\n",
    "best_alpha = None\n",
    "best_rmse = float(\"inf\")\n",
    "alpha_rmse_list = []\n",
    "\n",
    "for alpha in np.arange(0.0, 1.05, 0.05):\n",
    "    rmse = compute_rmse(alpha)\n",
    "    alpha_rmse_list.append((alpha, rmse))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_alpha = alpha\n",
    "\n",
    "# In kết quả\n",
    "print(\"Alpha - RMSE:\")\n",
    "for alpha, rmse in alpha_rmse_list:\n",
    "    print(f\"{alpha:.2f} -> {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Best alpha = {best_alpha:.2f} with RMSE = {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269829da-4180-44d3-b323-339d7d06babc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c253f-6fd4-46bd-9d85-b5a3879f01aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d32f6-f951-4e64-a5ce-c43157cf7e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326733f5-30bf-4fa9-99e0-3d847e3d41a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921cc1b-df8e-46da-b0fd-e65f979d6348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24950da2-b62c-42d8-9ac1-059f695e6896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0ee77-25a7-4e66-8cd4-14e810701a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fea522-db11-423e-aec9-b09ae2d1e04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab64fcf-39d3-4745-9673-7ba6f9a6dc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4eca64-c33c-4083-986d-9947cc7f8b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8692d-8c9b-479d-8c13-dc31618d1aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fba32c-da5f-43a4-9a95-82bb89167bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
