{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3faed848-6cf4-4740-8253-46200247bb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import numpy as np\n",
    "import time\n",
    "from pyspark.sql.functions import col, mean, lit, udf\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.ml.linalg import Vectors, DenseVector  # Import DenseVector trực tiếp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ecb2eb-c351-4005-9f59-c520ca278f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Optimized\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4G\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"4G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"2G\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978f2f79-d3fb-4272-bd51-7fbdc68510b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+------------+\n",
      "|userId|movieId|predict|true_predict|\n",
      "+------+-------+-------+------------+\n",
      "|   148|    260|   3.95|         4.0|\n",
      "|   148|   2028|   3.94|         4.0|\n",
      "|   148|   4993|   3.83|         5.0|\n",
      "|   148|   6373|    4.0|         3.5|\n",
      "|   148|   8665|   3.94|         4.0|\n",
      "|   148|  31696|   3.94|         4.0|\n",
      "|   148|  40815|   3.95|         4.0|\n",
      "|   148|  58559|    4.0|         3.5|\n",
      "|   148|  68954|   4.06|         3.0|\n",
      "|   148| 111759|   3.89|         4.5|\n",
      "|   229|      2|    2.9|         3.0|\n",
      "|   229|      5|   2.91|         1.0|\n",
      "|   229|     12|   2.91|         1.0|\n",
      "|   229|     16|    2.9|         3.0|\n",
      "|   229|     19|   2.91|         1.0|\n",
      "|   229|     21|    2.9|         3.0|\n",
      "|   229|     25|    2.9|         4.0|\n",
      "|   229|     70|    2.9|         3.0|\n",
      "|   229|     77|    2.9|         3.0|\n",
      "|   229|     85|    2.9|         3.0|\n",
      "+------+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Thời gian xử lý: 119.52 giây\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import time\n",
    "\n",
    "# Đo thời gian bắt đầu\n",
    "start_time = time.time()\n",
    "\n",
    "# def get_vecs_optimized():\n",
    "#     # Đọc dữ liệu vector đặc trưng\n",
    "#     df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta\")\n",
    "    \n",
    "#     # Chuyển đổi thành từ điển {movie_id: list của vector}\n",
    "#     movie_vecs_dict = {\n",
    "#         row[\"id\"]: list(row[\"vecs\"]) for row in df_list.collect()\n",
    "#     }\n",
    "    \n",
    "#     # Phát sóng (broadcast) vector đặc trưng tới tất cả worker\n",
    "#     return spark.sparkContext.broadcast(movie_vecs_dict)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_vecs_optimized():\n",
    "    # Đọc dữ liệu vector đặc trưng\n",
    "    df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta\")\n",
    "    \n",
    "    # Đọc dữ liệu ratings để lấy danh sách movieId\n",
    "    ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "    ratings_movie_ids = ratings.select(\"movieId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Chỉ giữ lại các vector có trong ratings\n",
    "    filtered_df = df_list.filter(F.col(\"id\").isin(ratings_movie_ids))\n",
    "    \n",
    "    # Chuyển đổi thành từ điển {movie_id: list của vector}\n",
    "    movie_vecs_dict = {\n",
    "        row[\"id\"]: list(row[\"vecs\"]) for row in filtered_df.collect()\n",
    "    }\n",
    "    \n",
    "    return movie_vecs_dict  # Trả về dictionary mà không dùng Broadcast\n",
    "\n",
    "\n",
    "# Broadcast vector đặc trưng phim\n",
    "movie_vecs_dict = get_vecs_optimized()\n",
    "\n",
    "# Đọc dữ liệu ratings\n",
    "ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "# Lọc bỏ các movieId không có vector đặc trưng\n",
    "all_movie_vecs = set(movie_vecs_dict.keys())  # Trực tiếp dùng movie_vecs_dict, không còn .value\n",
    "ratings_filtered = ratings.filter(F.col(\"movieId\").isin(list(all_movie_vecs)))\n",
    "\n",
    "# UDF để tính toán dự đoán điểm mà không dùng numpy\n",
    "@F.udf(DoubleType())\n",
    "def calculate_prediction(user_ratings, movie_id):\n",
    "    movie_vecs = movie_vecs_dict\n",
    "    if movie_id not in movie_vecs:\n",
    "        return 0.0\n",
    "    \n",
    "    input_vec = movie_vecs[movie_id]\n",
    "    similarities = []\n",
    "\n",
    "    for other_movie, rating in user_ratings:\n",
    "        if other_movie == movie_id:\n",
    "            continue\n",
    "        if other_movie not in movie_vecs:\n",
    "            continue\n",
    "        \n",
    "        other_vec = movie_vecs[other_movie]\n",
    "        \n",
    "        # Tính Cosine Similarity thủ công\n",
    "        numerator = sum(x * y for x, y in zip(input_vec, other_vec))\n",
    "        denominator_a = sum(x * x for x in input_vec) ** 0.5\n",
    "        denominator_b = sum(y * y for y in other_vec) ** 0.5\n",
    "        \n",
    "        if denominator_a == 0 or denominator_b == 0:\n",
    "            continue\n",
    "        \n",
    "        sim = numerator / (denominator_a * denominator_b)\n",
    "        if sim > 0:\n",
    "            similarities.append((sim, rating))\n",
    "    \n",
    "    if similarities:\n",
    "        numerator = sum(sim * rating for sim, rating in similarities)\n",
    "        denominator = sum(abs(sim) for sim, _ in similarities)\n",
    "        return round(numerator / denominator, 2) if denominator != 0 else 0.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "# Tổ chức lại dữ liệu để xử lý song song\n",
    "ratings_grouped = ratings_filtered.groupBy(\"userId\").agg(\n",
    "    F.collect_list(F.struct(\"movieId\", \"rating\")).alias(\"user_ratings\")\n",
    ").select(\n",
    "    \"userId\",\n",
    "    F.explode(\"user_ratings\").alias(\"movie_info\"),\n",
    "    \"user_ratings\"\n",
    ").select(\n",
    "    \"userId\",\n",
    "    F.col(\"movie_info.movieId\").alias(\"movieId\"),\n",
    "    F.col(\"movie_info.rating\").alias(\"true_predict\"),\n",
    "    \"user_ratings\"\n",
    ")\n",
    "\n",
    "# Tính toán dự đoán\n",
    "predictions_df = ratings_grouped.withColumn(\n",
    "    \"predict\", \n",
    "    calculate_prediction(F.col(\"user_ratings\"), F.col(\"movieId\"))\n",
    ").select(\"userId\", \"movieId\", \"predict\", \"true_predict\")\n",
    "\n",
    "# Hiển thị kết quả\n",
    "predictions_df.show(20)\n",
    "\n",
    "# Đo thời gian kết thúc\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian xử lý: {round(end_time - start_time, 2)} giây\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6265760-5ad7-4ba7-a167-fbf246538dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.withColumn(\"true_predict\", F.col(\"true_predict\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87996090-20f1-4784-959c-2a7a78c88c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54848)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o52863.count",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Đảm bảo predictions_df được cache ngay sau khi tính xong\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m predictions_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpredictions_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Kích hoạt cache, tránh tính lại\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Đo thời gian bắt đầu tính RMSE\u001b[39;00m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o52863.count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Đảm bảo predictions_df được cache ngay sau khi tính xong\n",
    "predictions_df = predictions_df.cache()\n",
    "predictions_df.count()  # Kích hoạt cache, tránh tính lại\n",
    "\n",
    "# Đo thời gian bắt đầu tính RMSE\n",
    "start_time = time.time()\n",
    "\n",
    "# Giới hạn chỉ 1000 dòng để tính RMSE\n",
    "limited_predictions_df = predictions_df.limit(1000).cache()\n",
    "limited_predictions_df.count()  # Đảm bảo dữ liệu đã được cache\n",
    "\n",
    "# Tính RMSE\n",
    "rmse_df = limited_predictions_df.withColumn(\n",
    "    \"squared_error\", F.pow(F.col(\"predict\") - F.col(\"true_predict\"), 2)\n",
    ")\n",
    "\n",
    "rmse_value = rmse_df.agg(F.sqrt(F.avg(F.col(\"squared_error\")))).first()[0]\n",
    "print(f\"RMSE của mô hình với 1000 dòng: {rmse_value}\")\n",
    "\n",
    "# Đo thời gian kết thúc\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian xử lý RMSE: {round(end_time - start_time, 2)} giây\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4292e9-507e-4fff-bef8-c93d714bf103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df7d3e-ab50-48d9-9b75-7af18b6f5eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8289a1-85fa-441d-a39a-2ffdf15606b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1f53de-9f3d-478e-a650-07c85895f110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vecs():\n",
    "    global df_list, all_vecs, flag2\n",
    "    # if not flag2:\n",
    "    df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/haha\")\n",
    "    data_original = df_list.collect()\n",
    "    all_vecs = [(row.id, Vectors.dense(row.vecs)) for row in data_original]\n",
    "        # flag2 = True\n",
    "    return all_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea192c91-b0a2-4644-8ceb-8fbba3525498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CosineSim(vec1, vec2):\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2))\n",
    "    return float(numerator / denominator) if denominator != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a35f4fa-8edb-4627-8160-6aed59c23be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Tính 1 ng\n",
    "def get_cb_predictions(user_id, movie_id):\n",
    "    global df_merge, ratings\n",
    "\n",
    "    # Đọc dữ liệu ratings\n",
    "    ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "    ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "    # Đọc dữ liệu phim\n",
    "    df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/MergeData\")\n",
    "\n",
    "    # Đổi tên cột 'id' thành 'movieId' để tương thích và đảm bảo là số nguyên\n",
    "    df_merge = df_merge.withColumnRenamed(\"id\", \"movieId\").withColumn(\"movieId\", F.col(\"movieId\").cast(\"int\"))\n",
    "\n",
    "    # Lấy vector đặc trưng của các bộ phim\n",
    "    all_movies_vecs = get_vecs()\n",
    "\n",
    "    # Đảm bảo tất cả movie_id trong vectors là số nguyên\n",
    "    all_movies_vecs = [(int(r[0]), r[1]) for r in all_movies_vecs]\n",
    "\n",
    "    # In ra kiểu dữ liệu của movie_id trong ratings và df_merge\n",
    "    print(\"Rating Data Movie ID Type:\", ratings.select(\"movieId\").dtypes)\n",
    "    print(\"Merge Data Movie ID Type:\", df_merge.select(\"movieId\").dtypes)\n",
    "    print(\"Vector Data Movie ID Type:\", type(all_movies_vecs[0][0]))\n",
    "\n",
    "    # Kiểm tra xem vector của phim có tồn tại không\n",
    "    input_vecs = [r[1] for r in all_movies_vecs if r[0] == int(movie_id)]\n",
    "    if not input_vecs:\n",
    "        return f\"Movie ID {movie_id} not found in vector data.\"\n",
    "\n",
    "    input_vec = input_vecs[0]\n",
    "\n",
    "    # Lấy các phim mà người dùng đã đánh giá\n",
    "    user_ratings = ratings.filter(F.col(\"userId\") == user_id)\n",
    "\n",
    "    # Kiểm tra người dùng có đánh giá không\n",
    "    if user_ratings.count() == 0:\n",
    "        return f\"User {user_id} has not rated any movies.\"\n",
    "\n",
    "    # Tính độ tương đồng giữa phim đầu vào và các phim đã đánh giá\n",
    "    user_ratings = user_ratings.join(df_merge, \"movieId\")\n",
    "\n",
    "    # Đảm bảo tương đồng tính toán đúng\n",
    "    user_ratings = user_ratings.withColumn(\"similarity\", F.udf(lambda x: CosineSim(input_vec, [r[1] for r in all_movies_vecs if r[0] == int(x)][0]) if int(x) in [r[0] for r in all_movies_vecs] else 0)(F.col(\"movieId\")))\n",
    "\n",
    "    # Tính điểm dự đoán theo công thức\n",
    "    numerator = user_ratings.withColumn(\"weighted_score\", F.col(\"similarity\") * F.col(\"rating\"))\n",
    "    denominator = user_ratings.agg(F.sum(F.abs(F.col(\"similarity\")))).first()[0]\n",
    "\n",
    "    # Tổng hợp để tính điểm dự đoán\n",
    "    if denominator == 0:\n",
    "        return f\"No similar movies found for user {user_id}.\"\n",
    "\n",
    "    predicted_score = numerator.agg(F.sum(F.col(\"weighted_score\")).alias(\"numerator\")).first()[0] / denominator\n",
    "\n",
    "    return predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2253024e-7c80-4cfc-834a-26d6ec234440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###T TÍNH 1 uSER\n",
    "\n",
    "def get_all_cb_predictions1(user_id):\n",
    "    global df_merge, ratings\n",
    "\n",
    "    # Đọc dữ liệu ratings\n",
    "    ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "    ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "    # Đọc dữ liệu phim\n",
    "    df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/MergeData\")\n",
    "    df_merge = df_merge.withColumnRenamed(\"id\", \"movieId\").withColumn(\"movieId\", F.col(\"movieId\").cast(\"int\"))\n",
    "\n",
    "    # Lấy vector đặc trưng của các bộ phim\n",
    "    all_movies_vecs = get_vecs()\n",
    "    movie_vecs_dict = {int(r[0]): r[1] for r in all_movies_vecs}\n",
    "\n",
    "    # Lấy các phim mà người dùng đã đánh giá\n",
    "    user_ratings = ratings.filter(F.col(\"userId\") == user_id)\n",
    "    if user_ratings.count() == 0:\n",
    "        return f\"User {user_id} has not rated any movies.\"\n",
    "\n",
    "    # Chuyển dữ liệu ratings của người dùng thành danh sách\n",
    "    user_ratings_list = user_ratings.collect()\n",
    "\n",
    "    # Tính toán dự đoán cho tất cả các movieId mà người dùng đã đánh giá\n",
    "    prediction_results = []\n",
    "\n",
    "    for row in user_ratings_list:\n",
    "        movie_id = row[\"movieId\"]\n",
    "        actual_rating = row[\"rating\"]\n",
    "\n",
    "        # Bỏ qua phim không có vector đặc trưng\n",
    "        if movie_id not in movie_vecs_dict:\n",
    "            continue\n",
    "        \n",
    "        input_vec = movie_vecs_dict[movie_id]\n",
    "        similarities = []\n",
    "\n",
    "        for other_id in user_ratings_list:\n",
    "            if other_id.movieId == movie_id:\n",
    "                continue\n",
    "            if other_id.movieId not in movie_vecs_dict:\n",
    "                continue\n",
    "            other_vec = movie_vecs_dict[other_id.movieId]\n",
    "            sim = CosineSim(input_vec, other_vec)\n",
    "            similarities.append((sim, other_id[\"rating\"]))  # Sử dụng rating của phim tương tự\n",
    "\n",
    "\n",
    "        # Tính dự đoán điểm\n",
    "        if similarities:\n",
    "            numerator = sum(sim * rating for sim, rating in similarities)\n",
    "            denominator = sum(abs(sim) for sim, _ in similarities)\n",
    "            predicted_score = numerator / denominator if denominator != 0 else 0\n",
    "        else:\n",
    "            predicted_score = 0  # Nếu không có phim tương tự, dự đoán là 0\n",
    "\n",
    "        # Thêm vào kết quả\n",
    "        prediction_results.append((user_id, movie_id, round(predicted_score, 2), actual_rating))\n",
    "\n",
    "    # Chuyển kết quả sang DataFrame với 3 cột yêu cầu\n",
    "    prediction_df = spark.createDataFrame(prediction_results, [\"userId\", \"movieId\", \"predict\", \"true_predict\"])\n",
    "    return prediction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a6ae81e-fc79-465b-bf53-a3e295d33a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+------------+\n",
      "|userId|movieId|predict|true_predict|\n",
      "+------+-------+-------+------------+\n",
      "|  3422|    110|   4.17|         4.0|\n",
      "|  3422|    180|   4.13|         5.0|\n",
      "|  3422|    223|   4.13|         5.0|\n",
      "|  3422|    296|   4.13|         5.0|\n",
      "|  3422|    377|   4.17|         4.0|\n",
      "|  3422|    429|   4.22|         3.0|\n",
      "|  3422|   1380|   4.17|         4.0|\n",
      "|  3422|   1394|   4.17|         4.0|\n",
      "|  3422|   1721|   4.22|         3.0|\n",
      "|  3422|   1732|   4.13|         5.0|\n",
      "|  3422|   1895|   4.17|         4.0|\n",
      "|  3422|   2000|   4.12|         5.0|\n",
      "|  3422|   2042|   4.22|         3.0|\n",
      "|  3422|   2114|   4.13|         5.0|\n",
      "|  3422|   2502|   4.13|         5.0|\n",
      "|  3422|   2762|   4.22|         3.0|\n",
      "|  3422|   2791|   4.17|         4.0|\n",
      "|  3422|   3052|   4.13|         5.0|\n",
      "|  3422|   3072|   4.22|         3.0|\n",
      "|  3422|   3114|   4.13|         5.0|\n",
      "+------+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 3422\n",
    "predictions_df = get_all_cb_predictions1(user_id)\n",
    "predictions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee4facf2-ea45-48a5-a91c-07aafd6bd747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: long (nullable = true)\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- predict: double (nullable = true)\n",
      " |-- true_predict: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d6ad2-4698-43c4-9bad-c859f119e228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "\n",
    "# Đo thời gian bắt đầu tính RMSE\n",
    "start_time = time.time()\n",
    "\n",
    "# Giới hạn chỉ 100 dòng để tính RMSE\n",
    "\n",
    "# Tính RMSE\n",
    "rmse_df = predictions_df.withColumn(\n",
    "    \"squared_error\", F.pow(F.col(\"predict\") - F.col(\"true_predict\"), 2)\n",
    ")\n",
    "\n",
    "rmse_value = rmse_df.agg(F.sqrt(F.avg(F.col(\"squared_error\")))).first()[0]\n",
    "print(f\"RMSE  {rmse_value}\")\n",
    "\n",
    "# Đo thời gian kết thúc\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian xử lý RMSE: {round(end_time - start_time, 2)} giây\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc084dd-c17a-4e52-9830-1d3f96bb74bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
