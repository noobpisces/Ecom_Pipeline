{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06adaf1-6864-47ad-874f-ad5def0b0280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, IntegerType\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit, collect_list, struct,sqrt,avg\n",
    "from pyspark.sql import types as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad38475e-ec97-4a6c-8efd-36a92eed72a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enable-non-concurrent-writes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/17 18:23:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HybridRecommenderMaxPerformance\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d485ae2-778e-4502-8977-616f430f0413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/17 18:23:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/05/17 18:23:36 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "CB = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/hehe123\")\n",
    "filtered_CB = CB.filter(CB[\"id\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe08f8aa-c694-45e0-ab6e-836dda2a5473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "movie_vec_dict = {int(row.id): row.vecs for row in filtered_CB.select(\"id\", \"vecs\").collect()}\n",
    "b_movie_vec = sc.broadcast(movie_vec_dict)\n",
    "ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\")\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "ratings = ratings.dropna(subset=['userId', 'movieId'])\n",
    "\n",
    "ratings = ratings.withColumn('userId', ratings['userId'].cast('int')).\\\n",
    "withColumn('movieId', ratings['movieId'].cast('int')).withColumn('rating', ratings['rating'].cast('float'))\n",
    "train, validation, test = ratings.randomSplit([0.7, 0.2, 0.1], seed=0)\n",
    "test = test.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45199044-4f91-44b6-9620-346a6100f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "CF_model = ALSModel.load(\"s3a://lakehouse/CF/als_best_model\")\n",
    "PredcitCF = CF_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d76f7c-ab65-47b5-9508-79fa2da579af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rating(user_history, target_id):\n",
    "    vec_dict = b_movie_vec.value\n",
    "    target = vec_dict.get(target_id)\n",
    "    if target is None: return 0.0\n",
    "    num, den = 0.0, 0.0\n",
    "    for mid, r in user_history:\n",
    "        v = vec_dict.get(mid)\n",
    "        if v is None: continue\n",
    "        # cosine similarity\n",
    "        sim = float(np.dot(target, v) / (np.linalg.norm(target)*np.linalg.norm(v))) if np.linalg.norm(target)*np.linalg.norm(v)>0 else 0.0\n",
    "        num += sim * r\n",
    "        den += builtins.abs(sim)\n",
    "    return float(num/den) if den>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5457eefd-bc6e-44eb-945c-0f5943a46137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thay vì lấy lịch sử từ ratings_in_cb (dựa trên test), ta lấy từ train\n",
    "predict_udf = F.udf(predict_rating, FloatType())\n",
    "\n",
    "# 1. ratings_in_cb từ test (giữ nguyên để đánh giá)\n",
    "ratings_in_cb = (\n",
    "    test\n",
    "    .join(CB.select(F.col(\"id\").alias(\"cb_id\")), test.movieId == F.col(\"cb_id\"), \"inner\")\n",
    "    .select(\"userId\", \"movieId\", \"rating\")\n",
    ")\n",
    "\n",
    "# 2. Tạo ratings_in_cb_train từ train để gom lịch sử user\n",
    "ratings_in_cb_train = (\n",
    "    train\n",
    "    .join(CB.select(F.col(\"id\").alias(\"cb_id\")), train.movieId == F.col(\"cb_id\"), \"inner\")\n",
    "    .select(\"userId\", \"movieId\", \"rating\")\n",
    ")\n",
    "\n",
    "# 3. Gom lịch sử train thành array per user (dùng từ ratings_in_cb_train)\n",
    "train_hist = (\n",
    "    ratings_in_cb_train\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(\n",
    "        collect_list(\n",
    "            struct(\"movieId\",\"rating\")\n",
    "        ).alias(\"history\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Kết hợp sample_ratings (test) với history train (giữ nguyên)\n",
    "eval_df = (\n",
    "    ratings_in_cb\n",
    "    .join(train_hist, on=\"userId\", how=\"inner\")\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Dự đoán content-based rating bằng UDF và thêm cột predicted_rating_CT\n",
    "eval_df = (\n",
    "    eval_df\n",
    "    .withColumn(\"predicted_rating_CT\", predict_udf(\"history\", \"movieId\"))\n",
    "    # Join với CF prediction\n",
    "    .join(\n",
    "        PredcitCF.select(\"userId\", \"movieId\", \"prediction\").withColumnRenamed(\"prediction\", \"predicted_rating_CF\"),\n",
    "        on=[\"userId\", \"movieId\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570f2fd-4c01-4a1c-beef-774efbf1526d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee706481-6839-4c6b-aa3a-f041179eb242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============>    (6 + 2) / 8][Stage 34:=============>    (6 + 2) / 8]"
     ]
    }
   ],
   "source": [
    "# Chỉ giữ lại các cột cần thiết\n",
    "eval_df = eval_df.select(\"predicted_rating_CT\", \"predicted_rating_CF\", \"rating\")\n",
    "\n",
    "# Cache để tránh tính lại nhiều lần\n",
    "eval_df.cache()\n",
    "eval_df.count()  # Action để trigger cache\n",
    "\n",
    "# Hàm tính RMSE cho mỗi alpha\n",
    "def compute_rmse(alpha):\n",
    "    hybrid_df = eval_df.withColumn(\n",
    "        \"predicted_hybrid\",\n",
    "        lit(alpha) * col(\"predicted_rating_CT\") + (1 - alpha) * col(\"predicted_rating_CF\")\n",
    "    )\n",
    "    rmse = hybrid_df.withColumn(\n",
    "        \"squared_error\", (col(\"predicted_hybrid\") - col(\"rating\")) ** 2\n",
    "    ).agg(sqrt(avg(\"squared_error\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n",
    "    return rmse\n",
    "\n",
    "# Tìm best alpha\n",
    "best_alpha = None\n",
    "best_rmse = float(\"inf\")\n",
    "alpha_rmse_list = []\n",
    "\n",
    "for alpha in np.arange(0.0, 1.05, 0.05):\n",
    "    rmse = compute_rmse(alpha)\n",
    "    alpha_rmse_list.append((alpha, rmse))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_alpha = alpha\n",
    "\n",
    "# In kết quả\n",
    "print(\"Alpha - RMSE:\")\n",
    "for alpha, rmse in alpha_rmse_list:\n",
    "    print(f\"{alpha:.2f} -> {rmse:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Best alpha = {best_alpha:.2f} with RMSE = {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269829da-4180-44d3-b323-339d7d06babc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c253f-6fd4-46bd-9d85-b5a3879f01aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d32f6-f951-4e64-a5ce-c43157cf7e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326733f5-30bf-4fa9-99e0-3d847e3d41a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921cc1b-df8e-46da-b0fd-e65f979d6348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24950da2-b62c-42d8-9ac1-059f695e6896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
