{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4866e270-8907-48be-a0d2-d90cfb9f92db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, expr\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer,\n",
    "    Normalizer,\n",
    "    StopWordsCleaner,\n",
    "    BertEmbeddings,\n",
    "    SentenceEmbeddings,DistilBertEmbeddings\n",
    ")\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5db7e-959f-4667-a738-4c0fdf402726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f15a29f-bd2b-44ca-bc75-b0263fa58f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enable-non-concurrent-writes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/15 15:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MinIO with Delta Lake BERT\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#     .config(\"spark.driver.cores\", \"6\") \\\n",
    "#     .config(\"spark.executor.cores\", \"6\") \\\n",
    "#     .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\") \\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#     .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "#     .config(\"spark.dynamicAllocation.maxExecutors\", \"1\") \\\n",
    "#     .config(\"spark.dynamicAllocation.initialExecutors\", \"1\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "#     .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "#     .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "#     .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "#     .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "#     .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HybridRecommenderMaxPerformance\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/spark-nlp_2.12-5.1.3.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad379430-674b-4884-87cf-6c4665a65991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/15 15:28:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/gold/machineDataBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09888595-e5b3-48c6-a32f-d05011fd4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/15 15:32:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.load.\n: java.lang.NoClassDefFoundError: ai/onnxruntime/OrtEnvironment\n\tat com.johnsnowlabs.ml.onnx.OnnxWrapper$.com$johnsnowlabs$ml$onnx$OnnxWrapper$$withSafeOnnxModelLoader(OnnxWrapper.scala:86)\n\tat com.johnsnowlabs.ml.onnx.OnnxWrapper$.read(OnnxWrapper.scala:131)\n\tat com.johnsnowlabs.ml.onnx.ReadOnnxModel.readOnnxModel(OnnxSerializeModel.scala:101)\n\tat com.johnsnowlabs.ml.onnx.ReadOnnxModel.readOnnxModel$(OnnxSerializeModel.scala:74)\n\tat com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings$.readOnnxModel(DistilBertEmbeddings.scala:493)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.readModel(DistilBertEmbeddings.scala:438)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.readModel$(DistilBertEmbeddings.scala:429)\n\tat com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings$.readModel(DistilBertEmbeddings.scala:493)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.$anonfun$$init$$1(DistilBertEmbeddings.scala:446)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.$anonfun$$init$$1$adapted(DistilBertEmbeddings.scala:446)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1(ParamsAndFeaturesReadable.scala:50)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1$adapted(ParamsAndFeaturesReadable.scala:49)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.onRead(ParamsAndFeaturesReadable.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1$adapted(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:38)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: ai.onnxruntime.OrtEnvironment\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 58 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PipelineModel\n\u001b[0;32m----> 3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://lakehouse/bert_pipeline1000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/ml/util.py:353\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:284\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    282\u001b[0m metadata \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39mloadMetadata(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJavaMLReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJavaMLReadable[PipelineModel]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     uid, stages \u001b[38;5;241m=\u001b[39m PipelineSharedReadWrite\u001b[38;5;241m.\u001b[39mload(metadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc, path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/ml/util.py:302\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 302\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[1;32m    306\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.load.\n: java.lang.NoClassDefFoundError: ai/onnxruntime/OrtEnvironment\n\tat com.johnsnowlabs.ml.onnx.OnnxWrapper$.com$johnsnowlabs$ml$onnx$OnnxWrapper$$withSafeOnnxModelLoader(OnnxWrapper.scala:86)\n\tat com.johnsnowlabs.ml.onnx.OnnxWrapper$.read(OnnxWrapper.scala:131)\n\tat com.johnsnowlabs.ml.onnx.ReadOnnxModel.readOnnxModel(OnnxSerializeModel.scala:101)\n\tat com.johnsnowlabs.ml.onnx.ReadOnnxModel.readOnnxModel$(OnnxSerializeModel.scala:74)\n\tat com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings$.readOnnxModel(DistilBertEmbeddings.scala:493)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.readModel(DistilBertEmbeddings.scala:438)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.readModel$(DistilBertEmbeddings.scala:429)\n\tat com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings$.readModel(DistilBertEmbeddings.scala:493)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.$anonfun$$init$$1(DistilBertEmbeddings.scala:446)\n\tat com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel.$anonfun$$init$$1$adapted(DistilBertEmbeddings.scala:446)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1(ParamsAndFeaturesReadable.scala:50)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1$adapted(ParamsAndFeaturesReadable.scala:49)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.onRead(ParamsAndFeaturesReadable.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1$adapted(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:38)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: ai.onnxruntime.OrtEnvironment\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 58 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "nlp = PipelineModel.load(\"s3a://lakehouse/bert_pipeline1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf726f8-e4cf-4627-9ef4-237ddc27a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/15 13:19:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS Cosine Index ch·ª©a 20658 vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "vec_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/bert2\")\n",
    "rows = vec_df.select(\"id\", \"vecs\").rdd.map(lambda row: (row[\"id\"], np.array(row[\"vecs\"], dtype=np.float32))).collect()\n",
    "\n",
    "ids = [r[0] for r in rows]\n",
    "vectors = np.stack([r[1] for r in rows])\n",
    "\n",
    "norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "normalized_vectors = vectors / norms\n",
    "\n",
    "index = faiss.IndexFlatIP(normalized_vectors.shape[1])\n",
    "index.add(normalized_vectors)\n",
    "\n",
    "print(f\"‚úÖ FAISS Cosine Index ch·ª©a {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f8448bf-1e17-4f6b-a975-e50697d70341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Top k·∫øt qu·∫£ (Cosine Similarity):\n",
      "1. Movie ID: 177271, Cosine Score: 0.7915\n",
      "2. Movie ID: 17445, Cosine Score: 0.7694\n",
      "3. Movie ID: 1924, Cosine Score: 0.7691\n",
      "4. Movie ID: 103269, Cosine Score: 0.7639\n",
      "5. Movie ID: 72693, Cosine Score: 0.7617\n",
      "6. Movie ID: 283995, Cosine Score: 0.7596\n",
      "7. Movie ID: 323027, Cosine Score: 0.7593\n",
      "8. Movie ID: 436969, Cosine Score: 0.7585\n",
      "9. Movie ID: 16237, Cosine Score: 0.7573\n",
      "10. Movie ID: 3604, Cosine Score: 0.7551\n",
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id    |comb_bert                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|177271|Title: Lego Batman: The Movie - DC Super Heroes Unite. This movie is titled 'Lego Batman: The Movie - DC Super Heroes Unite'. Tagline: Fighting Crime Brick by Brick. Tagline repeated: Fighting Crime Brick by Brick. Plot summary: Joker teams up with Lex Luthor to destroy the world one brick at a time. It's up to Batman, Superman and the rest of the Justice League to stop them.. This is a story about: Joker teams up with Lex Luthor to destroy the world one brick at a time. It's up to Batman, Superman and the rest of the Justice League to stop them..                                                                                                           |\n",
      "|3604  |Title: Flash Gordon. This movie is titled 'Flash Gordon'. Tagline: He'll save every one of us!. Tagline repeated: He'll save every one of us!. Plot summary: A football player and his friends travel to the planet Mongo and find themselves fighting the tyrant, Ming the Merciless, to save Earth.. This is a story about: A football player and his friends travel to the planet Mongo and find themselves fighting the tyrant, Ming the Merciless, to save Earth..                                                                                                                                                                                                             |\n",
      "|16237 |Title: Teen Titans: Trouble in Tokyo. This movie is titled 'Teen Titans: Trouble in Tokyo'. Tagline: The Titans go ninja on the notorious villain Brushogun in their awesome first feature-length movie.. Tagline repeated: The Titans go ninja on the notorious villain Brushogun in their awesome first feature-length movie.. Plot summary: America's coolest heroes, the Teen Titans, go to Tokyo to track down the mysterious Japanese criminal Brushogun.. This is a story about: America's coolest heroes, the Teen Titans, go to Tokyo to track down the mysterious Japanese criminal Brushogun..                                                                           |\n",
      "|103269|Title: Superman vs. The Elite. This movie is titled 'Superman vs. The Elite'. Tagline: World saved. Humanity lost.. Tagline repeated: World saved. Humanity lost.. Plot summary: The Man of Steel finds himself outshone by a new team of ruthless superheroes who hold his idealism in contempt.. This is a story about: The Man of Steel finds himself outshone by a new team of ruthless superheroes who hold his idealism in contempt..                                                                                                                                                                                                                                         |\n",
      "|283995|Title: Guardians of the Galaxy Vol. 2. This movie is titled 'Guardians of the Galaxy Vol. 2'. Tagline: Anyone can save the galaxy once.. Tagline repeated: Anyone can save the galaxy once.. Plot summary: The Guardians must fight to keep their newfound family together as they unravel the mysteries of Peter Quill's true parentage.. This is a story about: The Guardians must fight to keep their newfound family together as they unravel the mysteries of Peter Quill's true parentage..                                                                                                                                                                                   |\n",
      "|436969|Title: The Suicide Squad. This movie is titled 'The Suicide Squad'. Tagline: They're dying to save the world.. Tagline repeated: They're dying to save the world.. Plot summary: Supervillains Harley Quinn, Bloodsport, Peacemaker and a collection of nutty cons at Belle Reve prison join the super-secret, super-shady Task Force X as they are dropped off at the remote, enemy-infused island of Corto Maltese.. This is a story about: Supervillains Harley Quinn, Bloodsport, Peacemaker and a collection of nutty cons at Belle Reve prison join the super-secret, super-shady Task Force X as they are dropped off at the remote, enemy-infused island of Corto Maltese.. |\n",
      "|1924  |Title: Superman. This movie is titled 'Superman'. Tagline: You'll Believe a Man Can Fly!. Tagline repeated: You'll Believe a Man Can Fly!. Plot summary: Mild-mannered Clark Kent works as a reporter at the Daily Planet alongside his crush, Lois Lane ‚àí who's in love with Superman. Clark must summon his superhero alter ego when the nefarious Lex Luthor launches a plan to take over the world.. This is a story about: Mild-mannered Clark Kent works as a reporter at the Daily Planet alongside his crush, Lois Lane ‚àí who's in love with Superman. Clark must summon his superhero alter ego when the nefarious Lex Luthor launches a plan to take over the world..     |\n",
      "|323027|Title: Justice League: Gods and Monsters. This movie is titled 'Justice League: Gods and Monsters'. Tagline: From dark heroes, justice will dawn.. Tagline repeated: From dark heroes, justice will dawn.. Plot summary: In an alternate universe, very different versions of DC's Trinity fight against the government after they are framed for an embassy bombing.. This is a story about: In an alternate universe, very different versions of DC's Trinity fight against the government after they are framed for an embassy bombing..                                                                                                                                         |\n",
      "|17445 |Title: Green Lantern: First Flight. This movie is titled 'Green Lantern: First Flight'. Tagline: Superman was born a hero. Batman became a hero. But only Green Lantern was chosen.. Tagline repeated: Superman was born a hero. Batman became a hero. But only Green Lantern was chosen.. Plot summary: Test pilot Hal Jordan finds himself recruited as the newest member of the intergalactic police force, The Green Lantern Corps.. This is a story about: Test pilot Hal Jordan finds himself recruited as the newest member of the intergalactic police force, The Green Lantern Corps..                                                                                     |\n",
      "|72693 |Title: Master of the World. This movie is titled 'Master of the World'. Tagline: The fabulous adventures of the man who conquered the earth to save it!. Tagline repeated: The fabulous adventures of the man who conquered the earth to save it!. Plot summary: A mad genius tries to bomb the world into peace.. This is a story about: A mad genius tries to bomb the world into peace..                                                                                                                                                                                                                                                                                         |\n",
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Truy v·∫•n ng∆∞·ªùi d√πng nhwos nh·∫≠p ch·ªï n√†y\n",
    "##\n",
    "#\n",
    "user_query = \"superhero save the earth\"\n",
    "\n",
    "\n",
    "\n",
    "query_df = spark.createDataFrame([[user_query]], [\"comb_bert\"])\n",
    "\n",
    "# L·∫•y vector\n",
    "query_vector = nlp.transform(query_df) \\\n",
    "    .select(col(\"sentence_embeddings.embeddings\")[0].alias(\"vec\")) \\\n",
    "    .collect()[0][\"vec\"]\n",
    "query_vector = np.array(query_vector, dtype=np.float32)\n",
    "\n",
    "# Normalize\n",
    "query_vector /= np.linalg.norm(query_vector)\n",
    "query_vector = query_vector.reshape(1, -1)\n",
    "\n",
    "# FAISS search\n",
    "k = 10\n",
    "scores, indices = index.search(query_vector, k)\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"üéØ Top k·∫øt qu·∫£ (Cosine Similarity):\")\n",
    "matched_ids = []\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    matched_ids.append(ids[idx])\n",
    "    print(f\"{i+1}. Movie ID: {ids[idx]}, Cosine Score: {scores[0][i]:.4f}\")\n",
    "\n",
    "# L·ªçc k·∫øt qu·∫£ t·ª´ DataFrame\n",
    "results = spark_df.filter(col(\"id\").isin(matched_ids)) \\\n",
    "                  .select(\"id\", \"comb_bert\")\n",
    "\n",
    "results.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ed9b6-7b88-46b9-8b79-878e5e2fe010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73024d8c-4b64-4b75-9be3-85bee1846f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a0e0-9b8e-4bd1-bada-b60a0182485f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5be00-2a63-468e-8b4c-d7d5878972f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa4903-62fa-444b-990a-21cc2ee260a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254a27a-d1a4-4521-9637-8ddab5921a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb22095-c061-41a9-9a98-3ee6df243006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1f090-79e8-4134-b54c-4346eae2c852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ad55e-9dcc-4d76-97aa-501c45f62604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e43532-676d-4f19-83ba-69bb4ece8409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77ae41-6af0-4ceb-b2d6-c4c8d0ff1da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd4f21-46a0-4783-8826-67a9c01daf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8cb0f2-0fb6-4632-976c-aa30bd1993fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
