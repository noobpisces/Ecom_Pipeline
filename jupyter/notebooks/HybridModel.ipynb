{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28864f54-881d-4d80-9a9c-4ab886c7d2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9317314f-3d5e-4a42-84ca-b7dcfcdcc9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enable-non-concurrent-writes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-76652a8a-dc82-4a06-8fe4-fe3de8d5a071;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.1.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 1778ms :: artifacts dl 119ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.1.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-76652a8a-dc82-4a06-8fe4-fe3de8d5a071\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/30ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/16 13:55:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HybridRecommenderMaxPerformance\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36e681-da11-4f20-b24d-f8ead5c456bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1143cb58-fdd8-44d3-8a88-c5bf78f8b4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/14 18:18:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\").cache()\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d86267-75f3-436b-b7bd-0514b6ee2e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b18cb66-b5aa-4bb5-a862-8be62dca5996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "CF = ALSModel.load(\"s3a://lakehouse/data/CF_als_best_model\")\n",
    "# Đọc dữ liệu movie_names và ratings_data từ Delta\n",
    "movies  = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/dim_movie\")\n",
    "movies  = movies .select(\"id\", \"title\")\n",
    "movies  = movies .withColumnRenamed(\"id\", \"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d723aaf-c009-4f3b-a721-6dcf8bc8c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/16 13:57:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/06/16 13:57:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "t  = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/bertTest\")\n",
    "t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaff6e8-40fe-40db-9d0a-ac9f9cbd2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "45433"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2dc65ef-39e4-45a4-9535-6b2d1c8d3607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "29718"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/16 14:05:00 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/06/16 14:05:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:218)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:923)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/16 14:05:00 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/06/16 14:05:00 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "a  = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/bertTestHiHi\")\n",
    "a.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1064e186-5b86-4d5c-937c-26371e43c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies liked by user with ID 123\n",
      "+-------+-------------------------------------------------+------+\n",
      "|movieId|title                                            |rating|\n",
      "+-------+-------------------------------------------------+------+\n",
      "|11     |Star Wars                                        |5.0   |\n",
      "|278    |The Shawshank Redemption                         |3.0   |\n",
      "|424    |Schindler's List                                 |4.5   |\n",
      "|562    |Die Hard                                         |4.0   |\n",
      "|1891   |The Empire Strikes Back                          |5.0   |\n",
      "|2493   |The Princess Bride                               |5.0   |\n",
      "|85     |Raiders of the Lost Ark                          |5.0   |\n",
      "|1892   |Return of the Jedi                               |4.0   |\n",
      "|137    |Groundhog Day                                    |2.5   |\n",
      "|105    |Back to the Future                               |4.5   |\n",
      "|89     |Indiana Jones and the Last Crusade               |4.0   |\n",
      "|857    |Saving Private Ryan                              |3.0   |\n",
      "|120    |The Lord of the Rings: The Fellowship of the Ring|4.0   |\n",
      "|121    |The Lord of the Rings: The Two Towers            |4.0   |\n",
      "|122    |The Lord of the Rings: The Return of the King    |4.0   |\n",
      "|272    |Batman Begins                                    |3.0   |\n",
      "|155    |The Dark Knight                                  |2.5   |\n",
      "|13475  |Star Trek                                        |2.5   |\n",
      "|118340 |Guardians of the Galaxy                          |3.5   |\n",
      "|140607 |Star Wars: The Force Awakens                     |5.0   |\n",
      "+-------+-------------------------------------------------+------+\n",
      "\n",
      "Recommended movies for user with ID 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:=================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------+----------+\n",
      "|movieId|title                         |prediction|\n",
      "+-------+------------------------------+----------+\n",
      "|267506 |UFO - Distruggete base Luna!  |6.095041  |\n",
      "|84601  |The Wearing of the Grin       |5.9462733 |\n",
      "|373453 |Brahman Naman                 |5.5657406 |\n",
      "|189820 |Fifth Avenue, New York        |5.411268  |\n",
      "|37193  |Hue and Cry                   |5.2873645 |\n",
      "|52488  |A More Perfect Union          |5.236807  |\n",
      "|363846 |Motherland                    |5.1648088 |\n",
      "|143453 |Thrust in Me                  |5.1428375 |\n",
      "|250275 |Getting That Girl             |5.0788    |\n",
      "|36952  |Dreams with Sharp Teeth       |5.016851  |\n",
      "|439314 |The Garden of Afflictions 2017|4.9734693 |\n",
      "|55739  |Single Santa Seeks Mrs. Claus |4.9212956 |\n",
      "|156141 |Stalingrad                    |4.918215  |\n",
      "|48955  |Everybody Go Home             |4.9170036 |\n",
      "|53419  |Behind the Screen             |4.866202  |\n",
      "|80809  |Perry Mason Returns           |4.818535  |\n",
      "|10844  |Out of the Blue               |4.808767  |\n",
      "|28666  |The Woman Chaser              |4.7972474 |\n",
      "|92662  |The Hollywood Complex         |4.7689853 |\n",
      "|167858 |The Story of the Voyages      |4.7362127 |\n",
      "+-------+------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# select a single user from the test set\n",
    "user_id = 123\n",
    "single_user_ratings = ratings.filter(ratings['userId'] == user_id).select(['movieId', 'userId', 'rating'])\n",
    "\n",
    "# display the movies the user has liked\n",
    "print(\"Movies liked by user with ID\", user_id)\n",
    "single_user_ratings.join(movies, 'movieId').select('movieId', 'title', 'rating').show(truncate=False)\n",
    "\n",
    "# generate recommendations for the user\n",
    "all_movies = ratings.select('movieId').distinct()\n",
    "user_movies = single_user_ratings.select('movieId').distinct()\n",
    "movies_to_recommend = all_movies.subtract(user_movies)\n",
    "\n",
    "# predict ratings for movies the user has not rated yet\n",
    "recommendations = CF.transform(movies_to_recommend.withColumn('userId', lit(user_id)))\n",
    "\n",
    "# filter out the movies that the user has already rated or seen (this filters out the movies that the user has not liked as well)\n",
    "recommendations = recommendations.filter(col('prediction') > 0)\n",
    "\n",
    "# display the recommendations with movie names\n",
    "print(\"Recommended movies for user with ID\", user_id)\n",
    "recommended_movies = recommendations.join(movies, 'movieId').select('movieId', 'title', 'prediction')\n",
    "\n",
    "# Sort recommended movies by prediction in descending order\n",
    "ordered_recommendations = recommended_movies.orderBy(col('prediction').desc())\n",
    "\n",
    "# Display the ordered recommendations\n",
    "ordered_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa1e1bb1-f453-4180-9c58-4b9717426a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movie name:  Toy Story\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 142:>                                                        (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------+----------+\n",
      "|id    |title                                 |score     |\n",
      "+------+--------------------------------------+----------+\n",
      "|863   |Toy Story 2                           |0.9661569 |\n",
      "|10193 |Toy Story 3                           |0.9649793 |\n",
      "|33914 |Frankenweenie                         |0.9564637 |\n",
      "|62214 |Frankenweenie                         |0.955091  |\n",
      "|888   |The Flintstones                       |0.9543923 |\n",
      "|13378 |Mickey's Twice Upon a Christmas       |0.9543722 |\n",
      "|13397 |The Year Without a Santa Claus        |0.95293456|\n",
      "|10996 |Stuart Little 2                       |0.9511653 |\n",
      "|258509|Alvin and the Chipmunks: The Road Chip|0.95008785|\n",
      "|11114 |Pete's Dragon                         |0.94886434|\n",
      "+------+--------------------------------------+----------+\n",
      "\n",
      "Total Runtime: 9.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "import numpy as np\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import time\n",
    "start_time = time.time()\n",
    "# # Đọc và cache dữ liệu ratings\n",
    "# ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\") \\\n",
    "#     .select(\"userId\", \"movieId\", \"rating\") \\\n",
    "#     .dropna(subset=['userId', 'movieId']) \\\n",
    "#     .withColumn('userId', F.col('userId').cast(IntegerType())) \\\n",
    "#     .withColumn('rating', F.col('rating').cast(FloatType())) \\\n",
    "#     .cache()\n",
    "\n",
    "\n",
    "# ratings = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/ratings\").cache()\n",
    "# ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "# CF = ALSModel.load(\"s3a://lakehouse/data/CF_als_best_model\")\n",
    "\n",
    "# Đọc dữ liệu phim và kiểm tra tên phim\n",
    "movie_name = input(\"Enter a movie name: \")\n",
    "df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/MergeData\").cache()\n",
    "\n",
    "movie_df = df_merge.filter(F.col(\"title\") == movie_name).select(\"id\").limit(1)\n",
    "if movie_df.count() == 0:\n",
    "    print(\"Sorry! The movie you searched is not in our database.\")\n",
    "else:\n",
    "    # Lấy ID phim đầu vào\n",
    "    m_id = movie_df.collect()[0]['id']\n",
    "\n",
    "    # Đọc dữ liệu vector phim và sử dụng Broadcast cho vector phim đầu vào\n",
    "    df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/bertTestHiHi\").cache()\n",
    "    input_vec = df_list.filter(F.col(\"id\") == m_id).select(\"vecs\").collect()[0][0]\n",
    "    broadcast_input_vec = spark.sparkContext.broadcast(np.array(input_vec))\n",
    "\n",
    "    # Định nghĩa UDF tính cosine similarity\n",
    "    def cosine_similarity(v1, v2):\n",
    "        v1 = np.array(v1)\n",
    "        v2 = np.array(v2)\n",
    "        numerator = float(np.dot(v1, v2))\n",
    "        denominator = float(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        return numerator / denominator if denominator != 0 else 0.0\n",
    "\n",
    "    cosine_udf = F.udf(lambda vec: float(cosine_similarity(broadcast_input_vec.value, vec)), FloatType())\n",
    "\n",
    "    # Tính toán cosine similarity cho tất cả phim\n",
    "    recommendations_spark_df = df_list \\\n",
    "        .withColumn(\"score\", cosine_udf(F.col(\"vecs\"))) \\\n",
    "        .filter(F.col(\"id\") != m_id) \\\n",
    "        .select(F.col(\"id\").alias(\"movies_id\"), \"score\") \\\n",
    "        .orderBy(F.col(\"score\").desc()) \\\n",
    "        .limit(10)\n",
    "\n",
    "    recommendations_spark_df = recommendations_spark_df.join(\n",
    "        df_merge,\n",
    "        F.col(\"movies_id\") == F.col(\"id\"),\n",
    "        'inner'\n",
    "    )\n",
    "\n",
    "    # Chọn các cột mong muốn và hiển thị kết quả\n",
    "    recommendations_spark_df.select(\n",
    "        recommendations_spark_df[\"id\"],\n",
    "        recommendations_spark_df['title'],\n",
    "        recommendations_spark_df[\"score\"]\n",
    "    ).orderBy(F.col(\"score\").desc()).show(truncate=False)\n",
    "\n",
    "    # Hiển thị kết quả\n",
    "    # recommendations_spark_df.show()\n",
    "print('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce2f764b-9c11-4fbc-be3a-99085f511ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies liked by user with ID 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+\n",
      "|movieId|               title|rating|\n",
      "+-------+--------------------+------+\n",
      "|    157|Star Trek III: Th...|   1.0|\n",
      "|    639|When Harry Met Sa...|   3.5|\n",
      "|   2640|            Heathers|   3.0|\n",
      "|   8849|               Alfie|   4.0|\n",
      "|   9340|         The Goonies|   3.0|\n",
      "|   9715|         Hope Floats|   4.5|\n",
      "|  11224|          Cinderella|   3.0|\n",
      "+-------+--------------------+------+\n",
      "\n",
      "Recommended movies for user with ID 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n",
      "|movieId|               title|prediction|\n",
      "+-------+--------------------+----------+\n",
      "| 397403|   Brexit: The Movie|  6.505104|\n",
      "|   1731|               Helen|  6.163009|\n",
      "| 393841|      Pelli Choopulu|  5.947569|\n",
      "| 412758|          Eat Locals| 5.9273725|\n",
      "| 355177|Tig Notaro: Boyis...| 5.9270086|\n",
      "| 261810|            Silenced| 5.8784776|\n",
      "| 409502|     I'm Not Ashamed|  5.839202|\n",
      "| 228198|Eu Não Faço a Men...|  5.811181|\n",
      "| 407036|        On the Brain| 5.6171036|\n",
      "| 461088|50 Kilos of Sour ...| 5.5664444|\n",
      "|  65488|     Raintree County|  5.538809|\n",
      "|  56759| Notre Dame de Paris|   5.46525|\n",
      "|  11496|They Call Me Rene...| 5.4383755|\n",
      "| 398854|A Midsummer Night...|  5.412646|\n",
      "| 176088|   The Story of Luke|  5.370575|\n",
      "|  63039|          Noah's Ark| 5.3683953|\n",
      "|  42427|           Fair Play|  5.366928|\n",
      "|  17022|Michael Jackson: ...| 5.3630924|\n",
      "|  15199|Rent: Filmed Live...|  5.343305|\n",
      "| 432025|The Art of Loving...|  5.272885|\n",
      "+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# select a single user from the test set\n",
    "user_id = 12\n",
    "single_user_ratings = test.filter(test['userId'] == user_id).select(['movieId', 'userId', 'rating'])\n",
    "\n",
    "# display the movies the user has liked\n",
    "print(\"Movies liked by user with ID\", user_id)\n",
    "single_user_ratings.join(movies, 'movieId').select('movieId', 'title', 'rating').show()\n",
    "\n",
    "# generate recommendations for the user\n",
    "all_movies = df.select('movieId').distinct()\n",
    "user_movies = single_user_ratings.select('movieId').distinct()\n",
    "movies_to_recommend = all_movies.subtract(user_movies)\n",
    "\n",
    "# predict ratings for movies the user has not rated yet\n",
    "recommendations = final_model.transform(movies_to_recommend.withColumn('userId', lit(user_id)))\n",
    "\n",
    "# filter out the movies that the user has already rated or seen (this filters out the movies that the user has not liked as well)\n",
    "recommendations = recommendations.filter(col('prediction') > 0)\n",
    "\n",
    "# display the recommendations with movie names\n",
    "print(\"Recommended movies for user with ID\", user_id)\n",
    "recommended_movies = recommendations.join(movies, 'movieId').select('movieId', 'title', 'prediction')\n",
    "\n",
    "# Sort recommended movies by prediction in descending order\n",
    "ordered_recommendations = recommended_movies.orderBy(col('prediction').desc())\n",
    "\n",
    "# Display the ordered recommendations\n",
    "ordered_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a263ec6-40e6-4abf-a323-3085f67b64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/bertTestHi\").cache()\n",
    "df_merge = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/MergeData\").cache()\n",
    "# Đọc dữ liệu movie_names và ratings_data từ Delta\n",
    "movies  = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/dim_movie\")\n",
    "movies  = movies .select(\"id\", \"title\")\n",
    "movies  = movies .withColumnRenamed(\"id\", \"movieId\")\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "temp = ALSModel.load(\"s3a://lakehouse/data/CF_als_best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02789a2d-606b-4dc5-b5ed-ee23c2a0cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "# 1. Chọn 1 user\n",
    "user_id = 123\n",
    "single_user_ratings = ratings.filter(ratings['userId'] == user_id).select(['movieId', 'userId', 'rating'])\n",
    "\n",
    "# 2. In các phim user đã rating\n",
    "print(\"Phim user đã xem:\")\n",
    "single_user_ratings.join(df_merge, single_user_ratings['movieId'] == df_merge['id']).select(\n",
    "    \"title\", \"genres\", \"director_names\", \"cast_names\", \"popularity\", \"vote_average\", \"vote_count\", \"rating\").show()\n",
    "\n",
    "# 3. Lấy danh sách phim chưa xem\n",
    "all_movies = df.select('movieId').distinct()\n",
    "user_movies = single_user_ratings.select('movieId').distinct()\n",
    "movies_to_recommend = all_movies.subtract(user_movies)\n",
    "\n",
    "# 4. Dự đoán bằng CF\n",
    "recommendations = temp.transform(movies_to_recommend.withColumn('userId', lit(user_id)))\n",
    "recommendations = recommendations.filter(col('prediction') > 3.7)\n",
    "\n",
    "# 5. Join info phim\n",
    "recommended_movies = recommendations.join(df_merge, recommendations['movieId'] == df_merge['id']).select(\n",
    "    \"title\", \"genres\",  \"popularity\", \"vote_average\", \n",
    "    \"vote_count\", \"movieId\", \"prediction\"\n",
    ")\n",
    "\n",
    "# 6. Lấy vector các phim user đã rating\n",
    "rated_movies = single_user_ratings.join(df_list, single_user_ratings.movieId == df_list.id)\n",
    "\n",
    "# 7. Tính vector trung bình\n",
    "user_vec = rated_movies.select('vecs').rdd.map(lambda row: np.array(row['vecs'])).mean()\n",
    "broadcast_user_vec = spark.sparkContext.broadcast(user_vec)\n",
    "\n",
    "# 8. UDF tính cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    numerator = float(np.dot(v1, v2))\n",
    "    denominator = float(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    return numerator / denominator if denominator != 0 else 0.0\n",
    "\n",
    "cosine_udf = F.udf(lambda vec: float(cosine_similarity(broadcast_user_vec.value, vec)), FloatType())\n",
    "\n",
    "# 9. Join vector vào các phim gợi ý và tính similarity\n",
    "cb_filtered = recommended_movies.join(df_list, recommended_movies.movieId == df_list.id)\n",
    "cb_filtered = cb_filtered.withColumn('similarity', cosine_udf(F.col('vecs')))\n",
    "\n",
    "# 10. Sắp xếp theo similarity\n",
    "cb_filtered = cb_filtered.orderBy(F.col('similarity').desc())\n",
    "\n",
    "# 11. Hiển thị kết quả sau lọc bằng CB\n",
    "print(\"Phim được lọc lại bằng CB (cosine):\")\n",
    "cb_filtered.select(\n",
    "    \"title\", \"genres\", \"popularity\", \"vote_average\", \n",
    "    \"vote_count\", \"similarity\",\"prediction\"\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
