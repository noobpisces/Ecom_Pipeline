{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4866e270-8907-48be-a0d2-d90cfb9f92db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, expr\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer,\n",
    "    Normalizer,\n",
    "    StopWordsCleaner,\n",
    "    BertEmbeddings,\n",
    "    SentenceEmbeddings,DistilBertEmbeddings\n",
    ")\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f15a29f-bd2b-44ca-bc75-b0263fa58f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enable-non-concurrent-writes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-acf371e4-e0db-4002-a6ce-f4374a9b966e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.1.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 1551ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.1.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-acf371e4-e0db-4002-a6ce-f4374a9b966e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/15 05:15:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta Lake BERT\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.cores\", \"6\") \\\n",
    "    .config(\"spark.executor.cores\", \"6\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f712d5-e230-4454-9897-eaeea88ff103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ xu·∫•t ra Excel t·∫°i: ./kkk/machineDataBert.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Parquet\n",
    "spark_df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/gold/machineDataBert\")\n",
    "\n",
    "pandas_df = spark_df.toPandas()\n",
    "\n",
    "# Ghi ra file Excel\n",
    "output_path = \"./kkk/machineDataBert.xlsx\"  # ho·∫∑c ƒë∆∞·ªùng d·∫´n b·∫°n mu·ªën\n",
    "pandas_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ xu·∫•t ra Excel t·∫°i: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad379430-674b-4884-87cf-6c4665a65991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df = spark.read.format(\"parquet\").load(\"s3a://lakehouse/gold/machineDataBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "795002fe-d8b9-48fe-9062-0baab0433831",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df2 = spark.read.format(\"parquet\").load(\"s3a://lakehouse/gold/machineDataTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cadee4c3-6d71-41f3-9f72-f00399be528a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert_base_uncased download started this may take some time.\n",
      "Approximate size to download 235.8 MB\n",
      "[OK!]\n",
      "25/06/15 03:36:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comb_bert\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "distilbert_embeddings = DistilBertEmbeddings.pretrained(\"distilbert_base_uncased\", \"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"bert_vec\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setMaxSentenceLength(512)\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"bert_vec\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    distilbert_embeddings,\n",
    "    sentence_embeddings\n",
    "])\n",
    "\n",
    "dummy_df = spark.createDataFrame([[\"dummy\"]], [\"comb_bert\"])\n",
    "nlp_pipeline_model = nlp_pipeline.fit(dummy_df)\n",
    "processed_df = nlp_pipeline_model.transform(spark_df)\n",
    "\n",
    "\n",
    "# nlp_pipeline_model = nlp_pipeline.fit(spark_df)\n",
    "# processed_df = nlp_pipeline_model.transform(spark_df)\n",
    "\n",
    "\n",
    "processed_df = processed_df.select(\"id\", \"sentence_embeddings.embeddings\")\n",
    "processed_df = processed_df.withColumn(\"vecs\", col(\"embeddings\")[0]).drop(\"embeddings\")\n",
    "processed_df.select('id', 'vecs').write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/bert2\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664f2044-b154-4763-a7a1-2018d8be5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline_model.write().overwrite().save(\"./work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb35efa-6715-4be1-891a-76fc21f69148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# K·∫øt n·ªëi t·ªõi MinIO\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=\"conbo123\",\n",
    "    aws_secret_access_key=\"123conbo\",\n",
    "    endpoint_url='http://minio:9000'\n",
    ")\n",
    "def delete_s3_folder(bucket, s3_prefix):\n",
    "    \"\"\"\n",
    "    X√≥a t·∫•t c·∫£ c√°c object trong bucket theo prefix (gi·ªëng nh∆∞ x√≥a th∆∞ m·ª•c).\n",
    "    \"\"\"\n",
    "    print(f\"Deleting existing objects in s3://{bucket}/{s3_prefix}\")\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=s3_prefix)\n",
    "\n",
    "    keys_to_delete = []\n",
    "    for page in pages:\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            keys_to_delete.append({\"Key\": obj[\"Key\"]})\n",
    "\n",
    "    if keys_to_delete:\n",
    "        # Xo√° theo batch (t·ªëi ƒëa 1000 object m·ªói l·∫ßn)\n",
    "        for i in range(0, len(keys_to_delete), 1000):\n",
    "            batch = keys_to_delete[i:i+1000]\n",
    "            s3_client.delete_objects(Bucket=bucket, Delete={\"Objects\": batch})\n",
    "        print(f\"Deleted {len(keys_to_delete)} objects.\")\n",
    "    else:\n",
    "        print(\"No existing objects found.\")\n",
    "def upload_folder_to_s3(local_folder, bucket, s3_prefix):\n",
    "    \"\"\"\n",
    "    Upload to√†n b·ªô th∆∞ m·ª•c local_folder l√™n MinIO (ho·∫∑c S3) v√†o bucket/s3_prefix.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            # T√≠nh ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi so v·ªõi th∆∞ m·ª•c g·ªëc\n",
    "            relative_path = os.path.relpath(local_path, local_folder)\n",
    "            # S·ª≠ d·ª•ng d·∫•u \"/\" ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi S3 path\n",
    "            s3_key = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "            print(f\"Uploading {local_path} to s3://{bucket}/{s3_key}\")\n",
    "            s3_client.upload_file(local_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579d6dcc-b733-4483-82b1-9b2f53135707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing objects in s3://lakehouse/bert_pipeline1000\n",
      "No existing objects found.\n",
      "Uploading ./work/metadata/.part-00000.crc to s3://lakehouse/bert_pipeline1000/metadata/.part-00000.crc\n",
      "Uploading ./work/metadata/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/metadata/._SUCCESS.crc\n",
      "Uploading ./work/metadata/part-00000 to s3://lakehouse/bert_pipeline1000/metadata/part-00000\n",
      "Uploading ./work/metadata/_SUCCESS to s3://lakehouse/bert_pipeline1000/metadata/_SUCCESS\n",
      "Uploading ./work/stages/0_DocumentAssembler_22b1808fafd8/metadata/.part-00000.crc to s3://lakehouse/bert_pipeline1000/stages/0_DocumentAssembler_22b1808fafd8/metadata/.part-00000.crc\n",
      "Uploading ./work/stages/0_DocumentAssembler_22b1808fafd8/metadata/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/stages/0_DocumentAssembler_22b1808fafd8/metadata/._SUCCESS.crc\n",
      "Uploading ./work/stages/0_DocumentAssembler_22b1808fafd8/metadata/part-00000 to s3://lakehouse/bert_pipeline1000/stages/0_DocumentAssembler_22b1808fafd8/metadata/part-00000\n",
      "Uploading ./work/stages/0_DocumentAssembler_22b1808fafd8/metadata/_SUCCESS to s3://lakehouse/bert_pipeline1000/stages/0_DocumentAssembler_22b1808fafd8/metadata/_SUCCESS\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00000.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00000.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00001.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00001.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00002.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00002.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00003.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00003.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00004.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00004.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00005.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00005.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00006.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00006.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00007.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/.part-00007.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/._SUCCESS.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00000 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00000\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00001 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00001\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00002 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00002\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00003 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00003\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00004 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00004\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00005 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00005\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00006 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00006\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00007 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/part-00007\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/_SUCCESS to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/fields/rules/_SUCCESS\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/.part-00000.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/.part-00000.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/._SUCCESS.crc\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/part-00000 to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/part-00000\n",
      "Uploading ./work/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/_SUCCESS to s3://lakehouse/bert_pipeline1000/stages/1_REGEX_TOKENIZER_52c3ea53609a/metadata/_SUCCESS\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/.bert_onnx.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/.bert_onnx.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/bert_onnx to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/bert_onnx\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00000.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00000.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00001.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00001.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00002.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00002.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00003.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00003.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00004.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00004.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00005.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00005.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00006.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00006.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00007.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/.part-00007.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/._SUCCESS.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00000 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00000\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00001 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00001\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00002 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00002\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00003 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00003\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00004 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00004\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00005 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00005\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00006 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00006\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00007 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/part-00007\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/_SUCCESS to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/fields/vocabulary/_SUCCESS\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/.part-00000.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/.part-00000.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/._SUCCESS.crc\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/part-00000 to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/part-00000\n",
      "Uploading ./work/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/_SUCCESS to s3://lakehouse/bert_pipeline1000/stages/2_DISTILBERT_EMBEDDINGS_7fff29a217c8/metadata/_SUCCESS\n",
      "Uploading ./work/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/.part-00000.crc to s3://lakehouse/bert_pipeline1000/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/.part-00000.crc\n",
      "Uploading ./work/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/._SUCCESS.crc to s3://lakehouse/bert_pipeline1000/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/._SUCCESS.crc\n",
      "Uploading ./work/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/part-00000 to s3://lakehouse/bert_pipeline1000/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/part-00000\n",
      "Uploading ./work/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/_SUCCESS to s3://lakehouse/bert_pipeline1000/stages/3_SentenceEmbeddings_0d75abf34ef2/metadata/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "delete_s3_folder(\"lakehouse\",\"bert_pipeline1000\")\n",
    "upload_folder_to_s3(\"./work\", \"lakehouse\", \"bert_pipeline1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09888595-e5b3-48c6-a32f-d05011fd4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/15 05:19:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "nlp = PipelineModel.load(\"s3a://lakehouse/bert_pipeline1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaf726f8-e4cf-4627-9ef4-237ddc27a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS Cosine Index ch·ª©a 20658 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "vec_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/bert2\")\n",
    "rows = vec_df.select(\"id\", \"vecs\").rdd.map(lambda row: (row[\"id\"], np.array(row[\"vecs\"], dtype=np.float32))).collect()\n",
    "\n",
    "ids = [r[0] for r in rows]\n",
    "vectors = np.stack([r[1] for r in rows])\n",
    "\n",
    "norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "normalized_vectors = vectors / norms\n",
    "\n",
    "index = faiss.IndexFlatIP(normalized_vectors.shape[1])\n",
    "index.add(normalized_vectors)\n",
    "\n",
    "print(f\"‚úÖ FAISS Cosine Index ch·ª©a {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f8448bf-1e17-4f6b-a975-e50697d70341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Top k·∫øt qu·∫£ (Cosine Similarity):\n",
      "1. Movie ID: 26342, Cosine Score: 0.9012\n",
      "2. Movie ID: 140607, Cosine Score: 0.8856\n",
      "3. Movie ID: 849, Cosine Score: 0.8828\n",
      "4. Movie ID: 284053, Cosine Score: 0.8826\n",
      "5. Movie ID: 68721, Cosine Score: 0.8824\n",
      "6. Movie ID: 197624, Cosine Score: 0.8808\n",
      "7. Movie ID: 52837, Cosine Score: 0.8807\n",
      "8. Movie ID: 299687, Cosine Score: 0.8806\n",
      "9. Movie ID: 217993, Cosine Score: 0.8769\n",
      "10. Movie ID: 45162, Cosine Score: 0.8769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=849, comb_bert=\"Title: Krull. This movie is titled 'Krull'. Tagline: A world light-years beyond your imagination.. Tagline repeated: A world light-years beyond your imagination.. Plot summary: A prince and a fellowship of companions set out to rescue his bride from a fortress of alien invaders who have arrived on their home planet.. This is a story about: A prince and a fellowship of companions set out to rescue his bride from a fortress of alien invaders who have arrived on their home planet.. \"),\n",
       " Row(id=217993, comb_bert=\"Title: Justice League: War. This movie is titled 'Justice League: War'. Tagline: A time when heroes became legends.. Tagline repeated: A time when heroes became legends.. Plot summary: The world is under attack by an alien armada led by the powerful Apokoliptian, Darkseid. A group of superheroes consisting of Superman, Batman, Wonder Woman, The Flash, Green Lantern, Cyborg, and Shazam must set aside their differences and gather together to defend Earth.. This is a story about: The world is under attack by an alien armada led by the powerful Apokoliptian, Darkseid. A group of superheroes consisting of Superman, Batman, Wonder Woman, The Flash, Green Lantern, Cyborg, and Shazam must set aside their differences and gather together to defend Earth.. \"),\n",
       " Row(id=45162, comb_bert=\"Title: Superman/Batman: Apocalypse. This movie is titled 'Superman/Batman: Apocalypse'. Tagline: The end is near.. Tagline repeated: The end is near.. Plot summary: Batman discovers a mysterious teen-aged girl with superhuman powers and a connection to Superman. When the girl comes to the attention of Darkseid, the evil overlord of Apokolips, events take a decidedly dangerous turn.. This is a story about: Batman discovers a mysterious teen-aged girl with superhuman powers and a connection to Superman. When the girl comes to the attention of Darkseid, the evil overlord of Apokolips, events take a decidedly dangerous turn.. \"),\n",
       " Row(id=26342, comb_bert=\"Title: The Terminators. This movie is titled 'The Terminators'. Tagline: Built to protect mankind, now programmed to destroy.. Tagline repeated: Built to protect mankind, now programmed to destroy.. Plot summary: A small band of resistance fighters battle the cyborgs that have taken control of the planet.. This is a story about: A small band of resistance fighters battle the cyborgs that have taken control of the planet.. \"),\n",
       " Row(id=284053, comb_bert=\"Title: Thor: Ragnarok. This movie is titled 'Thor: Ragnarok'. Tagline: No hammer. No problem.. Tagline repeated: No hammer. No problem.. Plot summary: Thor is imprisoned on the other side of the universe and finds himself in a race against time to get back to Asgard to stop Ragnarok, the destruction of his home-world and the end of Asgardian civilization, at the hands of a powerful new threat, the ruthless Hela.. This is a story about: Thor is imprisoned on the other side of the universe and finds himself in a race against time to get back to Asgard to stop Ragnarok, the destruction of his home-world and the end of Asgardian civilization, at the hands of a powerful new threat, the ruthless Hela.. \"),\n",
       " Row(id=68721, comb_bert=\"Title: Iron Man 3. This movie is titled 'Iron Man 3'. Tagline: Unleash the power behind the armor.. Tagline repeated: Unleash the power behind the armor.. Plot summary: When Tony Stark's world is torn apart by a formidable terrorist called the Mandarin, he starts an odyssey of rebuilding and retribution.. This is a story about: When Tony Stark's world is torn apart by a formidable terrorist called the Mandarin, he starts an odyssey of rebuilding and retribution.. \"),\n",
       " Row(id=52837, comb_bert=\"Title: Warrior of the Lost World. This movie is titled 'Warrior of the Lost World'. Tagline: Only one rider can destroy the Omega Force.. Tagline repeated: Only one rider can destroy the Omega Force.. Plot summary: A nomad mercenary on a high-tech motorcycle helps bring about the downfall of the evil Orwellian government, the Omega.. This is a story about: A nomad mercenary on a high-tech motorcycle helps bring about the downfall of the evil Orwellian government, the Omega.. \"),\n",
       " Row(id=140607, comb_bert=\"Title: Star Wars: The Force Awakens. This movie is titled 'Star Wars: The Force Awakens'. Tagline: Every generation has a story.. Tagline repeated: Every generation has a story.. Plot summary: Thirty years after defeating the Galactic Empire, Han Solo and his allies face a new threat from the evil Kylo Ren and his army of Stormtroopers.. This is a story about: Thirty years after defeating the Galactic Empire, Han Solo and his allies face a new threat from the evil Kylo Ren and his army of Stormtroopers.. \"),\n",
       " Row(id=197624, comb_bert=\"Title: AE: Apocalypse Earth. This movie is titled 'AE: Apocalypse Earth'. Tagline: The battle for mankind will not be on earth. Tagline repeated: The battle for mankind will not be on earth. Plot summary: A group of refugees from Earth land on an exotic planet, where they must fight ruthless aliens to survive.. This is a story about: A group of refugees from Earth land on an exotic planet, where they must fight ruthless aliens to survive.. \"),\n",
       " Row(id=299687, comb_bert=\"Title: The 5th Wave. This movie is titled 'The 5th Wave'. Tagline: Protect Your Own. Tagline repeated: Protect Your Own. Plot summary: 16-year-old Cassie Sullivan tries to survive in a world devastated by the waves of an alien invasion that has already decimated the population and knocked mankind back to the Stone Age.. This is a story about: 16-year-old Cassie Sullivan tries to survive in a world devastated by the waves of an alien invasion that has already decimated the population and knocked mankind back to the Stone Age.. \")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Truy v·∫•n ng∆∞·ªùi d√πng\n",
    "user_query = \"This is a story about: The world is under attack by an alien armada led by the powerful Apokoliptian, Darkseid\"\n",
    "query_df = spark.createDataFrame([[user_query]], [\"comb_bert\"])\n",
    "\n",
    "# L·∫•y vector\n",
    "query_vector = nlp.transform(query_df) \\\n",
    "    .select(col(\"sentence_embeddings.embeddings\")[0].alias(\"vec\")) \\\n",
    "    .collect()[0][\"vec\"]\n",
    "query_vector = np.array(query_vector, dtype=np.float32)\n",
    "\n",
    "# Normalize\n",
    "query_vector /= np.linalg.norm(query_vector)\n",
    "query_vector = query_vector.reshape(1, -1)\n",
    "\n",
    "# FAISS search\n",
    "k = 10\n",
    "scores, indices = index.search(query_vector, k)\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"üéØ Top k·∫øt qu·∫£ (Cosine Similarity):\")\n",
    "matched_ids = []\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    matched_ids.append(ids[idx])\n",
    "    print(f\"{i+1}. Movie ID: {ids[idx]}, Cosine Score: {scores[0][i]:.4f}\")\n",
    "\n",
    "# L·ªçc k·∫øt qu·∫£ t·ª´ DataFrame\n",
    "results = spark_df.filter(col(\"id\").isin(matched_ids)) \\\n",
    "                  .select(\"id\", \"comb_bert\")\n",
    "\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ed9b6-7b88-46b9-8b79-878e5e2fe010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73024d8c-4b64-4b75-9be3-85bee1846f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a0e0-9b8e-4bd1-bada-b60a0182485f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5be00-2a63-468e-8b4c-d7d5878972f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa4903-62fa-444b-990a-21cc2ee260a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254a27a-d1a4-4521-9637-8ddab5921a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb22095-c061-41a9-9a98-3ee6df243006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1f090-79e8-4134-b54c-4346eae2c852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ad55e-9dcc-4d76-97aa-501c45f62604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e43532-676d-4f19-83ba-69bb4ece8409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77ae41-6af0-4ceb-b2d6-c4c8d0ff1da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd4f21-46a0-4783-8826-67a9c01daf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8cb0f2-0fb6-4632-976c-aa30bd1993fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
