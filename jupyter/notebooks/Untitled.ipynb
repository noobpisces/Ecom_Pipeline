{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84ba852-7eec-4d92-943e-9d82a1203cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (37.6.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.40.21)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2023.3)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.21 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.40.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.13.1)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.21->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.21->boto3) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.21->boto3) (1.16.0)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install faker boto3 dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a62e98e8-1942-4024-b610-2e492aa5d54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "import boto3\n",
    "#import psycopg2\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ea998f-5bd8-47c1-9587-c191f1e414ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faker\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'"
     ]
    }
   ],
   "source": [
    "class RecentEcommerceDataGenerator:\n",
    "    def __init__(self, id_offset=1000000):  # Add a large offset to some IDs\n",
    "        # Set end date to today and start date to 30 days ago\n",
    "        self.end_date = datetime.now()\n",
    "        self.start_date = self.end_date - timedelta(days=30)\n",
    "        self.fake = Faker()\n",
    "        self.id_offset = id_offset  # Store the offset\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "        \n",
    "        # Keep existing templates and aspects from the original code\n",
    "        self.review_templates = {\n",
    "            'positive': [\n",
    "                \"Great {product_type}! {positive_aspect}. Highly recommend!\",\n",
    "                \"Really happy with this purchase. {positive_aspect} and {another_positive}.\",\n",
    "                \"Excellent quality {product_type}. {positive_aspect}.\",\n",
    "                \"Best {product_type} I've bought. {positive_aspect}.\",\n",
    "                \"{positive_aspect}. Worth every penny!\"\n",
    "            ],\n",
    "            'neutral': [\n",
    "                \"Decent {product_type}. {positive_aspect}, but {negative_aspect}.\",\n",
    "                \"Average {product_type}. {neutral_comment}.\",\n",
    "                \"Good enough for the price. {neutral_comment}.\",\n",
    "                \"{positive_aspect}, however {negative_aspect}.\",\n",
    "                \"Not bad, but not great. {neutral_comment}.\"\n",
    "            ],\n",
    "            'negative': [\n",
    "                \"Disappointed with this {product_type}. {negative_aspect}.\",\n",
    "                \"Not worth the price. {negative_aspect}.\",\n",
    "                \"Had issues with {negative_aspect}.\",\n",
    "                \"Wouldn't recommend. {negative_aspect}.\",\n",
    "                \"Poor quality {product_type}. {negative_aspect}.\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "        self.positive_aspects = [\n",
    "            \"Excellent build quality\",\n",
    "            \"Fast shipping\",\n",
    "            \"Great customer service\",\n",
    "            \"Perfect fit\",\n",
    "            \"Amazing features\",\n",
    "            \"Beautiful design\",\n",
    "            \"Easy to use\",\n",
    "            \"Incredible value\",\n",
    "            \"Superior performance\",\n",
    "            \"Long battery life\"\n",
    "        ]\n",
    "        \n",
    "        self.negative_aspects = [\n",
    "            \"Poor build quality\",\n",
    "            \"Slow delivery\",\n",
    "            \"Unresponsive customer service\",\n",
    "            \"Doesn't fit well\",\n",
    "            \"Missing features\",\n",
    "            \"Unappealing design\",\n",
    "            \"Complicated to use\",\n",
    "            \"Overpriced\",\n",
    "            \"Underperforms\",\n",
    "            \"Short battery life\"\n",
    "        ]\n",
    "        \n",
    "        self.neutral_comments = [\n",
    "            \"Meets basic expectations\",\n",
    "            \"Standard quality\",\n",
    "            \"Average performance\",\n",
    "            \"Fair for the price\",\n",
    "            \"Could be better\"\n",
    "        ]\n",
    "\n",
    "    def generate_product_categories(self):\n",
    "        \"\"\"Generate product categories and subcategories\"\"\"\n",
    "        categories = {\n",
    "            'Electronics': ['Smartphones', 'Laptops', 'Accessories', 'Tablets', 'Wearables'],\n",
    "            'Fashion': ['Men\\'s Clothing', 'Women\\'s Clothing', 'Children\\'s Clothing', 'Shoes', 'Accessories'],\n",
    "            'Home & Living': ['Furniture', 'Kitchen', 'Decor', 'Bedding', 'Storage'],\n",
    "            'Beauty': ['Skincare', 'Makeup', 'Haircare', 'Fragrances', 'Tools'],\n",
    "            'Sports': ['Exercise Equipment', 'Sportswear', 'Outdoor Gear', 'Accessories', 'Footwear']\n",
    "        }\n",
    "        \n",
    "        category_data = []\n",
    "        subcategory_data = []\n",
    "        \n",
    "        for cat_id, (category, subcategories) in enumerate(categories.items(), 1):\n",
    "            category_data.append({\n",
    "                'category_id': cat_id,\n",
    "                'category_name': category,\n",
    "                'created_at': self.start_date\n",
    "            })\n",
    "            \n",
    "            for sub_id, subcategory in enumerate(subcategories, 1):\n",
    "                subcategory_data.append({\n",
    "                    'subcategory_id': (cat_id * 100) + sub_id,\n",
    "                    'category_id': cat_id,\n",
    "                    'subcategory_name': subcategory,\n",
    "                    'created_at': self.start_date\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(category_data), pd.DataFrame(subcategory_data)\n",
    "\n",
    "    def generate_random_date(self):\n",
    "        \"\"\"Generate a random date within the last 30 days\"\"\"\n",
    "        days_offset = random.randint(0, 30)\n",
    "        return self.end_date - timedelta(days=days_offset)\n",
    "\n",
    "    def generate_products(self, n_products=1000):\n",
    "        \"\"\"Generate product catalog with recent dates\"\"\"\n",
    "        categories_df, subcategories_df = self.generate_product_categories()\n",
    "        \n",
    "        products = []\n",
    "        for product_id in range(1, n_products + 1):\n",
    "            category_id = random.randint(1, len(categories_df))\n",
    "            valid_subcats = subcategories_df[subcategories_df['category_id'] == category_id]\n",
    "            subcategory_id = random.choice(valid_subcats['subcategory_id'].values)\n",
    "            \n",
    "            base_price = random.uniform(10, 1000)\n",
    "            \n",
    "            # Use recent date for created_at\n",
    "            created_at = self.generate_random_date()\n",
    "            \n",
    "            products.append({\n",
    "                'product_id': product_id,\n",
    "                'category_id': category_id,\n",
    "                'subcategory_id': subcategory_id,\n",
    "                'product_name': f\"{self.fake.company()} {self.fake.word().title()}\",\n",
    "                'description': self.fake.text(max_nb_chars=200),\n",
    "                'base_price': round(base_price, 2),\n",
    "                'sale_price': round(base_price * random.uniform(0.8, 1.0), 2),\n",
    "                'stock_quantity': random.randint(0, 1000),\n",
    "                'weight_kg': round(random.uniform(0.1, 20.0), 2),\n",
    "                'is_active': random.random() > 0.1,\n",
    "                'created_at': created_at,\n",
    "                'brand': self.fake.company(),\n",
    "                'sku': f\"SKU-{random.randint(10000, 99999)}\",\n",
    "                'rating': round(random.uniform(3.0, 5.0), 1),\n",
    "                'review_count': random.randint(0, 100)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(products), categories_df, subcategories_df\n",
    "\n",
    "    def generate_customers(self, n_customers=1000, historic_customers_file='de-ecommerce/data/customers.csv'):\n",
    "        \"\"\"\n",
    "        Generate customer data with a mix of returning and new customers\n",
    "        \n",
    "        Parameters:\n",
    "            n_customers: Number of total customers to generate\n",
    "            historic_customers_file: Path to historic customers CSV file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with customer data\n",
    "        \"\"\"\n",
    "        # Try to load historic customers\n",
    "        try:\n",
    "            historic_df = pd.read_csv(historic_customers_file)\n",
    "            print(f\"Loaded {len(historic_df)} historic customers\")\n",
    "            \n",
    "            # Calculate number of returning customers (30% of historic customers)\n",
    "            n_returning = min(int(len(historic_df) * 0.3), n_customers)\n",
    "            n_new = n_customers - n_returning\n",
    "            \n",
    "            print(f\"Generating {n_returning} returning customers and {n_new} new customers\")\n",
    "            \n",
    "            # Select random returning customers\n",
    "            returning_customers = historic_df.sample(n=n_returning).copy()\n",
    "            \n",
    "            # Update their activity for the recent period\n",
    "            for idx in returning_customers.index:\n",
    "                # Generate new login date within the last 30 days\n",
    "                last_login = self.generate_random_date()\n",
    "                returning_customers.loc[idx, 'last_login'] = last_login\n",
    "                \n",
    "                # Maybe update some other fields that might change\n",
    "                returning_customers.loc[idx, 'annual_income'] = max(15000, int(np.random.normal(65000, 30000)))\n",
    "                returning_customers.loc[idx, 'marital_status'] = random.choice(['Single', 'Married', 'Divorced', 'Widowed'])\n",
    "                returning_customers.loc[idx, 'location_type'] = random.choice(['Urban', 'Suburban', 'Rural'])\n",
    "                returning_customers.loc[idx, 'preferred_channel'] = random.choice(['Web', 'Mobile App', 'Email'])\n",
    "                returning_customers.loc[idx, 'is_active'] = True  # They're active since they're returning\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"Historic customers file not found. Generating all new customers.\")\n",
    "            n_returning = 0\n",
    "            n_new = n_customers\n",
    "            returning_customers = pd.DataFrame()\n",
    "        \n",
    "        # Generate new customers\n",
    "        new_customers = []\n",
    "        start_id = self.id_offset if not len(returning_customers) else max(returning_customers['customer_id']) + 1\n",
    "        \n",
    "        for customer_id in range(start_id, start_id + n_new):\n",
    "            signup_date = self.generate_random_date()\n",
    "            last_login = min(signup_date + timedelta(days=random.randint(0, 5)), self.end_date)\n",
    "            \n",
    "            new_customers.append({\n",
    "                'customer_id': customer_id,\n",
    "                'email': self.fake.email(),\n",
    "                'first_name': self.fake.first_name(),\n",
    "                'last_name': self.fake.last_name(),\n",
    "                'age': max(18, min(90, int(np.random.normal(45, 15)))),\n",
    "                'gender': random.choice(['M', 'F', 'Other']),\n",
    "                'annual_income': max(15000, int(np.random.normal(65000, 30000))),\n",
    "                'marital_status': random.choice(['Single', 'Married', 'Divorced', 'Widowed']),\n",
    "                'education': random.choice(['High School', 'Some College', 'Bachelor', 'Master', 'PhD']),\n",
    "                'location_type': random.choice(['Urban', 'Suburban', 'Rural']),\n",
    "                'city': self.fake.city(),\n",
    "                'state': self.fake.state(),\n",
    "                'country': 'USA',\n",
    "                'signup_date': signup_date,\n",
    "                'last_login': last_login,\n",
    "                'preferred_channel': random.choice(['Web', 'Mobile App', 'Email']),\n",
    "                'is_active': random.random() > 0.1\n",
    "            })\n",
    "        \n",
    "        # Combine returning and new customers\n",
    "        new_customers_df = pd.DataFrame(new_customers)\n",
    "        final_customers_df = pd.concat([returning_customers, new_customers_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Generated total of {len(final_customers_df)} customers\")\n",
    "        print(f\"- Returning customers: {len(returning_customers)}\")\n",
    "        print(f\"- New customers: {len(new_customers_df)}\")\n",
    "        \n",
    "        return final_customers_df\n",
    "\n",
    "    def generate_orders(self, customers_df, products_df):\n",
    "        \"\"\"Generate order data for the last 30 days\"\"\"\n",
    "        orders = []\n",
    "        order_items = []\n",
    "        order_id = 1\n",
    "        \n",
    "        for _, customer in customers_df.iterrows():\n",
    "            # Reduce number of orders for 30-day period\n",
    "            num_orders = np.random.poisson(2)  # Reduced from 5 to 2\n",
    "            \n",
    "            for _ in range(num_orders):\n",
    "                order_id = order_id + self.id_offset\n",
    "                order_date = max(\n",
    "                    customer['signup_date'],\n",
    "                    self.generate_random_date()\n",
    "                )\n",
    "                \n",
    "                # Order status based on recent dates\n",
    "                days_since_order = (self.end_date - order_date).days\n",
    "                if days_since_order < 2:\n",
    "                    status = 'Pending'\n",
    "                elif days_since_order < 4:\n",
    "                    status = 'Processing'\n",
    "                elif days_since_order < 7:\n",
    "                    status = 'Shipped'\n",
    "                else:\n",
    "                    status = 'Delivered'\n",
    "                \n",
    "                # Generate order items\n",
    "                num_items = np.random.poisson(2) + 1\n",
    "                order_products = products_df.sample(n=min(num_items, len(products_df)))\n",
    "                \n",
    "                shipping_cost = round(random.uniform(5, 20), 2)\n",
    "                total_amount = shipping_cost\n",
    "                \n",
    "                for _, product in order_products.iterrows():\n",
    "                    quantity = random.randint(1, 3)\n",
    "                    price = product['sale_price']\n",
    "                    item_total = quantity * price\n",
    "                    total_amount += item_total\n",
    "                    \n",
    "                    order_items.append({\n",
    "                        'order_item_id': len(order_items) + 1,\n",
    "                        'order_id': order_id,\n",
    "                        'product_id': product['product_id'],\n",
    "                        'quantity': quantity,\n",
    "                        'unit_price': price,\n",
    "                        'total_price': item_total,\n",
    "                        'created_at': order_date\n",
    "                    })\n",
    "                \n",
    "                orders.append({\n",
    "                    'order_id': order_id,\n",
    "                    'customer_id': customer['customer_id'],\n",
    "                    'order_date': order_date,\n",
    "                    'status': status,\n",
    "                    'total_amount': round(total_amount, 2),\n",
    "                    'shipping_cost': shipping_cost,\n",
    "                    'payment_method': random.choice(['Credit Card', 'PayPal', 'Debit Card']),\n",
    "                    'shipping_address': self.fake.street_address(),\n",
    "                    'billing_address': self.fake.street_address(),\n",
    "                    'created_at': order_date,\n",
    "                    'updated_at': order_date + timedelta(days=random.randint(0, 2))\n",
    "                })\n",
    "                \n",
    "                order_id += 1\n",
    "        \n",
    "        return pd.DataFrame(orders), pd.DataFrame(order_items)\n",
    "\n",
    "    def generate_customer_interactions(self, customers_df, products_df):\n",
    "        \"\"\"Generate customer interactions for the last 30 days\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        for _, customer in customers_df.iterrows():\n",
    "            # Reduce number of interactions for 30-day period\n",
    "            num_interactions = np.random.poisson(10)  # Reduced from 20 to 10\n",
    "            \n",
    "            for _ in range(num_interactions):\n",
    "                event_date = max(\n",
    "                    customer['signup_date'],\n",
    "                    self.generate_random_date()\n",
    "                )\n",
    "                \n",
    "                product = products_df.sample(n=1).iloc[0]\n",
    "                \n",
    "                events.append({\n",
    "                    'event_id': len(events) + 1 + self.id_offset,\n",
    "                    'customer_id': customer['customer_id'],\n",
    "                    'product_id': product['product_id'],\n",
    "                    'event_type': random.choice(['view', 'cart_add', 'cart_remove', 'wishlist_add', 'search', 'purchase']),\n",
    "                    'event_date': event_date,\n",
    "                    'device_type': random.choice(['desktop', 'mobile', 'tablet']),\n",
    "                    'session_id': f\"session_{random.randint(10000, 99999)}\",\n",
    "                    'created_at': event_date\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(events)\n",
    "    \n",
    "    def generate_review_text(self, rating, product_type):\n",
    "        \"\"\"Generate realistic review text based on rating\"\"\"\n",
    "        if rating >= 4:\n",
    "            template = random.choice(self.review_templates['positive'])\n",
    "            aspect = random.choice(self.positive_aspects)\n",
    "            another = random.choice(self.positive_aspects)\n",
    "            return template.format(\n",
    "                product_type=product_type,\n",
    "                positive_aspect=aspect,\n",
    "                another_positive=another\n",
    "            )\n",
    "        elif rating >= 3:\n",
    "            template = random.choice(self.review_templates['neutral'])\n",
    "            return template.format(\n",
    "                product_type=product_type,\n",
    "                positive_aspect=random.choice(self.positive_aspects),\n",
    "                negative_aspect=random.choice(self.negative_aspects),\n",
    "                neutral_comment=random.choice(self.neutral_comments)\n",
    "            )\n",
    "        else:\n",
    "            template = random.choice(self.review_templates['negative'])\n",
    "            return template.format(\n",
    "                product_type=product_type,\n",
    "                negative_aspect=random.choice(self.negative_aspects)\n",
    "            )\n",
    "\n",
    "    def generate_reviews(self, orders_df, order_items_df, products_df, customers_df):\n",
    "        \"\"\"Generate reviews with new IDs\"\"\"\n",
    "        reviews_data = []\n",
    "        review_id = 1\n",
    "        \n",
    "        for _, row in orders_df.iterrows():\n",
    "            order_items = order_items_df[order_items_df['order_id'] == row['order_id']]\n",
    "\n",
    "            for _, item in order_items.iterrows():\n",
    "                reviews_data.append({\n",
    "                    'review_id': review_id + self.id_offset,\n",
    "                    'product_id': item['product_id'],\n",
    "                    'order_id': row['order_id'],\n",
    "                    'customer_id': row['customer_id'],\n",
    "                    'review_score': np.random.randint(1, 6),\n",
    "                    'review_text': self.generate_review_text(\n",
    "                        np.random.randint(1, 6),\n",
    "                        products_df[products_df['product_id'] == item['product_id']].iloc[0]['product_name']\n",
    "                    )\n",
    "                })\n",
    "                review_id += 1\n",
    "\n",
    "                # Update product review count\n",
    "                products_df.loc[products_df['product_id'] == item['product_id'], 'review_count'] += 1\n",
    "\n",
    "        return pd.DataFrame(reviews_data), products_df\n",
    "\n",
    "    def generate_all_data(self, n_customers=1000, n_products=1000):\n",
    "        \"\"\"Generate all e-commerce data for the last 30 days\"\"\"\n",
    "        output_formats = {\n",
    "            'customers': 'json',\n",
    "            'products': 'json',\n",
    "            'orders': 'json',\n",
    "            'reviews': 'json',\n",
    "            'categories': 'csv',\n",
    "            'subcategories': 'csv',\n",
    "            'order_items': 'csv',\n",
    "            'interactions': 'csv'\n",
    "        }\n",
    "        \n",
    "        print(f\"Generating data for period: {self.start_date.date()} to {self.end_date.date()}\")\n",
    "        \n",
    "        print(\"Generating products...\")\n",
    "        products_df, categories_df, subcategories_df = self.generate_products(n_products)\n",
    "        \n",
    "        print(\"Generating customers...\")\n",
    "        customers_df = self.generate_customers(n_customers)\n",
    "        \n",
    "        print(\"Generating orders...\")\n",
    "        orders_df, order_items_df = self.generate_orders(customers_df, products_df)\n",
    "        \n",
    "        print(\"Generating reviews...\")\n",
    "        reviews_df, updated_products_df = self.generate_reviews(orders_df, order_items_df, products_df, customers_df)\n",
    "        \n",
    "        print(\"Generating customer interactions...\")\n",
    "        interactions_df = self.generate_customer_interactions(customers_df, updated_products_df)\n",
    "        \n",
    "        data_dict = {\n",
    "            'customers': customers_df,\n",
    "            'products': updated_products_df,\n",
    "            'categories': categories_df,\n",
    "            'subcategories': subcategories_df,\n",
    "            'orders': orders_df,\n",
    "            'order_items': order_items_df,\n",
    "            'reviews': reviews_df,\n",
    "            'interactions': interactions_df\n",
    "        }\n",
    "        \n",
    "        # self.save_data(data_dict, output_formats)\n",
    "        \n",
    "        return data_dict\n",
    "    \n",
    "class EcommerceDataLoader:\n",
    "    def __init__(self, aws_access_key_id, aws_secret_access_key, bucket_name, \n",
    "                 pg_host, pg_port, pg_user, \n",
    "                 pg_password, pg_database):\n",
    "        \"\"\"Initialize connections to S3 and PostgreSQL\"\"\"\n",
    "        # S3 client setup\n",
    "        self.s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "        )\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "        # PostgreSQL connection setup\n",
    "        self.pg_conn_string = f\"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_database}\"\n",
    "        self.engine = create_engine(self.pg_conn_string)\n",
    "\n",
    "    def upload_json_to_s3(self, data_dict, table_name):\n",
    "        \"\"\"Upload JSON data directly to S3\"\"\"\n",
    "        try:\n",
    "            # Convert DataFrame to JSON format with metadata\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"table\": table_name,\n",
    "                    \"recordCount\": len(data_dict),\n",
    "                    \"generatedAt\": pd.Timestamp.now().isoformat(),\n",
    "                    \"version\": \"1.0\"\n",
    "                },\n",
    "                \"data\": json.loads(data_dict.to_json(orient='records', date_format='iso'))\n",
    "            }\n",
    "            \n",
    "            # Convert to JSON string\n",
    "            json_str = json.dumps(json_data)\n",
    "            \n",
    "            # Upload to S3\n",
    "            s3_key = f'json/{table_name}.json'\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.bucket_name,\n",
    "                Key=s3_key,\n",
    "                Body=json_str,\n",
    "                ContentType='application/json'\n",
    "            )\n",
    "            print(f\"Uploaded {table_name} to S3: s3://{self.bucket_name}/{s3_key}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {table_name} to S3: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_csv_to_postgres(self, df, table_name):\n",
    "        \"\"\"Load DataFrame directly to PostgreSQL\"\"\"\n",
    "        \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"df must be a pandas DataFrame\")\n",
    "        try:\n",
    "            # Create copy buffer\n",
    "            output = StringIO()\n",
    "            df.to_csv(output, index=False, header=True)\n",
    "            output.seek(0)\n",
    "            \n",
    "            # Create database connection\n",
    "            with self.engine.connect() as connection:\n",
    "                # Create table if it doesn't exist\n",
    "                df.head(0).to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "                \n",
    "                # Copy data\n",
    "                raw_conn = connection.connection\n",
    "                with raw_conn.cursor() as cursor:\n",
    "                    cursor.copy_expert(\n",
    "                        f\"COPY {table_name} FROM STDIN WITH CSV HEADER\",\n",
    "                        output\n",
    "                    )\n",
    "                raw_conn.commit()\n",
    "                \n",
    "            print(f\"Loaded {len(df)} rows to PostgreSQL table: {table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {table_name} to PostgreSQL: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def save_data(data_dict):\n",
    "    \"\"\"Save data to S3 and PostgreSQL\"\"\"\n",
    "    # AWS credentials (replace with your own)\n",
    "    aws_access_key_id = os.environ.get(\"AWS_S3_ACCESS_KEY_ID\")\n",
    "    aws_secret_access_key = os.environ.get(\"AWS_S3_SECRET_ACCESS_KEY\")\n",
    "    bucket_name = os.environ.get(\"AWS_S3_LATEST_SYNTH\")\n",
    "\n",
    "    # Validate AWS credentials\n",
    "    if not all([aws_access_key_id, aws_secret_access_key, bucket_name]):\n",
    "        raise ValueError(\"Required AWS environment variables are not set\")\n",
    "\n",
    "    # PostgreSQL connection details\n",
    "    pg_config = {\n",
    "        'pg_host': os.environ.get(\"POSTGRES_HOST\"),\n",
    "        'pg_port': int(os.environ.get(\"POSTGRES_PORT\")),\n",
    "        'pg_user': os.environ.get(\"POSTGRES_USER\"),\n",
    "        'pg_password': os.environ.get(\"POSTGRES_PASSWORD\"),\n",
    "        'pg_database': os.environ.get(\"POSTGRES_DB\")\n",
    "    }\n",
    "\n",
    "    # Initialize loader\n",
    "    loader = EcommerceDataLoader(\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        bucket_name=bucket_name,\n",
    "        **pg_config\n",
    "    )\n",
    "\n",
    "    # Define which tables go to which destination\n",
    "    json_tables = ['customers', 'products', 'orders', 'reviews']\n",
    "    csv_tables_mapping = {\n",
    "        'categories': 'latest_categories',\n",
    "        'subcategories': 'latest_subcategories',\n",
    "        'order_items': 'latest_order_items',\n",
    "        'interactions': 'latest_interactions'\n",
    "    }\n",
    "    \n",
    "    # Upload JSON files to S3\n",
    "    for table_name in json_tables:\n",
    "        if table_name in data_dict:\n",
    "            loader.upload_json_to_s3(data_dict[table_name], table_name)\n",
    "\n",
    "    # Load CSV files to PostgreSQL with the 'latest_' prefix\n",
    "    for original_name, latest_name in csv_tables_mapping.items():\n",
    "        if original_name in data_dict:\n",
    "            print(f\"Loading {original_name} to PostgreSQL as {latest_name}\")\n",
    "            loader.load_csv_to_postgres(data_dict[original_name], latest_name)\n",
    "            \n",
    "    output_dir = Path('generated_latest_data')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for table_name, df in data_dict.items():\n",
    "        csv_path = output_dir / f\"{table_name}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved {table_name} to {csv_path}\")\n",
    "        \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for table_name, df in data_dict.items():\n",
    "        csv_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved {table_name} to {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    generator = RecentEcommerceDataGenerator()\n",
    "    data = generator.generate_all_data()\n",
    "    \n",
    "    # Save data directly to S3 and PostgreSQL\n",
    "    save_data(data)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nData Generation and Loading Summary:\")\n",
    "    for table_name, df in data.items():\n",
    "        print(f\"\\n{table_name.upper()} Table:\")\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Different tables have different date columns\n",
    "        date_columns = {\n",
    "            'customers': 'signup_date',\n",
    "            'products': 'created_at',\n",
    "            'orders': 'order_date',\n",
    "            'latest_order_items': 'created_at',\n",
    "            'reviews': 'created_at',\n",
    "            'latest_interactions': 'event_date',\n",
    "            'latest_categories': 'created_at',\n",
    "            'latest_subcategories': 'created_at'\n",
    "        }\n",
    "        \n",
    "        # Get the appropriate date column for this table\n",
    "        date_col = date_columns.get(table_name)\n",
    "        if date_col and date_col in df.columns:\n",
    "            print(f\"Date range: {df[date_col].min()} to {df[date_col].max()}\")\n",
    "        \n",
    "        destination = 'S3' if table_name in ['customers', 'products', 'orders', 'reviews'] else 'PostgreSQL'\n",
    "        print(f\"Loaded to: {destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00103c2-e2ea-4e84-802d-db96f6ae427a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
