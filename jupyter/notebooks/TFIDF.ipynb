{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a142b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c00596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enable-non-concurrent-writes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/25 06:28:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MinIO with Delta Lake BERT\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#     .config(\"spark.driver.cores\", \"6\") \\\n",
    "#     .config(\"spark.executor.cores\", \"6\") \\\n",
    "#     .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\") \\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#     .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "#     .config(\"spark.dynamicAllocation.maxExecutors\", \"1\") \\\n",
    "#     .config(\"spark.dynamicAllocation.initialExecutors\", \"1\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "#     .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "#     .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "#     .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "#     .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "#     .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HybridRecommenderMaxPerformance\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10134af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "spark_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"comb\", outputCol=\"words\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    stopword_remover, \n",
    "    hashingTF, \n",
    "    idf])\n",
    "pipeline_mdl = pipeline.fit(spark_df)\n",
    "new_df = pipeline_mdl.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44af79e5-fa50-4a96-bd1f-a662a866c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa UDF để chuyển sparse vector thành dense vector\n",
    "def to_dense(sparse_vector):\n",
    "    return Vectors.dense(sparse_vector).tolist()\n",
    "\n",
    "to_dense_udf = udf(to_dense, ArrayType(DoubleType()))\n",
    "\n",
    "# Áp dụng UDF để tạo cột 'vecs'\n",
    "new_df_with_dense = new_df.withColumn(\"vecs\", to_dense_udf(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d825afb0-8b54-4963-a24a-e432eca38762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df_with_dense.select('id', 'vecs').write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/all_movies_delta_IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106c853-3223-4214-8ef3-302b544925d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/25 06:28:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/05/25 06:28:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, StopWordsRemover, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# === B1: Đọc dữ liệu ===\n",
    "spark_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"comb\", outputCol=\"words\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", vocabSize=5000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopword_remover,\n",
    "    count_vectorizer,\n",
    "    idf\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(spark_df)\n",
    "transformed_df = pipeline_model.transform(spark_df)\n",
    "\n",
    "def to_list(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "to_list_udf = F.udf(to_list, ArrayType(DoubleType()))\n",
    "new_df_with_dense = transformed_df.withColumn(\"vecs\", to_list_udf(\"features\")).select(\"id\", \"vecs\")\n",
    "new_df_with_dense.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/tfidf\")\n",
    "\n",
    "\n",
    "# pca = PCA(k=500, inputCol=\"features\", outputCol=\"pca_features\")  # Output tạm\n",
    "\n",
    "# ,\n",
    "#     pca\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f676b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/25 06:34:35 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 302924 ms exceeds timeout 120000 ms\n",
      "25/05/25 06:34:35 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 302821 ms exceeds timeout 120000 ms\n",
      "25/05/25 06:34:35 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:632)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/25 06:34:35 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.5: Executor heartbeat timed out after 302924 ms\n",
      "25/05/25 06:34:35 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.5: Executor heartbeat timed out after 302821 ms\n",
      "25/05/25 06:34:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_37 !\n",
      "25/05/25 06:34:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_17 !\n",
      "25/05/25 06:34:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_48 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_42 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_29 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_2 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_22 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_49 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_46 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_18 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_12 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_24 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_14 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_4 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_30 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_45 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_6 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_39 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_8 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_11 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_16 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_44 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_1 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_40 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_13 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_33 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_7 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_25 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_36 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_15 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_47 !\n",
      "25/05/25 06:34:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_41 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_19 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_23 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_9 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_32 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_43 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_28 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_10 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_38 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_0 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_20 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_26 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_27 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_35 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_21 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_5 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_3 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_34 !\n",
      "25/05/25 06:34:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_31 !\n",
      "25/05/25 06:34:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(spark_df)\n",
    "transformed_df = pipeline_model.transform(spark_df)\n",
    "def to_list(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "to_list_udf = F.udf(to_list, ArrayType(DoubleType()))\n",
    "new_df_with_dense = transformed_df.withColumn(\"vecs\", to_list_udf(\"features\")).select(\"id\", \"vecs\")\n",
    "new_df_with_dense.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11ed25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/25 07:33:50 WARN DAGScheduler: Broadcasting large task binary with size 19.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "new_df_with_dense.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c9936a-70f5-4cf6-82df-78312ef0fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                vecs|\n",
      "+---+--------------------+\n",
      "| 12|[-2.3039491471256...|\n",
      "| 13|[-5.2289678333324...|\n",
      "| 14|[-1.6661411464084...|\n",
      "| 18|[-1.2120289770257...|\n",
      "| 38|[-2.5447459642824...|\n",
      "| 67|[-1.1096207282233...|\n",
      "| 70|[-1.4568195231945...|\n",
      "| 93|[-0.8813424427756...|\n",
      "|107|[-2.3664428998391...|\n",
      "|148|[-2.7694230284943...|\n",
      "|157|[-0.0987215878968...|\n",
      "|161|[-1.4073323188082...|\n",
      "|171|[-3.1400422558676...|\n",
      "|186|[-1.9267669874678...|\n",
      "|198|[-0.2626666579157...|\n",
      "|203|[-0.9092396108795...|\n",
      "|218|[-1.0866922469058...|\n",
      "|221|[-3.2264572986080...|\n",
      "|225|[-1.2842097114594...|\n",
      "|232|[-1.1047228408632...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t= spark.read.format(\"delta\").load(\"s3a://lakehouse/data/tfidf\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4c38f-7566-4b26-8cba-9698a481e4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
