{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638f47a1-cae7-428d-a3e0-4b88217860cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType,ArrayType,LongType\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr,when,to_date ,udf, concat_ws,posexplode, from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, IntegerType, DateType, FloatType,DoubleType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099d0057-3dbd-4c6f-af7d-287a2cd5af66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO with Delta CB\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\") \\\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/delta-core_2.12-2.2.0.jar,jars/delta-storage-2.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"conbo123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"123conbo\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"delta.enable-non-concurrent-writes\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://lakehouse/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0fd99-ca15-4e34-9c68-30f59150e0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe455e2-cb4c-4ce9-a21b-542fe5cad549",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/machineData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978c40c-2613-4337-893c-71a677ee11f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(\n",
    "    gaps=False, pattern='\\w+', inputCol='comb', outputCol='token')\n",
    "stopWordsRemover = StopWordsRemover(\n",
    "    inputCol='token', outputCol='nostopwrd')\n",
    "word2Vec = Word2Vec(vectorSize=150, minCount=3,windowSize=10, \n",
    "                    inputCol='nostopwrd', outputCol='word_vec', seed=123)\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, word2Vec])\n",
    "pipeline_model = pipeline.fit(spark_df)\n",
    "pipeline_model.write().overwrite().save(\n",
    "    \"s3a://lakehouse/model/\" + 'pipeline_model')\n",
    "\n",
    "pipeline_mdl = PipelineModel.load(\"s3a://lakehouse/model/\" + 'pipeline_model')\n",
    "new_df = pipeline_mdl.transform(spark_df)\n",
    "all_movies_vecs = new_df.select('id', 'word_vec').rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "data = [(id, [float(x) for x in vec]) for id, vec in all_movies_vecs]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"vecs\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "\n",
    "all_movies_df = spark.createDataFrame(data, schema)\n",
    "all_movies_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/all_movies_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aae55e-6091-405b-9e10-a5a7aa0985ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9defaa8-8b91-4658-990d-f5a6eb798356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipeline_mdl = PipelineModel.load(\"s3a://lakehouse/model/\" + 'pipeline_model')\n",
    "new_df = pipeline_mdl.transform(spark_df)\n",
    "all_movies_vecs = new_df.select('id', 'word_vec').rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "data = [(id, [float(x) for x in vec]) for id, vec in all_movies_vecs]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"vecs\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "all_movies_df = spark.createDataFrame(data, schema)\n",
    "all_movies_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/data/all_movies_delta\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed3ed96-d612-41ee-ac52-dfbe6f98858f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45433"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/all_movies_delta\")\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c665f0cb-0f0c-44f1-b94e-ed09c42bbcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vecs: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/word2vec\")\n",
    "df_list.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398e749-c595-4f71-9c6d-1b182e51f218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movie name:  Toy Story\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, mean, lit, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df_merge = None\n",
    "df_list = None\n",
    "flag1 = False\n",
    "flag2 = False\n",
    "\n",
    "\n",
    "def CosineSim(vec1, vec2):\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2))\n",
    "    return float(numerator / denominator) if denominator != 0 else 0\n",
    "\n",
    "def get_titles():\n",
    "    global df_merge, flag1\n",
    "    if not flag1:\n",
    "        df_merge =  spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/MergeData\")\n",
    "        flag1 = True\n",
    "    return list(df_merge.select(\"title\").toPandas()[\"title\"])\n",
    "\n",
    "def get_vecs():\n",
    "    global df_list, all_vecs, flag2\n",
    "    df_list = spark.read.format(\"delta\").load(\"s3a://lakehouse/data/hehe123\")\n",
    "    data_original = df_list.collect()\n",
    "    all_vecs = [(int(row.id), Vectors.dense(row.vecs)) for row in data_original]\n",
    "    flag2 = True\n",
    "    return all_vecs\n",
    "\n",
    "def recommendation(m_title, sim_mov_limit=10):\n",
    "    global df_merge\n",
    "    df_merge =  spark.read.format(\"delta\").load(\"s3a://lakehouse/gold/MergeData\")\n",
    "    if df_merge.filter(col(\"title\") == m_title).count() == 0:\n",
    "        return \"Sorry! The movie you searched is not in our database. Please check the spelling or try another movie.\"\n",
    "    \n",
    "    all_movies_vecs = get_vecs()\n",
    "    m_id = df_merge.filter(col(\"title\") == m_title).select(col('id')).collect()[0][0]\n",
    "    # print(f\"type of m_id: {type(m_id)}\")\n",
    "    # print(f\"sample id in all_movies_vecs: {type(all_movies_vecs[0][0])}\")\n",
    "\n",
    "    input_vec = [r[1] for r in all_movies_vecs if r[0] == m_id][0]\n",
    "    # print(f\"type of m_id: {type(m_id)}\")\n",
    "    # print(f\"sample id in all_movies_vecs: {type(all_movies_vecs[0][0])}\")\n",
    "    # print(m_id)\n",
    "    # print(input_vec)\n",
    "    similar_movies_rdd = spark.sparkContext.parallelize(\n",
    "        [(i[0], CosineSim(input_vec, i[1])) for i in all_movies_vecs]\n",
    "    )\n",
    "    \n",
    "    similar_movies_df = spark.createDataFrame(similar_movies_rdd, [\"movies_id\", \"score\"]) \\\n",
    "        .orderBy(col(\"score\").desc()) \\\n",
    "        .filter(col(\"movies_id\") != m_id) \\\n",
    "        .limit(sim_mov_limit)\n",
    "    \n",
    "    similar_movies_df = similar_movies_df.withColumn(\"input_movies_id\", lit(m_id))\n",
    "    return similar_movies_df.toPandas()\n",
    "\n",
    "def getMovieDetails(in_mov):\n",
    "    global df_merge\n",
    "    vote_counts = df_merge.filter(col(\"vote_count\").isNotNull()).select(col(\"vote_count\"))\n",
    "    vote_averages = df_merge.filter(col(\"vote_average\").isNotNull()).select(col(\"vote_average\"))\n",
    "    C = vote_averages.select(mean(\"vote_average\")).collect()[0][0]\n",
    "    quantiles = vote_counts.approxQuantile(\"vote_count\", [0.7], 0.001)\n",
    "    m = quantiles[0]\n",
    "    qualified = df_merge.filter((col(\"vote_count\") >= m) & col(\"vote_count\").isNotNull() & col(\"vote_average\").isNotNull())\n",
    "    qualified = qualified.withColumn(\"vote_count\", col(\"vote_count\").cast(\"int\")) \\\n",
    "        .withColumn(\"vote_average\", col(\"vote_average\").cast(\"int\"))\n",
    "    weighted_rating_udf = udf(lambda v, R: (\n",
    "        v / (v + m) * R) + (m / (m + v) * C), FloatType())\n",
    "    qualified = qualified.withColumn(\"weighted_rating\", weighted_rating_udf(\n",
    "        col(\"vote_count\"), col(\"vote_average\")))\n",
    "    qualified = qualified.orderBy(col(\"weighted_rating\").desc())\n",
    "\n",
    "    if isinstance(in_mov, str):\n",
    "        return \"Invalid input\"\n",
    "    a = in_mov.alias(\"a\")\n",
    "    b = qualified.alias(\"b\")\n",
    "\n",
    "    raw = a.join(b, col(\"a.movies_id\") == col(\"b.id\"), 'inner') \\\n",
    "        .orderBy(\"score\", ascending=False) \\\n",
    "        .select([col('a.' + c) for c in a.columns] + [col('b.title'), col('b.genres'), col('b.keyword_names'), col(\"b.director_names\"), col(\"b.cast_names\"), col(\"b.weighted_rating\")])\n",
    "    \n",
    "    return raw.select(\"movies_id\", \"input_movies_id\", \"title\", \"genres\", \"director_names\", \"cast_names\", \"score\", \"weighted_rating\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     movie_name = input(\"Enter a movie name: \")\n",
    "#     recommendations = recommendation(movie_name, sim_mov_limit=5)\n",
    "#     print(\"Recommended movies:\")\n",
    "#     print(recommendations)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    movie_name = input(\"Enter a movie name: \")\n",
    "    recommendations = recommendation(movie_name, sim_mov_limit=10)\n",
    "\n",
    "    if isinstance(recommendations, str):  # Nếu không tìm thấy phim\n",
    "        print(recommendations)\n",
    "    else:\n",
    "        print(\"Recommended movies:\")\n",
    "        print(recommendations)\n",
    "\n",
    "        # Chuyển đổi kiểu dữ liệu của Pandas DataFrame về đúng dạng trước khi đưa vào Spark\n",
    "        recommendations = recommendations.astype({\n",
    "            \"movies_id\": int,\n",
    "            \"input_movies_id\": int,\n",
    "            \"score\": float\n",
    "        })\n",
    "\n",
    "        # Xác định schema cho Spark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"movies_id\", IntegerType(), True),\n",
    "            StructField(\"score\", FloatType(), True),\n",
    "            StructField(\"input_movies_id\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        # Chuyển đổi Pandas DataFrame thành danh sách từ điển\n",
    "        recommendations_list = recommendations.to_dict(orient=\"records\")\n",
    "\n",
    "        # Chuyển đổi danh sách từ điển thành Spark DataFrame\n",
    "        recommendations_spark_df = spark.createDataFrame(recommendations_list, schema=schema)\n",
    "        # recommendations_spark_df.show()\n",
    "        \n",
    "        details = getMovieDetails(recommendations_spark_df)\n",
    "\n",
    "\n",
    "        # Hiển thị thông tin chi tiết\n",
    "        print(\"Detailed movie recommendations:\")\n",
    "        print(details.toPandas())  # Chuyển về Pandas DataFrame để hiển thị\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecd266-9cb0-46f1-92bd-b568bad91882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
